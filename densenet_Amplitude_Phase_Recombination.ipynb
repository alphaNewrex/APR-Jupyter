{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v-E1ObShkKrG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import datetime\n",
        "import time\n",
        "import csv\n",
        "import os.path as osp\n",
        "import numpy as np\n",
        "import warnings\n",
        "import errno\n",
        "import importlib\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "_yp8KiUpkgzv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "rBN4J-w8kmqG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Base augmentations operators.\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps, ImageEnhance\n",
        "\n",
        "# ImageNet code should change this value\n",
        "IMAGE_SIZE = 32\n",
        "\n",
        "\n",
        "def int_parameter(level, maxval):\n",
        "  \"\"\"Helper function to scale `val` between 0 and maxval .\n",
        "  Args:\n",
        "    level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n",
        "    maxval: Maximum value that the operation can have. This will be scaled to\n",
        "      level/PARAMETER_MAX.\n",
        "  Returns:\n",
        "    An int that results from scaling `maxval` according to `level`.\n",
        "  \"\"\"\n",
        "  return int(level * maxval / 10)\n",
        "\n",
        "\n",
        "def float_parameter(level, maxval):\n",
        "  \"\"\"Helper function to scale `val` between 0 and maxval.\n",
        "  Args:\n",
        "    level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n",
        "    maxval: Maximum value that the operation can have. This will be scaled to\n",
        "      level/PARAMETER_MAX.\n",
        "  Returns:\n",
        "    A float that results from scaling `maxval` according to `level`.\n",
        "  \"\"\"\n",
        "  return float(level) * maxval / 10.\n",
        "\n",
        "\n",
        "def sample_level(n):\n",
        "  return np.random.uniform(low=0.1, high=n)\n",
        "\n",
        "\n",
        "def autocontrast(pil_img, _):\n",
        "  return ImageOps.autocontrast(pil_img)\n",
        "\n",
        "\n",
        "def equalize(pil_img, _):\n",
        "  return ImageOps.equalize(pil_img)\n",
        "\n",
        "\n",
        "def posterize(pil_img, level):\n",
        "  level = int_parameter(sample_level(level), 4)\n",
        "  return ImageOps.posterize(pil_img, 4 - level)\n",
        "\n",
        "\n",
        "def rotate(pil_img, level):\n",
        "  degrees = int_parameter(sample_level(level), 30)\n",
        "  if np.random.uniform() > 0.5:\n",
        "    degrees = -degrees\n",
        "  return pil_img.rotate(degrees, resample=Image.BILINEAR)\n",
        "\n",
        "\n",
        "def solarize(pil_img, level):\n",
        "  level = int_parameter(sample_level(level), 256)\n",
        "  return ImageOps.solarize(pil_img, 256 - level)\n",
        "\n",
        "\n",
        "def shear_x(pil_img, level):\n",
        "  level = float_parameter(sample_level(level), 0.3)\n",
        "  if np.random.uniform() > 0.5:\n",
        "    level = -level\n",
        "  return pil_img.transform((IMAGE_SIZE, IMAGE_SIZE),\n",
        "                           Image.AFFINE, (1, level, 0, 0, 1, 0),\n",
        "                           resample=Image.BILINEAR)\n",
        "\n",
        "\n",
        "def shear_y(pil_img, level):\n",
        "  level = float_parameter(sample_level(level), 0.3)\n",
        "  if np.random.uniform() > 0.5:\n",
        "    level = -level\n",
        "  return pil_img.transform((IMAGE_SIZE, IMAGE_SIZE),\n",
        "                           Image.AFFINE, (1, 0, 0, level, 1, 0),\n",
        "                           resample=Image.BILINEAR)\n",
        "\n",
        "\n",
        "def translate_x(pil_img, level):\n",
        "  level = int_parameter(sample_level(level), IMAGE_SIZE / 3)\n",
        "  if np.random.random() > 0.5:\n",
        "    level = -level\n",
        "  return pil_img.transform((IMAGE_SIZE, IMAGE_SIZE),\n",
        "                           Image.AFFINE, (1, 0, level, 0, 1, 0),\n",
        "                           resample=Image.BILINEAR)\n",
        "\n",
        "\n",
        "def translate_y(pil_img, level):\n",
        "  level = int_parameter(sample_level(level), IMAGE_SIZE / 3)\n",
        "  if np.random.random() > 0.5:\n",
        "    level = -level\n",
        "  return pil_img.transform((IMAGE_SIZE, IMAGE_SIZE),\n",
        "                           Image.AFFINE, (1, 0, 0, 0, 1, level),\n",
        "                           resample=Image.BILINEAR)\n",
        "\n",
        "\n",
        "# operation that overlaps with ImageNet-C's test set\n",
        "def color(pil_img, level):\n",
        "    level = float_parameter(sample_level(level), 1.8) + 0.1\n",
        "    return ImageEnhance.Color(pil_img).enhance(level)\n",
        "\n",
        "\n",
        "# operation that overlaps with ImageNet-C's test set\n",
        "def contrast(pil_img, level):\n",
        "    level = float_parameter(sample_level(level), 1.8) + 0.1\n",
        "    return ImageEnhance.Contrast(pil_img).enhance(level)\n",
        "\n",
        "\n",
        "# operation that overlaps with ImageNet-C's test set\n",
        "def brightness(pil_img, level):\n",
        "    level = float_parameter(sample_level(level), 1.8) + 0.1\n",
        "    return ImageEnhance.Brightness(pil_img).enhance(level)\n",
        "\n",
        "\n",
        "# operation that overlaps with ImageNet-C's test set\n",
        "def sharpness(pil_img, level):\n",
        "    level = float_parameter(sample_level(level), 1.8) + 0.1\n",
        "    return ImageEnhance.Sharpness(pil_img).enhance(level)\n",
        "\n",
        "\n",
        "augmentations = [\n",
        "    autocontrast, equalize, posterize, rotate, solarize, shear_x, shear_y,\n",
        "    translate_x, translate_y\n",
        "]\n",
        "\n",
        "augmentations_all = [\n",
        "    autocontrast, equalize, posterize, rotate, solarize, shear_x, shear_y,\n",
        "    translate_x, translate_y, color, contrast, brightness, sharpness\n",
        "]"
      ],
      "metadata": {
        "id": "sNZr99VEtShx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import random\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class APRecombination(object):\n",
        "    def __init__(self, img_size=32, aug=None):\n",
        "        if aug is None:\n",
        "            augmentations.IMAGE_SIZE = img_size\n",
        "            self.aug_list = augmentations.augmentations\n",
        "        else:\n",
        "            self.aug_list = aug.augmentations\n",
        "\n",
        "    def __call__(self, x):\n",
        "        '''\n",
        "        :param img: (PIL Image): Image\n",
        "        :return: code img (PIL Image): Image\n",
        "        '''\n",
        "\n",
        "        op = np.random.choice(self.aug_list)\n",
        "        x = op(x, 3)\n",
        "\n",
        "        p = random.uniform(0, 1)\n",
        "        if p > 0.5:\n",
        "            return x\n",
        "\n",
        "        x_aug = x.copy()\n",
        "        op = np.random.choice(self.aug_list)\n",
        "        x_aug = op(x_aug, 3)\n",
        "\n",
        "        x = np.array(x).astype(np.uint8) \n",
        "        x_aug = np.array(x_aug).astype(np.uint8)\n",
        "        \n",
        "        fft_1 = np.fft.fftshift(np.fft.fftn(x))\n",
        "        fft_2 = np.fft.fftshift(np.fft.fftn(x_aug))\n",
        "        \n",
        "        abs_1, angle_1 = np.abs(fft_1), np.angle(fft_1)\n",
        "        abs_2, angle_2 = np.abs(fft_2), np.angle(fft_2)\n",
        "\n",
        "        fft_1 = abs_1*np.exp((1j) * angle_2)\n",
        "        fft_2 = abs_2*np.exp((1j) * angle_1)\n",
        "\n",
        "        p = random.uniform(0, 1)\n",
        "\n",
        "        if p > 0.5:\n",
        "            x = np.fft.ifftn(np.fft.ifftshift(fft_1))\n",
        "        else:\n",
        "            x = np.fft.ifftn(np.fft.ifftshift(fft_2))\n",
        "\n",
        "        x = x.astype(np.uint8)\n",
        "        x = Image.fromarray(x)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "iY7qbQxLrzc1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "normalize = transforms.Compose([\n",
        "        transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]),\n",
        "    ])\n",
        "\n",
        "def train_transforms(_transforms):\n",
        "    transforms_list = []\n",
        "    if 'aprs' in _transforms:\n",
        "        print('APRecombination', _transforms)\n",
        "        transforms_list.extend([\n",
        "            transforms.RandomApply([APRecombination()], p=1.0),\n",
        "            transforms.RandomCrop(32, padding=4, fill=128),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "    else:\n",
        "        transforms_list.extend([\n",
        "            transforms.RandomCrop(32, padding=4, fill=128),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    return transforms_list\n",
        "\n",
        "\n",
        "def test_transforms():\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    return test_transform"
      ],
      "metadata": {
        "id": "MjjHKN8ktMKX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import pickle\n",
        "import numpy as np\n",
        "from scipy import signal\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10, CIFAR100, ImageFolder\n",
        "\n",
        "\n",
        "class CIFARC(CIFAR10):\n",
        "    def __init__(\n",
        "            self,\n",
        "            root,\n",
        "            key = 'zoom_blur',\n",
        "            transform = None,\n",
        "            target_transform = None,\n",
        "    ):\n",
        "\n",
        "        super(CIFAR10, self).__init__(root, transform=transform,\n",
        "                                      target_transform=target_transform)\n",
        "\n",
        "        data_path = os.path.join(root, key+'.npy')\n",
        "        labels_path = os.path.join(root, 'labels.npy')\n",
        "\n",
        "        self.data = np.load(data_path)\n",
        "        self.targets = np.load(labels_path)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "class CIFAR10D(object):\n",
        "    def __init__(self, dataroot='', use_gpu=True, num_workers=4, batch_size=128, _transforms='', _eval=False):\n",
        "\n",
        "        transforms_list = train_transforms(_transforms)\n",
        "\n",
        "        train_transform = transforms.Compose(transforms_list)\n",
        "        test_transform = test_transforms()\n",
        "        self.train_transform = train_transform\n",
        "\n",
        "        pin_memory = True if use_gpu else False\n",
        "\n",
        "        data_root = os.path.join(dataroot, 'cifar10')\n",
        "\n",
        "        trainset = CIFAR10(root=data_root, train=True, download=True, transform=train_transform)\n",
        "        \n",
        "        self.train_loader = torch.utils.data.DataLoader(\n",
        "            trainset, batch_size=batch_size, shuffle=True,\n",
        "            num_workers=num_workers, pin_memory=pin_memory,\n",
        "        )\n",
        "        \n",
        "        testset = CIFAR10(root=data_root, train=False, download=True, transform=test_transform)\n",
        "        \n",
        "        self.test_loader = torch.utils.data.DataLoader(\n",
        "            testset, batch_size=batch_size, shuffle=False,\n",
        "            num_workers=num_workers, pin_memory=pin_memory,\n",
        "        )\n",
        "\n",
        "        if _eval:\n",
        "            self.out_loaders = dict()\n",
        "            self.out_keys = ['gaussian_noise', 'shot_noise', 'impulse_noise', 'defocus_blur',\n",
        "                            'glass_blur', 'motion_blur', 'zoom_blur', 'snow', 'frost', 'fog',\n",
        "                            'brightness', 'contrast', 'elastic_transform', 'pixelate',\n",
        "                            'jpeg_compression']\n",
        "\n",
        "            data_root = os.path.join(dataroot, 'CIFAR-10-C')\n",
        "            for key in self.out_keys:\n",
        "                outset = CIFARC(root=data_root, key=key, transform=test_transform)\n",
        "                out_loader = torch.utils.data.DataLoader(\n",
        "                    outset, batch_size=batch_size, shuffle=False,\n",
        "                    num_workers=num_workers, pin_memory=pin_memory,\n",
        "                )\n",
        "                self.out_loaders[key] = out_loader\n",
        "        \n",
        "        self.num_classes = 10\n",
        "\n",
        "class CIFAR100D(object):\n",
        "    def __init__(self, dataroot='', use_gpu=True, num_workers=4, batch_size=128, _transforms='', _eval=False):\n",
        "\n",
        "        transforms_list = train_transforms(_transforms)\n",
        "\n",
        "        train_transform = transforms.Compose(transforms_list)\n",
        "        test_transform = test_transforms()\n",
        "        self.train_transform = train_transform\n",
        "\n",
        "        pin_memory = True if use_gpu else False\n",
        "\n",
        "        data_root = os.path.join(dataroot, 'cifar100')\n",
        "\n",
        "        trainset = CIFAR100(root=data_root, train=True, download=True, transform=train_transform)\n",
        "        \n",
        "        self.train_loader = torch.utils.data.DataLoader(\n",
        "            trainset, batch_size=batch_size, shuffle=True,\n",
        "            num_workers=num_workers, pin_memory=pin_memory,\n",
        "        )\n",
        "        \n",
        "        testset = CIFAR100(root=data_root, train=False, download=True, transform=test_transform)\n",
        "        \n",
        "        self.test_loader = torch.utils.data.DataLoader(\n",
        "            testset, batch_size=batch_size, shuffle=False,\n",
        "            num_workers=num_workers, pin_memory=pin_memory,\n",
        "        )\n",
        "\n",
        "        if _eval:\n",
        "            self.out_loaders = dict()\n",
        "            self.out_keys = ['gaussian_noise', 'shot_noise', 'impulse_noise', 'defocus_blur',\n",
        "                            'glass_blur', 'motion_blur', 'zoom_blur', 'snow', 'frost', 'fog',\n",
        "                            'brightness', 'contrast', 'elastic_transform', 'pixelate',\n",
        "                            'jpeg_compression']\n",
        "\n",
        "            data_root = os.path.join(dataroot, 'CIFAR-100-C')\n",
        "            for key in self.out_keys:\n",
        "                outset = CIFARC(root=data_root, key=key, transform=test_transform)\n",
        "                out_loader = torch.utils.data.DataLoader(\n",
        "                    outset, batch_size=batch_size, shuffle=False,\n",
        "                    num_workers=num_workers, pin_memory=pin_memory,\n",
        "                )\n",
        "                self.out_loaders[key] = out_loader\n",
        "\n",
        "        self.num_classes = 100"
      ],
      "metadata": {
        "id": "0KTRpO0HtZxi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''ResNet in PyTorch.\n",
        "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, rf=False, _eval=False):\n",
        "        if _eval:\n",
        "            self.eval()\n",
        "        else:\n",
        "            self.train()\n",
        "\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        y = self.linear(out)\n",
        "        if rf:\n",
        "            return out, y\n",
        "        return y\n",
        "\n",
        "\n",
        "def ResNet18(num_classes):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])"
      ],
      "metadata": {
        "id": "TnzLuRymtioH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2019 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"AllConv implementation (https://arxiv.org/abs/1412.6806).\"\"\"\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.sigmoid(1.702 * x) * x\n",
        "\n",
        "\n",
        "def make_layers(cfg):\n",
        "  \"\"\"Create a single layer.\"\"\"\n",
        "  layers = []\n",
        "  in_channels = 3\n",
        "  for v in cfg:\n",
        "    if v == 'Md':\n",
        "      layers += [nn.MaxPool2d(kernel_size=2, stride=2), nn.Dropout(p=0.5)]\n",
        "    elif v == 'A':\n",
        "      layers += [nn.AvgPool2d(kernel_size=8)]\n",
        "    elif v == 'NIN':\n",
        "      conv2d = nn.Conv2d(in_channels, in_channels, kernel_size=1, padding=1)\n",
        "      layers += [conv2d, nn.BatchNorm2d(in_channels), GELU()]\n",
        "    elif v == 'nopad':\n",
        "      conv2d = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=0)\n",
        "      layers += [conv2d, nn.BatchNorm2d(in_channels), GELU()]\n",
        "    else:\n",
        "      conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "      layers += [conv2d, nn.BatchNorm2d(v), GELU()]\n",
        "      in_channels = v\n",
        "  return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class AllConvNet(nn.Module):\n",
        "  \"\"\"AllConvNet main class.\"\"\"\n",
        "\n",
        "  def __init__(self, num_classes):\n",
        "    super(AllConvNet, self).__init__()\n",
        "\n",
        "    self.num_classes = num_classes\n",
        "    self.width1, w1 = 96, 96\n",
        "    self.width2, w2 = 192, 192\n",
        "\n",
        "    self.features = make_layers(\n",
        "        [w1, w1, w1, 'Md', w2, w2, w2, 'Md', 'nopad', 'NIN', 'NIN', 'A'])\n",
        "    self.classifier = nn.Linear(self.width2, num_classes)\n",
        "\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        m.weight.data.normal_(0, math.sqrt(2. / n))  # He initialization\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        m.bias.data.zero_()\n",
        "\n",
        "  def forward(self, x, rf=False, _eval=False):\n",
        "    if _eval:\n",
        "        # switch to eval mode\n",
        "        self.eval()\n",
        "    else:\n",
        "        self.train()\n",
        "\n",
        "    x = self.features(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    y = self.classifier(x)\n",
        "    if rf:\n",
        "        return x, y\n",
        "    return y"
      ],
      "metadata": {
        "id": "pDXheT0_soxd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"DenseNet implementation (https://arxiv.org/abs/1608.06993).\"\"\"\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "  \"\"\"Bottleneck block for DenseNet.\"\"\"\n",
        "\n",
        "  def __init__(self, n_channels, growth_rate):\n",
        "    super(Bottleneck, self).__init__()\n",
        "    inter_channels = 4 * growth_rate\n",
        "    self.bn1 = nn.BatchNorm2d(n_channels)\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        n_channels, inter_channels, kernel_size=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(inter_channels)\n",
        "    self.conv2 = nn.Conv2d(\n",
        "        inter_channels, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(F.relu(self.bn1(x)))\n",
        "    out = self.conv2(F.relu(self.bn2(out)))\n",
        "    out = torch.cat((x, out), 1)\n",
        "    return out\n",
        "\n",
        "\n",
        "class SingleLayer(nn.Module):\n",
        "  \"\"\"Layer container for blocks.\"\"\"\n",
        "\n",
        "  def __init__(self, n_channels, growth_rate):\n",
        "    super(SingleLayer, self).__init__()\n",
        "    self.bn1 = nn.BatchNorm2d(n_channels)\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        n_channels, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(F.relu(self.bn1(x)))\n",
        "    out = torch.cat((x, out), 1)\n",
        "    return out\n",
        "\n",
        "\n",
        "class Transition(nn.Module):\n",
        "  \"\"\"Transition block.\"\"\"\n",
        "\n",
        "  def __init__(self, n_channels, n_out_channels):\n",
        "    super(Transition, self).__init__()\n",
        "    self.bn1 = nn.BatchNorm2d(n_channels)\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        n_channels, n_out_channels, kernel_size=1, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(F.relu(self.bn1(x)))\n",
        "    out = F.avg_pool2d(out, 2)\n",
        "    return out\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "  \"\"\"DenseNet main class.\"\"\"\n",
        "\n",
        "  def __init__(self, growth_rate, depth, reduction, n_classes, bottleneck):\n",
        "    super(DenseNet, self).__init__()\n",
        "\n",
        "    if bottleneck:\n",
        "      n_dense_blocks = int((depth - 4) / 6)\n",
        "    else:\n",
        "      n_dense_blocks = int((depth - 4) / 3)\n",
        "\n",
        "    n_channels = 2 * growth_rate\n",
        "    self.conv1 = nn.Conv2d(3, n_channels, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "    self.dense1 = self._make_dense(n_channels, growth_rate, n_dense_blocks,\n",
        "                                   bottleneck)\n",
        "    n_channels += n_dense_blocks * growth_rate\n",
        "    n_out_channels = int(math.floor(n_channels * reduction))\n",
        "    self.trans1 = Transition(n_channels, n_out_channels)\n",
        "\n",
        "    n_channels = n_out_channels\n",
        "    self.dense2 = self._make_dense(n_channels, growth_rate, n_dense_blocks,\n",
        "                                   bottleneck)\n",
        "    n_channels += n_dense_blocks * growth_rate\n",
        "    n_out_channels = int(math.floor(n_channels * reduction))\n",
        "    self.trans2 = Transition(n_channels, n_out_channels)\n",
        "\n",
        "    n_channels = n_out_channels\n",
        "    self.dense3 = self._make_dense(n_channels, growth_rate, n_dense_blocks,\n",
        "                                   bottleneck)\n",
        "    n_channels += n_dense_blocks * growth_rate\n",
        "\n",
        "    self.bn1 = nn.BatchNorm2d(n_channels)\n",
        "    self.fc = nn.Linear(n_channels, n_classes)\n",
        "\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        m.bias.data.zero_()\n",
        "\n",
        "  def _make_dense(self, n_channels, growth_rate, n_dense_blocks, bottleneck):\n",
        "    layers = []\n",
        "    for _ in range(int(n_dense_blocks)):\n",
        "      if bottleneck:\n",
        "        layers.append(Bottleneck(n_channels, growth_rate))\n",
        "      else:\n",
        "        layers.append(SingleLayer(n_channels, growth_rate))\n",
        "      n_channels += growth_rate\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x, rf=False, _eval=False):\n",
        "    if _eval:\n",
        "        # switch to eval mode\n",
        "        self.eval()\n",
        "    else:\n",
        "        self.train()\n",
        "    out = self.conv1(x)\n",
        "    out = self.trans1(self.dense1(out))\n",
        "    out = self.trans2(self.dense2(out))\n",
        "    out = self.dense3(out)\n",
        "    out = torch.squeeze(F.avg_pool2d(F.relu(self.bn1(out)), 8))\n",
        "    y = self.fc(out)\n",
        "    if rf:\n",
        "        return out, y\n",
        "    return y\n",
        "\n",
        "def densenet(growth_rate=12, depth=40, num_classes=10):\n",
        "  model = DenseNet(growth_rate, depth, 1., num_classes, False)\n",
        "  return model"
      ],
      "metadata": {
        "id": "HA6nd4hassc-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"ResNeXt implementation (https://arxiv.org/abs/1611.05431).\"\"\"\n",
        "import math\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ResNeXtBottleneck(nn.Module):\n",
        "  \"\"\"ResNeXt Bottleneck Block type C (https://github.com/facebookresearch/ResNeXt/blob/master/models/resnext.lua).\"\"\"\n",
        "  expansion = 4\n",
        "\n",
        "  def __init__(self,\n",
        "               inplanes,\n",
        "               planes,\n",
        "               cardinality,\n",
        "               base_width,\n",
        "               stride=1,\n",
        "               downsample=None):\n",
        "    super(ResNeXtBottleneck, self).__init__()\n",
        "\n",
        "    dim = int(math.floor(planes * (base_width / 64.0)))\n",
        "\n",
        "    self.conv_reduce = nn.Conv2d(\n",
        "        inplanes,\n",
        "        dim * cardinality,\n",
        "        kernel_size=1,\n",
        "        stride=1,\n",
        "        padding=0,\n",
        "        bias=False)\n",
        "    self.bn_reduce = nn.BatchNorm2d(dim * cardinality)\n",
        "\n",
        "    self.conv_conv = nn.Conv2d(\n",
        "        dim * cardinality,\n",
        "        dim * cardinality,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=1,\n",
        "        groups=cardinality,\n",
        "        bias=False)\n",
        "    self.bn = nn.BatchNorm2d(dim * cardinality)\n",
        "\n",
        "    self.conv_expand = nn.Conv2d(\n",
        "        dim * cardinality,\n",
        "        planes * 4,\n",
        "        kernel_size=1,\n",
        "        stride=1,\n",
        "        padding=0,\n",
        "        bias=False)\n",
        "    self.bn_expand = nn.BatchNorm2d(planes * 4)\n",
        "\n",
        "    self.downsample = downsample\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "\n",
        "    bottleneck = self.conv_reduce(x)\n",
        "    bottleneck = F.relu(self.bn_reduce(bottleneck), inplace=True)\n",
        "\n",
        "    bottleneck = self.conv_conv(bottleneck)\n",
        "    bottleneck = F.relu(self.bn(bottleneck), inplace=True)\n",
        "\n",
        "    bottleneck = self.conv_expand(bottleneck)\n",
        "    bottleneck = self.bn_expand(bottleneck)\n",
        "\n",
        "    if self.downsample is not None:\n",
        "      residual = self.downsample(x)\n",
        "\n",
        "    return F.relu(residual + bottleneck, inplace=True)\n",
        "\n",
        "\n",
        "class CifarResNeXt(nn.Module):\n",
        "  \"\"\"ResNext optimized for the Cifar dataset, as specified in https://arxiv.org/pdf/1611.05431.pdf.\"\"\"\n",
        "\n",
        "  def __init__(self, block, depth, cardinality, base_width, num_classes):\n",
        "    super(CifarResNeXt, self).__init__()\n",
        "\n",
        "    # Model type specifies number of layers for CIFAR-10 and CIFAR-100 model\n",
        "    assert (depth - 2) % 9 == 0, 'depth should be one of 29, 38, 47, 56, 101'\n",
        "    layer_blocks = (depth - 2) // 9\n",
        "\n",
        "    self.cardinality = cardinality\n",
        "    self.base_width = base_width\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "    self.conv_1_3x3 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
        "    self.bn_1 = nn.BatchNorm2d(64)\n",
        "\n",
        "    self.inplanes = 64\n",
        "    self.stage_1 = self._make_layer(block, 64, layer_blocks, 1)\n",
        "    self.stage_2 = self._make_layer(block, 128, layer_blocks, 2)\n",
        "    self.stage_3 = self._make_layer(block, 256, layer_blocks, 2)\n",
        "    self.avgpool = nn.AvgPool2d(8)\n",
        "    self.classifier = nn.Linear(256 * block.expansion, num_classes)\n",
        "\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        init.kaiming_normal(m.weight)\n",
        "        m.bias.data.zero_()\n",
        "\n",
        "  def _make_layer(self, block, planes, blocks, stride=1):\n",
        "    downsample = None\n",
        "    if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "      downsample = nn.Sequential(\n",
        "          nn.Conv2d(\n",
        "              self.inplanes,\n",
        "              planes * block.expansion,\n",
        "              kernel_size=1,\n",
        "              stride=stride,\n",
        "              bias=False),\n",
        "          nn.BatchNorm2d(planes * block.expansion),\n",
        "      )\n",
        "\n",
        "    layers = []\n",
        "    layers.append(\n",
        "        block(self.inplanes, planes, self.cardinality, self.base_width, stride,\n",
        "              downsample))\n",
        "    self.inplanes = planes * block.expansion\n",
        "    for _ in range(1, blocks):\n",
        "      layers.append(\n",
        "          block(self.inplanes, planes, self.cardinality, self.base_width))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x, rf=False, _eval=False):\n",
        "    if _eval:\n",
        "        # switch to eval mode\n",
        "        self.eval()\n",
        "    else:\n",
        "        self.train()\n",
        "    x = self.conv_1_3x3(x)\n",
        "    x = F.relu(self.bn_1(x), inplace=True)\n",
        "    x = self.stage_1(x)\n",
        "    x = self.stage_2(x)\n",
        "    x = self.stage_3(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    y = self.classifier(x)\n",
        "\n",
        "    if rf:\n",
        "        return x, y\n",
        "    return y\n",
        "\n",
        "def resnext29(num_classes=10, cardinality=4, base_width=32):\n",
        "  model = CifarResNeXt(ResNeXtBottleneck, 29, cardinality, base_width,\n",
        "                       num_classes)\n",
        "  return model"
      ],
      "metadata": {
        "id": "FLqt2DoJsvPZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"WideResNet implementation (https://arxiv.org/abs/1605.07146).\"\"\"\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "  \"\"\"Basic ResNet block.\"\"\"\n",
        "\n",
        "  def __init__(self, in_planes, out_planes, stride, drop_rate=0.0):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "    self.relu1 = nn.ReLU(inplace=True)\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=1,\n",
        "        bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(out_planes)\n",
        "    self.relu2 = nn.ReLU(inplace=True)\n",
        "    self.conv2 = nn.Conv2d(\n",
        "        out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.drop_rate = drop_rate\n",
        "    self.is_in_equal_out = (in_planes == out_planes)\n",
        "    self.conv_shortcut = (not self.is_in_equal_out) and nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=1,\n",
        "        stride=stride,\n",
        "        padding=0,\n",
        "        bias=False) or None\n",
        "\n",
        "  def forward(self, x):\n",
        "    if not self.is_in_equal_out:\n",
        "      x = self.relu1(self.bn1(x))\n",
        "    else:\n",
        "      out = self.relu1(self.bn1(x))\n",
        "    if self.is_in_equal_out:\n",
        "      out = self.relu2(self.bn2(self.conv1(out)))\n",
        "    else:\n",
        "      out = self.relu2(self.bn2(self.conv1(x)))\n",
        "    if self.drop_rate > 0:\n",
        "      out = F.dropout(out, p=self.drop_rate, training=self.training)\n",
        "    out = self.conv2(out)\n",
        "    if not self.is_in_equal_out:\n",
        "      return torch.add(self.conv_shortcut(x), out)\n",
        "    else:\n",
        "      return torch.add(x, out)\n",
        "\n",
        "\n",
        "class NetworkBlock(nn.Module):\n",
        "  \"\"\"Layer container for blocks.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               nb_layers,\n",
        "               in_planes,\n",
        "               out_planes,\n",
        "               block,\n",
        "               stride,\n",
        "               drop_rate=0.0):\n",
        "    super(NetworkBlock, self).__init__()\n",
        "    self.layer = self._make_layer(block, in_planes, out_planes, nb_layers,\n",
        "                                  stride, drop_rate)\n",
        "\n",
        "  def _make_layer(self, block, in_planes, out_planes, nb_layers, stride,\n",
        "                  drop_rate):\n",
        "    layers = []\n",
        "    for i in range(nb_layers):\n",
        "      layers.append(\n",
        "          block(i == 0 and in_planes or out_planes, out_planes,\n",
        "                i == 0 and stride or 1, drop_rate))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer(x)\n",
        "\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "  \"\"\"WideResNet class.\"\"\"\n",
        "\n",
        "  def __init__(self, depth, num_classes, widen_factor=1, drop_rate=0.0):\n",
        "    super(WideResNet, self).__init__()\n",
        "    n_channels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n",
        "    assert (depth - 4) % 6 == 0\n",
        "    n = (depth - 4) // 6\n",
        "    block = BasicBlock\n",
        "    # 1st conv before any network block\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        3, n_channels[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    # 1st block\n",
        "    self.block1 = NetworkBlock(n, n_channels[0], n_channels[1], block, 1,\n",
        "                               drop_rate)\n",
        "    # 2nd block\n",
        "    self.block2 = NetworkBlock(n, n_channels[1], n_channels[2], block, 2,\n",
        "                               drop_rate)\n",
        "    # 3rd block\n",
        "    self.block3 = NetworkBlock(n, n_channels[2], n_channels[3], block, 2,\n",
        "                               drop_rate)\n",
        "    # global average pooling and classifier\n",
        "    self.bn1 = nn.BatchNorm2d(n_channels[3])\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.fc = nn.Linear(n_channels[3], num_classes)\n",
        "    self.n_channels = n_channels[3]\n",
        "\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        m.bias.data.zero_()\n",
        "\n",
        "  def forward(self, x, rf=False, _eval=False):\n",
        "    if _eval:\n",
        "        # switch to eval mode\n",
        "        self.eval()\n",
        "    else:\n",
        "        self.train()\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.block1(out)\n",
        "    out = self.block2(out)\n",
        "    out = self.block3(out)\n",
        "    out = self.relu(self.bn1(out))\n",
        "    out = F.avg_pool2d(out, 8)\n",
        "    out = out.view(-1, self.n_channels)\n",
        "    y =  self.fc(out)\n",
        "\n",
        "    if rf:\n",
        "        return out, y\n",
        "    return y"
      ],
      "metadata": {
        "id": "4gfUmcamsxhr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import os.path as osp\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch.fft\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def test(net, criterion, testloader, outloader, attack=None, epoch=None, **options):\n",
        "    net.eval()\n",
        "    correct, total, adv_correct = 0, 0, 0\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    _pred_k, _pred_u, _labels = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, labels in testloader:\n",
        "            if options['use_gpu']:\n",
        "                data, labels = data.cuda(), labels.cuda()             \n",
        "                data = normalize(data)\n",
        "                logits = net(data, _eval=True)\n",
        "                predictions = logits.data.max(1)[1]\n",
        "                total += labels.size(0)\n",
        "                correct += (predictions == labels.data).sum()\n",
        "            \n",
        "                _pred_k.append(logits.data.cpu().numpy())\n",
        "                _labels.append(labels.data.cpu().numpy())\n",
        "\n",
        "        for batch_idx, (data, labels) in enumerate(outloader):\n",
        "            if options['use_gpu']:\n",
        "                data, labels = data.cuda(), labels.cuda()\n",
        "                data = normalize(data)\n",
        "            with torch.set_grad_enabled(False):\n",
        "                logits = net(data, _eval=True)\n",
        "                _pred_u.append(logits.data.cpu().numpy())\n",
        "\n",
        "    _pred_k = np.concatenate(_pred_k, 0)\n",
        "    _pred_u = np.concatenate(_pred_u, 0)\n",
        "    _labels = np.concatenate(_labels, 0)\n",
        "    \n",
        "    # # Out-of-Distribution detction evaluation\n",
        "    x1, x2 = np.max(_pred_k, axis=1), np.max(_pred_u, axis=1)\n",
        "    results = metric_ood(x1, x2)['Bas']\n",
        "\n",
        "    # Accuracy\n",
        "    acc = float(correct) * 100. / float(total)\n",
        "    results['ACC'] = acc\n",
        "\n",
        "    print('Acc: {:.5f}'.format(acc))\n",
        "\n",
        "    return results\n",
        "\n",
        "def test_robustness(net, criterion, testloader, epoch=None, label='', **options):\n",
        "    net.eval()\n",
        "    results = dict()\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, labels in testloader:\n",
        "            if options['use_gpu']:\n",
        "                data, labels = data.cuda(), labels.cuda()\n",
        "                data = normalize(data)\n",
        "            with torch.set_grad_enabled(False):\n",
        "                logits = net(data, _eval=True)\n",
        "                predictions = logits.data.max(1)[1]\n",
        "                total += labels.size(0)\n",
        "                correct += (predictions == labels.data).sum()\n",
        "\n",
        "    # Accuracy\n",
        "    acc = float(correct) * 100. / float(total)\n",
        "    results['ACC'] = acc\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "tbsgoVV10myW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "def get_curve_online(known, novel, stypes = ['Bas']):\n",
        "    tp, fp = dict(), dict()\n",
        "    tnr_at_tpr95 = dict()\n",
        "    for stype in stypes:\n",
        "        known.sort()\n",
        "        novel.sort()\n",
        "        end = np.max([np.max(known), np.max(novel)])\n",
        "        start = np.min([np.min(known),np.min(novel)])\n",
        "        num_k = known.shape[0]\n",
        "        num_n = novel.shape[0]\n",
        "        tp[stype] = -np.ones([num_k+num_n+1], dtype=int)\n",
        "        fp[stype] = -np.ones([num_k+num_n+1], dtype=int)\n",
        "        tp[stype][0], fp[stype][0] = num_k, num_n\n",
        "        k, n = 0, 0\n",
        "        for l in range(num_k+num_n):\n",
        "            if k == num_k:\n",
        "                tp[stype][l+1:] = tp[stype][l]\n",
        "                fp[stype][l+1:] = np.arange(fp[stype][l]-1, -1, -1)\n",
        "                break\n",
        "            elif n == num_n:\n",
        "                tp[stype][l+1:] = np.arange(tp[stype][l]-1, -1, -1)\n",
        "                fp[stype][l+1:] = fp[stype][l]\n",
        "                break\n",
        "            else:\n",
        "                if novel[n] < known[k]:\n",
        "                    n += 1\n",
        "                    tp[stype][l+1] = tp[stype][l]\n",
        "                    fp[stype][l+1] = fp[stype][l] - 1\n",
        "                else:\n",
        "                    k += 1\n",
        "                    tp[stype][l+1] = tp[stype][l] - 1\n",
        "                    fp[stype][l+1] = fp[stype][l]\n",
        "        tpr95_pos = np.abs(tp[stype] / num_k - .95).argmin()\n",
        "        tnr_at_tpr95[stype] = 1. - fp[stype][tpr95_pos] / num_n\n",
        "    return tp, fp, tnr_at_tpr95\n",
        "\n",
        "def metric_ood(x1, x2, stypes = ['Bas'], verbose=True):\n",
        "    tp, fp, tnr_at_tpr95 = get_curve_online(x1, x2, stypes)\n",
        "    results = dict()\n",
        "    mtypes = ['TNR', 'AUROC', 'DTACC', 'AUIN', 'AUOUT']\n",
        "    if verbose:\n",
        "        print('      ', end='')\n",
        "        for mtype in mtypes:\n",
        "            print(' {mtype:6s}'.format(mtype=mtype), end='')\n",
        "        print('')\n",
        "        \n",
        "    for stype in stypes:\n",
        "        if verbose:\n",
        "            print('{stype:5s} '.format(stype=stype), end='')\n",
        "        results[stype] = dict()\n",
        "        \n",
        "        # TNR\n",
        "        mtype = 'TNR'\n",
        "        results[stype][mtype] = 100.*tnr_at_tpr95[stype]\n",
        "        if verbose:\n",
        "            print(' {val:6.3f}'.format(val=results[stype][mtype]), end='')\n",
        "        \n",
        "        # AUROC\n",
        "        mtype = 'AUROC'\n",
        "        tpr = np.concatenate([[1.], tp[stype]/tp[stype][0], [0.]])\n",
        "        fpr = np.concatenate([[1.], fp[stype]/fp[stype][0], [0.]])\n",
        "        results[stype][mtype] = 100.*(-np.trapz(1.-fpr, tpr))\n",
        "        if verbose:\n",
        "            print(' {val:6.3f}'.format(val=results[stype][mtype]), end='')\n",
        "        \n",
        "        # DTACC\n",
        "        mtype = 'DTACC'\n",
        "        results[stype][mtype] = 100.*(.5 * (tp[stype]/tp[stype][0] + 1.-fp[stype]/fp[stype][0]).max())\n",
        "        if verbose:\n",
        "            print(' {val:6.3f}'.format(val=results[stype][mtype]), end='')\n",
        "        \n",
        "        # AUIN\n",
        "        mtype = 'AUIN'\n",
        "        denom = tp[stype]+fp[stype]\n",
        "        denom[denom == 0.] = -1.\n",
        "        pin_ind = np.concatenate([[True], denom > 0., [True]])\n",
        "        pin = np.concatenate([[.5], tp[stype]/denom, [0.]])\n",
        "        results[stype][mtype] = 100.*(-np.trapz(pin[pin_ind], tpr[pin_ind]))\n",
        "        if verbose:\n",
        "            print(' {val:6.3f}'.format(val=results[stype][mtype]), end='')\n",
        "        \n",
        "        # AUOUT\n",
        "        mtype = 'AUOUT'\n",
        "        denom = tp[stype][0]-tp[stype]+fp[stype][0]-fp[stype]\n",
        "        denom[denom == 0.] = -1.\n",
        "        pout_ind = np.concatenate([[True], denom > 0., [True]])\n",
        "        pout = np.concatenate([[0.], (fp[stype][0]-fp[stype])/denom, [.5]])\n",
        "        results[stype][mtype] = 100.*(np.trapz(pout[pout_ind], 1.-fpr[pout_ind]))\n",
        "        if verbose:\n",
        "            print(' {val:6.3f}'.format(val=results[stype][mtype]), end='')\n",
        "            print('')\n",
        "    \n",
        "    return results\n",
        "\n",
        "def compute_oscr(pred_k, pred_u, labels):\n",
        "    x1, x2 = np.max(pred_k, axis=1), np.max(pred_u, axis=1)\n",
        "    pred = np.argmax(pred_k, axis=1)\n",
        "    correct = (pred == labels)\n",
        "    m_x1 = np.zeros(len(x1))\n",
        "    m_x1[pred == labels] = 1\n",
        "    k_target = np.concatenate((m_x1, np.zeros(len(x2))), axis=0)\n",
        "    u_target = np.concatenate((np.zeros(len(x1)), np.ones(len(x2))), axis=0)\n",
        "    predict = np.concatenate((x1, x2), axis=0)\n",
        "    n = len(predict)\n",
        "\n",
        "    # Cutoffs are of prediction values\n",
        "    \n",
        "    CCR = [0 for x in range(n+2)]\n",
        "    FPR = [0 for x in range(n+2)] \n",
        "\n",
        "    idx = predict.argsort()\n",
        "\n",
        "    s_k_target = k_target[idx]\n",
        "    s_u_target = u_target[idx]\n",
        "\n",
        "    for k in range(n-1):\n",
        "        CC = s_k_target[k+1:].sum()\n",
        "        FP = s_u_target[k:].sum()\n",
        "\n",
        "        # True\tPositive Rate\n",
        "        CCR[k] = float(CC) / float(len(x1))\n",
        "        # False Positive Rate\n",
        "        FPR[k] = float(FP) / float(len(x2))\n",
        "\n",
        "    CCR[n] = 0.0\n",
        "    FPR[n] = 0.0\n",
        "    CCR[n+1] = 1.0\n",
        "    FPR[n+1] = 1.0\n",
        "\n",
        "    # Positions of ROC curve (FPR, TPR)\n",
        "    ROC = sorted(zip(FPR, CCR), reverse=True)\n",
        "\n",
        "    OSCR = 0\n",
        "\n",
        "    # Compute AUROC Using Trapezoidal Rule\n",
        "    for j in range(n+1):\n",
        "        h =   ROC[j][0] - ROC[j+1][0]\n",
        "        w =  (ROC[j][1] + ROC[j+1][1]) / 2.0\n",
        "\n",
        "        OSCR = OSCR + h*w\n",
        "\n",
        "    return OSCR\n"
      ],
      "metadata": {
        "id": "NC3UcZuv0wOh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mkdir_if_missing(directory):\n",
        "    if not osp.exists(directory):\n",
        "        try:\n",
        "            os.makedirs(directory)\n",
        "        except OSError as e:\n",
        "            if e.errno != errno.EEXIST:\n",
        "                raise\n",
        "def save_networks(networks, result_dir, name='', loss='', criterion=None):\n",
        "    mkdir_if_missing(osp.join(result_dir, 'checkpoints'))\n",
        "    weights = networks.state_dict()\n",
        "    filename = '{}/checkpoints/{}_{}.pth'.format(result_dir, name, loss)\n",
        "    torch.save(weights, filename)\n",
        "    if criterion:\n",
        "        weights = criterion.state_dict()\n",
        "        filename = '{}/checkpoints/{}_{}_criterion.pth'.format(result_dir, name, loss)\n",
        "        torch.save(weights, filename)\n",
        "\n",
        "def load_networks(networks, result_dir, name='', loss='', criterion=None):\n",
        "    weights = networks.state_dict()\n",
        "    filename = '{}/checkpoints/{}_{}.pth'.format(result_dir, name, loss)\n",
        "    networks.load_state_dict(torch.load(filename))\n",
        "    if criterion:\n",
        "        weights = criterion.state_dict()\n",
        "        filename = '{}/checkpoints/{}_{}_criterion.pth'.format(result_dir, name, loss)\n",
        "        criterion.load_state_dict(torch.load(filename))\n",
        "\n",
        "    return networks, criterion"
      ],
      "metadata": {
        "id": "PJYT7Nd10ALr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def mix_data(x, use_cuda=True, prob=0.6):\n",
        "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
        "\n",
        "    p = random.uniform(0, 1)\n",
        "\n",
        "    if p > prob:\n",
        "        return x\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    if use_cuda:\n",
        "        index = torch.randperm(batch_size).cuda()\n",
        "    else:\n",
        "        index = torch.randperm(batch_size)\n",
        "\n",
        "    fft_1 = torch.fft.fftn(x, dim=(1,2,3))\n",
        "    abs_1, angle_1 = torch.abs(fft_1), torch.angle(fft_1)\n",
        "\n",
        "    fft_2 = torch.fft.fftn(x[index, :], dim=(1,2,3))\n",
        "    abs_2, angle_2 = torch.abs(fft_2), torch.angle(fft_2)\n",
        "\n",
        "    fft_1 = abs_2*torch.exp((1j) * angle_1)\n",
        "\n",
        "    mixed_x = torch.fft.ifftn(fft_1, dim=(1,2,3)).float()\n",
        "\n",
        "    return mixed_x\n",
        "\n",
        "\n",
        "def train(net, criterion, optimizer, trainloader, epoch=None, **options):\n",
        "    net.train()\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    loss_all = 0\n",
        "    for batch_idx, (data, labels) in enumerate(trainloader):\n",
        "        if options['use_gpu']:\n",
        "            inputs, targets = data.cuda(), labels.cuda()\n",
        "\n",
        "        inputs_mix = mix_data(inputs)\n",
        "        inputs_mix = Variable(inputs_mix)\n",
        "        batch_size = inputs.size(0)\n",
        "        inputs, inputs_mix = normalize(inputs), normalize(inputs_mix)\n",
        "\n",
        "        inputs = torch.cat([inputs, inputs_mix], 0)\n",
        "\n",
        "        with torch.set_grad_enabled(True):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            _, y = net(inputs, True)\n",
        "            loss = criterion(y[:batch_size], targets) + criterion(y[batch_size:], targets)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        losses.update(loss.item(), targets.size(0))\n",
        "\n",
        "        if (batch_idx+1) % options['print_freq'] == 0:\n",
        "            print(\"Batch {}/{}\\t Loss {:.6f} ({:.6f})\" \\\n",
        "                  .format(batch_idx+1, len(trainloader), losses.val, losses.avg))\n",
        "        \n",
        "        loss_all += losses.avg\n",
        "\n",
        "    return loss_all"
      ],
      "metadata": {
        "id": "vUp5pEUm2Xk1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value.\n",
        "       \n",
        "       Code imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "class Logger(object):\n",
        "    \"\"\"\n",
        "    Write console output to external text file.\n",
        "    \n",
        "    Code imported from https://github.com/Cysu/open-reid/blob/master/reid/utils/logging.py.\n",
        "    \"\"\"\n",
        "    def __init__(self, fpath=None):\n",
        "        self.console = sys.stdout\n",
        "        self.file = None\n",
        "        if fpath is not None:\n",
        "            mkdir_if_missing(os.path.dirname(fpath))\n",
        "            self.file = open(fpath, 'w')\n",
        "\n",
        "    def __del__(self):\n",
        "        self.close()\n",
        "\n",
        "    def __enter__(self):\n",
        "        pass\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        self.close()\n",
        "\n",
        "    def write(self, msg):\n",
        "        self.console.write(msg)\n",
        "        if self.file is not None:\n",
        "            self.file.write(msg)\n",
        "\n",
        "    def flush(self):\n",
        "        self.console.flush()\n",
        "        if self.file is not None:\n",
        "            self.file.flush()\n",
        "            os.fsync(self.file.fileno())\n",
        "\n",
        "    def close(self):\n",
        "        self.console.close()\n",
        "        if self.file is not None:\n",
        "            self.file.close()"
      ],
      "metadata": {
        "id": "8laTrZuZ2eJc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./outf\n",
        "!mkdir ./data\n",
        "\n",
        "!wget https://zenodo.org/record/2535967/files/CIFAR-10-C.tar"
      ],
      "metadata": {
        "id": "_TJg-jZAhwmG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19ca4a39-7533-4137-81f8-762ab48735bc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-24 15:19:10--  https://zenodo.org/record/2535967/files/CIFAR-10-C.tar\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.124.72\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.124.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2918471680 (2.7G) [application/octet-stream]\n",
            "Saving to: ‘CIFAR-10-C.tar’\n",
            "\n",
            "CIFAR-10-C.tar      100%[===================>]   2.72G  1.17MB/s    in 51m 24s \n",
            "\n",
            "2023-04-24 16:10:37 (924 KB/s) - ‘CIFAR-10-C.tar’ saved [2918471680/2918471680]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvf CIFAR-10-C.tar -C ./data"
      ],
      "metadata": {
        "id": "w5gRM-dwntwX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3809d56c-be02-42a8-ec02-405f68d42711"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIFAR-10-C/\n",
            "CIFAR-10-C/fog.npy\n",
            "CIFAR-10-C/jpeg_compression.npy\n",
            "CIFAR-10-C/zoom_blur.npy\n",
            "CIFAR-10-C/speckle_noise.npy\n",
            "CIFAR-10-C/glass_blur.npy\n",
            "CIFAR-10-C/spatter.npy\n",
            "CIFAR-10-C/shot_noise.npy\n",
            "CIFAR-10-C/defocus_blur.npy\n",
            "CIFAR-10-C/elastic_transform.npy\n",
            "CIFAR-10-C/gaussian_blur.npy\n",
            "CIFAR-10-C/frost.npy\n",
            "CIFAR-10-C/saturate.npy\n",
            "CIFAR-10-C/brightness.npy\n",
            "CIFAR-10-C/snow.npy\n",
            "CIFAR-10-C/gaussian_noise.npy\n",
            "CIFAR-10-C/motion_blur.npy\n",
            "CIFAR-10-C/contrast.npy\n",
            "CIFAR-10-C/impulse_noise.npy\n",
            "CIFAR-10-C/labels.npy\n",
            "CIFAR-10-C/pixelate.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ./data/CIFAR-10-C/"
      ],
      "metadata": {
        "id": "3J0OI71FmZwy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a670e59d-8110-490e-dffd-e44ad20bffd7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "brightness.npy\t       gaussian_noise.npy    saturate.npy\n",
            "contrast.npy\t       glass_blur.npy\t     shot_noise.npy\n",
            "defocus_blur.npy       impulse_noise.npy     snow.npy\n",
            "elastic_transform.npy  jpeg_compression.npy  spatter.npy\n",
            "fog.npy\t\t       labels.npy\t     speckle_noise.npy\n",
            "frost.npy\t       motion_blur.npy\t     zoom_blur.npy\n",
            "gaussian_blur.npy      pixelate.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "options = {\n",
        "    'data': './data',\n",
        "    'outf': './results',\n",
        "    'dataset': 'cifar10',\n",
        "    'workers': 8,\n",
        "    'batch_size': 16,\n",
        "    'lr': 0.1,\n",
        "    'max_epoch': 30,\n",
        "    'stepsize': 10,\n",
        "    'aug': 'none',\n",
        "    'model': 'densenet',\n",
        "    'eval_freq': 5,\n",
        "    'print_freq': 100,\n",
        "    'gpu': '0',\n",
        "    'seed': 0,\n",
        "    'use_cpu': False,\n",
        "    'eval': True,\n",
        "    'epsilon': 0.0157,\n",
        "    'alpha': 0.00784,\n",
        "    'k': 10,\n",
        "    'perturbation_type': 'linf'\n",
        "}\n"
      ],
      "metadata": {
        "id": "kpqP-MrH0Ald"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'cifar10' == options['dataset']:\n",
        "        Data = CIFAR10D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['aug'], _eval=options['eval'])\n",
        "        OODData = CIFAR100D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['aug'])\n",
        "else:\n",
        "        Data = CIFAR100D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['aug'], _eval=options['eval'])\n",
        "        OODData = CIFAR10D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['aug'])"
      ],
      "metadata": {
        "id": "PsH_MKlSjd-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c7858bb-0336-47c2-dccc-ea8f87222eb1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:12<00:00, 13169610.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar10/cifar-10-python.tar.gz to ./data/cifar10\n",
            "Files already downloaded and verified\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar100/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:13<00:00, 12657193.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar100/cifar-100-python.tar.gz to ./data/cifar100\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader, testloader, outloader = Data.train_loader, Data.test_loader, OODData.test_loader\n",
        "num_classes = Data.num_classes\n",
        "\n",
        "\n",
        "\"\"\"DenseNet implementation (https://arxiv.org/abs/1608.06993).\"\"\"\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "  \"\"\"Bottleneck block for DenseNet.\"\"\"\n",
        "\n",
        "  def __init__(self, n_channels, growth_rate):\n",
        "    super(Bottleneck, self).__init__()\n",
        "    inter_channels = 4 * growth_rate\n",
        "    self.bn1 = nn.BatchNorm2d(n_channels)\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        n_channels, inter_channels, kernel_size=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(inter_channels)\n",
        "    self.conv2 = nn.Conv2d(\n",
        "        inter_channels, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(F.relu(self.bn1(x)))\n",
        "    out = self.conv2(F.relu(self.bn2(out)))\n",
        "    out = torch.cat((x, out), 1)\n",
        "    return out\n",
        "\n",
        "\n",
        "class SingleLayer(nn.Module):\n",
        "  \"\"\"Layer container for blocks.\"\"\"\n",
        "\n",
        "  def __init__(self, n_channels, growth_rate):\n",
        "    super(SingleLayer, self).__init__()\n",
        "    self.bn1 = nn.BatchNorm2d(n_channels)\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        n_channels, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(F.relu(self.bn1(x)))\n",
        "    out = torch.cat((x, out), 1)\n",
        "    return out\n",
        "\n",
        "\n",
        "class Transition(nn.Module):\n",
        "  \"\"\"Transition block.\"\"\"\n",
        "\n",
        "  def __init__(self, n_channels, n_out_channels):\n",
        "    super(Transition, self).__init__()\n",
        "    self.bn1 = nn.BatchNorm2d(n_channels)\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        n_channels, n_out_channels, kernel_size=1, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(F.relu(self.bn1(x)))\n",
        "    out = F.avg_pool2d(out, 2)\n",
        "    return out\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "  \"\"\"DenseNet main class.\"\"\"\n",
        "\n",
        "  def __init__(self, growth_rate, depth, reduction, n_classes, bottleneck):\n",
        "    super(DenseNet, self).__init__()\n",
        "\n",
        "    if bottleneck:\n",
        "      n_dense_blocks = int((depth - 4) / 6)\n",
        "    else:\n",
        "      n_dense_blocks = int((depth - 4) / 3)\n",
        "\n",
        "    n_channels = 2 * growth_rate\n",
        "    self.conv1 = nn.Conv2d(3, n_channels, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "    self.dense1 = self._make_dense(n_channels, growth_rate, n_dense_blocks,\n",
        "                                   bottleneck)\n",
        "    n_channels += n_dense_blocks * growth_rate\n",
        "    n_out_channels = int(math.floor(n_channels * reduction))\n",
        "    self.trans1 = Transition(n_channels, n_out_channels)\n",
        "\n",
        "    n_channels = n_out_channels\n",
        "    self.dense2 = self._make_dense(n_channels, growth_rate, n_dense_blocks,\n",
        "                                   bottleneck)\n",
        "    n_channels += n_dense_blocks * growth_rate\n",
        "    n_out_channels = int(math.floor(n_channels * reduction))\n",
        "    self.trans2 = Transition(n_channels, n_out_channels)\n",
        "\n",
        "    n_channels = n_out_channels\n",
        "    self.dense3 = self._make_dense(n_channels, growth_rate, n_dense_blocks,\n",
        "                                   bottleneck)\n",
        "    n_channels += n_dense_blocks * growth_rate\n",
        "\n",
        "    self.bn1 = nn.BatchNorm2d(n_channels)\n",
        "    self.fc = nn.Linear(n_channels, n_classes)\n",
        "\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        m.bias.data.zero_()\n",
        "\n",
        "  def _make_dense(self, n_channels, growth_rate, n_dense_blocks, bottleneck):\n",
        "    layers = []\n",
        "    for _ in range(int(n_dense_blocks)):\n",
        "      if bottleneck:\n",
        "        layers.append(Bottleneck(n_channels, growth_rate))\n",
        "      else:\n",
        "        layers.append(SingleLayer(n_channels, growth_rate))\n",
        "      n_channels += growth_rate\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x, rf=False, _eval=False):\n",
        "    if _eval:\n",
        "        # switch to eval mode\n",
        "        self.eval()\n",
        "    else:\n",
        "        self.train()\n",
        "    out = self.conv1(x)\n",
        "    out = self.trans1(self.dense1(out))\n",
        "    out = self.trans2(self.dense2(out))\n",
        "    out = self.dense3(out)\n",
        "    out = torch.squeeze(F.avg_pool2d(F.relu(self.bn1(out)), 8))\n",
        "    y = self.fc(out)\n",
        "    if rf:\n",
        "        return out, y\n",
        "    return y\n",
        "\n",
        "def densenet(growth_rate=12, depth=40, num_classes=10):\n",
        "  model = DenseNet(growth_rate, depth, 1., num_classes, False)\n",
        "  return model"
      ],
      "metadata": {
        "id": "cGGLtRXoobo1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating model: {}\".format(options['model']))\n",
        "if 'wide_resnet' in options['model']:\n",
        "        print('wide_resnet')\n",
        "        net = WideResNet(40, num_classes, 2, 0.0)\n",
        "elif 'allconv' in options['model']:\n",
        "        print('allconv')\n",
        "        net = AllConvNet(num_classes)\n",
        "elif 'densenet' in options['model']:\n",
        "        print('densenet')\n",
        "        net = densenet(num_classes=num_classes)\n",
        "elif 'resnext' in options['model']:\n",
        "        print('resnext29')\n",
        "        net = resnext29(num_classes)\n",
        "else:\n",
        "        print('resnet18')\n",
        "        net = ResNet18(num_classes=num_classes)\n"
      ],
      "metadata": {
        "id": "4NmhUnj5opN6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1a5e431-a912-4431-9c91-3b60cd6e8d50"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model: densenet\n",
            "densenet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(options['seed'])\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = options['gpu']\n",
        "use_gpu = torch.cuda.is_available()\n",
        "if options['use_cpu']: use_gpu = False\n",
        "\n",
        "options.update({'use_gpu': use_gpu})\n",
        "options['use_gpu']"
      ],
      "metadata": {
        "id": "c_XXanTQdxDh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53eb52d5-3e2a-490b-bf17-537b73d97e6b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "if use_gpu:\n",
        "        net = nn.DataParallel(net, device_ids=[i for i in range(len(options['gpu'].split(',')))]).cuda()\n",
        "        criterion = criterion.cuda()"
      ],
      "metadata": {
        "id": "UJ8nlpIcpzci"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = '{}_{}_{}'.format(options['model'], options['dataset'], options['aug'])\n"
      ],
      "metadata": {
        "id": "-3eUpU7ztjaM"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_list = [{'params': net.parameters()},\n",
        "                {'params': criterion.parameters()}]"
      ],
      "metadata": {
        "id": "OJlKwuxb04v-"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(params_list, lr=options['lr'], momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
        "scheduler = lr_scheduler.MultiStepLR(optimizer, gamma=0.2, milestones=[60, 120, 160, 190])\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "best_acc = 0.0"
      ],
      "metadata": {
        "id": "y9Ho8WZQ08LG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(options['max_epoch']):\n",
        "        print(\"==> Epoch {}/{}\".format(epoch+1, options['max_epoch']))\n",
        "\n",
        "        train(net, criterion, optimizer, trainloader, epoch=epoch, **options)\n",
        "\n",
        "        if options['eval_freq'] > 0 and (epoch+1) % options['eval_freq'] == 0 or (epoch+1) == options['max_epoch'] or epoch > 160:\n",
        "            print(\"==> Test\")\n",
        "            results = test(net, criterion, testloader, outloader, epoch=epoch, **options)\n",
        "\n",
        "            if best_acc < results['ACC']:\n",
        "                best_acc = results['ACC']\n",
        "                print(\"Best Acc (%): {:.3f}\\t\".format(best_acc))\n",
        "            \n",
        "            save_networks(net, options['outf'], file_name, criterion=criterion)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "elapsed = round(time.time() - start_time)\n",
        "elapsed = str(datetime.timedelta(seconds=elapsed))\n",
        "print(\"Finished. Total elapsed time (h:m:s): {}\".format(elapsed))"
      ],
      "metadata": {
        "id": "IEFaTaqm1r_6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed7d4c93-e9a6-4370-c7f2-8b930ed50a3d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Epoch 1/30\n",
            "Batch 100/3125\t Loss 3.830131 (5.437128)\n",
            "Batch 200/3125\t Loss 4.752025 (4.865413)\n",
            "Batch 300/3125\t Loss 4.066632 (4.653672)\n",
            "Batch 400/3125\t Loss 4.912817 (4.526932)\n",
            "Batch 500/3125\t Loss 4.222028 (4.445062)\n",
            "Batch 600/3125\t Loss 3.447995 (4.391805)\n",
            "Batch 700/3125\t Loss 3.533146 (4.337196)\n",
            "Batch 800/3125\t Loss 3.419644 (4.290328)\n",
            "Batch 900/3125\t Loss 4.434329 (4.252665)\n",
            "Batch 1000/3125\t Loss 3.656487 (4.213680)\n",
            "Batch 1100/3125\t Loss 3.841406 (4.185337)\n",
            "Batch 1200/3125\t Loss 3.699815 (4.155300)\n",
            "Batch 1300/3125\t Loss 3.312160 (4.124714)\n",
            "Batch 1400/3125\t Loss 4.525392 (4.103715)\n",
            "Batch 1500/3125\t Loss 3.348178 (4.083403)\n",
            "Batch 1600/3125\t Loss 4.615271 (4.053405)\n",
            "Batch 1700/3125\t Loss 3.068424 (4.033440)\n",
            "Batch 1800/3125\t Loss 2.678349 (4.007702)\n",
            "Batch 1900/3125\t Loss 3.224787 (3.985968)\n",
            "Batch 2000/3125\t Loss 2.818827 (3.968825)\n",
            "Batch 2100/3125\t Loss 3.925318 (3.948304)\n",
            "Batch 2200/3125\t Loss 2.913440 (3.923877)\n",
            "Batch 2300/3125\t Loss 2.783914 (3.901254)\n",
            "Batch 2400/3125\t Loss 3.282014 (3.879450)\n",
            "Batch 2500/3125\t Loss 3.128356 (3.861422)\n",
            "Batch 2600/3125\t Loss 3.760113 (3.836167)\n",
            "Batch 2700/3125\t Loss 3.065853 (3.817166)\n",
            "Batch 2800/3125\t Loss 2.997914 (3.794908)\n",
            "Batch 2900/3125\t Loss 2.641057 (3.773137)\n",
            "Batch 3000/3125\t Loss 3.333289 (3.753247)\n",
            "Batch 3100/3125\t Loss 2.917695 (3.730837)\n",
            "==> Epoch 2/30\n",
            "Batch 100/3125\t Loss 3.024443 (3.022708)\n",
            "Batch 200/3125\t Loss 3.676221 (3.030155)\n",
            "Batch 300/3125\t Loss 3.903954 (3.033845)\n",
            "Batch 400/3125\t Loss 3.384937 (3.036562)\n",
            "Batch 500/3125\t Loss 3.379118 (3.016057)\n",
            "Batch 600/3125\t Loss 1.875679 (2.984437)\n",
            "Batch 700/3125\t Loss 3.515311 (2.978632)\n",
            "Batch 800/3125\t Loss 3.218402 (2.974187)\n",
            "Batch 900/3125\t Loss 3.105263 (2.963230)\n",
            "Batch 1000/3125\t Loss 2.803920 (2.959260)\n",
            "Batch 1100/3125\t Loss 3.278294 (2.949278)\n",
            "Batch 1200/3125\t Loss 2.492965 (2.936037)\n",
            "Batch 1300/3125\t Loss 3.162253 (2.923301)\n",
            "Batch 1400/3125\t Loss 2.881661 (2.922689)\n",
            "Batch 1500/3125\t Loss 3.505974 (2.914612)\n",
            "Batch 1600/3125\t Loss 2.321817 (2.910443)\n",
            "Batch 1700/3125\t Loss 2.262299 (2.898842)\n",
            "Batch 1800/3125\t Loss 3.054077 (2.890229)\n",
            "Batch 1900/3125\t Loss 3.190959 (2.882097)\n",
            "Batch 2000/3125\t Loss 2.472414 (2.867567)\n",
            "Batch 2100/3125\t Loss 2.154738 (2.860604)\n",
            "Batch 2200/3125\t Loss 2.906303 (2.853517)\n",
            "Batch 2300/3125\t Loss 2.668592 (2.845774)\n",
            "Batch 2400/3125\t Loss 2.920434 (2.836336)\n",
            "Batch 2500/3125\t Loss 3.223026 (2.829745)\n",
            "Batch 2600/3125\t Loss 2.481498 (2.824074)\n",
            "Batch 2700/3125\t Loss 3.176358 (2.814000)\n",
            "Batch 2800/3125\t Loss 2.146703 (2.807572)\n",
            "Batch 2900/3125\t Loss 1.555622 (2.799543)\n",
            "Batch 3000/3125\t Loss 3.535423 (2.792772)\n",
            "Batch 3100/3125\t Loss 3.843224 (2.788073)\n",
            "==> Epoch 3/30\n",
            "Batch 100/3125\t Loss 2.561442 (2.538611)\n",
            "Batch 200/3125\t Loss 2.901075 (2.545881)\n",
            "Batch 300/3125\t Loss 1.458476 (2.527144)\n",
            "Batch 400/3125\t Loss 2.151325 (2.555723)\n",
            "Batch 500/3125\t Loss 2.841352 (2.546057)\n",
            "Batch 600/3125\t Loss 1.957682 (2.550946)\n",
            "Batch 700/3125\t Loss 2.030504 (2.557247)\n",
            "Batch 800/3125\t Loss 1.365352 (2.541018)\n",
            "Batch 900/3125\t Loss 1.727107 (2.547069)\n",
            "Batch 1000/3125\t Loss 2.270755 (2.549128)\n",
            "Batch 1100/3125\t Loss 1.848180 (2.543373)\n",
            "Batch 1200/3125\t Loss 2.715931 (2.536166)\n",
            "Batch 1300/3125\t Loss 1.961487 (2.525312)\n",
            "Batch 1400/3125\t Loss 2.347902 (2.519776)\n",
            "Batch 1500/3125\t Loss 3.519787 (2.518606)\n",
            "Batch 1600/3125\t Loss 1.496752 (2.503968)\n",
            "Batch 1700/3125\t Loss 2.720878 (2.508116)\n",
            "Batch 1800/3125\t Loss 2.548029 (2.501416)\n",
            "Batch 1900/3125\t Loss 2.225865 (2.494858)\n",
            "Batch 2000/3125\t Loss 1.052205 (2.492454)\n",
            "Batch 2100/3125\t Loss 2.746589 (2.486140)\n",
            "Batch 2200/3125\t Loss 1.868477 (2.483507)\n",
            "Batch 2300/3125\t Loss 1.621917 (2.482280)\n",
            "Batch 2400/3125\t Loss 2.897879 (2.477282)\n",
            "Batch 2500/3125\t Loss 2.725057 (2.474480)\n",
            "Batch 2600/3125\t Loss 2.194072 (2.469134)\n",
            "Batch 2700/3125\t Loss 3.132037 (2.466824)\n",
            "Batch 2800/3125\t Loss 3.231759 (2.465563)\n",
            "Batch 2900/3125\t Loss 2.563134 (2.462834)\n",
            "Batch 3000/3125\t Loss 2.078907 (2.463983)\n",
            "Batch 3100/3125\t Loss 1.479298 (2.459930)\n",
            "==> Epoch 4/30\n",
            "Batch 100/3125\t Loss 2.065701 (2.421958)\n",
            "Batch 200/3125\t Loss 3.142834 (2.371283)\n",
            "Batch 300/3125\t Loss 2.236287 (2.396486)\n",
            "Batch 400/3125\t Loss 1.959985 (2.400863)\n",
            "Batch 500/3125\t Loss 2.652365 (2.397259)\n",
            "Batch 600/3125\t Loss 3.999511 (2.384918)\n",
            "Batch 700/3125\t Loss 2.922834 (2.390781)\n",
            "Batch 800/3125\t Loss 3.015825 (2.397250)\n",
            "Batch 900/3125\t Loss 3.937582 (2.382005)\n",
            "Batch 1000/3125\t Loss 2.158543 (2.375722)\n",
            "Batch 1100/3125\t Loss 1.544706 (2.370434)\n",
            "Batch 1200/3125\t Loss 1.863366 (2.375601)\n",
            "Batch 1300/3125\t Loss 2.364084 (2.373923)\n",
            "Batch 1400/3125\t Loss 2.977074 (2.371815)\n",
            "Batch 1500/3125\t Loss 2.728022 (2.372892)\n",
            "Batch 1600/3125\t Loss 2.308773 (2.373090)\n",
            "Batch 1700/3125\t Loss 2.241170 (2.375841)\n",
            "Batch 1800/3125\t Loss 3.184038 (2.379835)\n",
            "Batch 1900/3125\t Loss 2.877464 (2.381731)\n",
            "Batch 2000/3125\t Loss 2.048785 (2.378995)\n",
            "Batch 2100/3125\t Loss 2.117942 (2.377409)\n",
            "Batch 2200/3125\t Loss 3.738554 (2.374830)\n",
            "Batch 2300/3125\t Loss 2.982311 (2.374472)\n",
            "Batch 2400/3125\t Loss 1.313246 (2.372982)\n",
            "Batch 2500/3125\t Loss 1.443326 (2.368334)\n",
            "Batch 2600/3125\t Loss 2.148513 (2.365171)\n",
            "Batch 2700/3125\t Loss 2.380136 (2.365575)\n",
            "Batch 2800/3125\t Loss 3.545655 (2.362104)\n",
            "Batch 2900/3125\t Loss 2.403640 (2.357181)\n",
            "Batch 3000/3125\t Loss 2.235846 (2.357233)\n",
            "Batch 3100/3125\t Loss 1.368138 (2.357785)\n",
            "==> Epoch 5/30\n",
            "Batch 100/3125\t Loss 2.616141 (2.333809)\n",
            "Batch 200/3125\t Loss 2.347814 (2.293380)\n",
            "Batch 300/3125\t Loss 2.914526 (2.351159)\n",
            "Batch 400/3125\t Loss 2.669161 (2.367249)\n",
            "Batch 500/3125\t Loss 2.895953 (2.345810)\n",
            "Batch 600/3125\t Loss 2.413871 (2.335667)\n",
            "Batch 700/3125\t Loss 1.540278 (2.320385)\n",
            "Batch 800/3125\t Loss 1.939091 (2.318936)\n",
            "Batch 900/3125\t Loss 2.385426 (2.315061)\n",
            "Batch 1000/3125\t Loss 2.962103 (2.312877)\n",
            "Batch 1100/3125\t Loss 1.942190 (2.316835)\n",
            "Batch 1200/3125\t Loss 1.986778 (2.313089)\n",
            "Batch 1300/3125\t Loss 2.301686 (2.318524)\n",
            "Batch 1400/3125\t Loss 1.856496 (2.313305)\n",
            "Batch 1500/3125\t Loss 2.417424 (2.307520)\n",
            "Batch 1600/3125\t Loss 2.058063 (2.311070)\n",
            "Batch 1700/3125\t Loss 1.812273 (2.305844)\n",
            "Batch 1800/3125\t Loss 1.915930 (2.299589)\n",
            "Batch 1900/3125\t Loss 1.860753 (2.300325)\n",
            "Batch 2000/3125\t Loss 3.218652 (2.300839)\n",
            "Batch 2100/3125\t Loss 2.440236 (2.297323)\n",
            "Batch 2200/3125\t Loss 3.118663 (2.295665)\n",
            "Batch 2300/3125\t Loss 2.648107 (2.288720)\n",
            "Batch 2400/3125\t Loss 2.401082 (2.294327)\n",
            "Batch 2500/3125\t Loss 2.954712 (2.294172)\n",
            "Batch 2600/3125\t Loss 1.604270 (2.293134)\n",
            "Batch 2700/3125\t Loss 1.895468 (2.295602)\n",
            "Batch 2800/3125\t Loss 1.672713 (2.295707)\n",
            "Batch 2900/3125\t Loss 2.246749 (2.297643)\n",
            "Batch 3000/3125\t Loss 3.716385 (2.297516)\n",
            "Batch 3100/3125\t Loss 2.750707 (2.302003)\n",
            "==> Test\n",
            "       TNR    AUROC  DTACC  AUIN   AUOUT \n",
            "Bas    15.070 71.977 66.445 73.874 68.064\n",
            "Acc: 66.32000\n",
            "Best Acc (%): 66.320\t\n",
            "==> Epoch 6/30\n",
            "Batch 100/3125\t Loss 1.953835 (2.244202)\n",
            "Batch 200/3125\t Loss 1.432824 (2.295269)\n",
            "Batch 300/3125\t Loss 3.141823 (2.285139)\n",
            "Batch 400/3125\t Loss 2.233823 (2.280635)\n",
            "Batch 500/3125\t Loss 1.112180 (2.271191)\n",
            "Batch 600/3125\t Loss 3.044044 (2.271285)\n",
            "Batch 700/3125\t Loss 3.768560 (2.289015)\n",
            "Batch 800/3125\t Loss 1.924153 (2.291975)\n",
            "Batch 900/3125\t Loss 1.573231 (2.288270)\n",
            "Batch 1000/3125\t Loss 2.063448 (2.283211)\n",
            "Batch 1100/3125\t Loss 2.896329 (2.281920)\n",
            "Batch 1200/3125\t Loss 1.626664 (2.279998)\n",
            "Batch 1300/3125\t Loss 2.442757 (2.275044)\n",
            "Batch 1400/3125\t Loss 2.595404 (2.278275)\n",
            "Batch 1500/3125\t Loss 2.084274 (2.276242)\n",
            "Batch 1600/3125\t Loss 1.935474 (2.278094)\n",
            "Batch 1700/3125\t Loss 2.360653 (2.273719)\n",
            "Batch 1800/3125\t Loss 2.799016 (2.275764)\n",
            "Batch 1900/3125\t Loss 1.277282 (2.277902)\n",
            "Batch 2000/3125\t Loss 2.438075 (2.276433)\n",
            "Batch 2100/3125\t Loss 2.831386 (2.277991)\n",
            "Batch 2200/3125\t Loss 2.452733 (2.275036)\n",
            "Batch 2300/3125\t Loss 1.542699 (2.276072)\n",
            "Batch 2400/3125\t Loss 1.960263 (2.276384)\n",
            "Batch 2500/3125\t Loss 2.334527 (2.277553)\n",
            "Batch 2600/3125\t Loss 3.027575 (2.279998)\n",
            "Batch 2700/3125\t Loss 1.465066 (2.279745)\n",
            "Batch 2800/3125\t Loss 2.157854 (2.276858)\n",
            "Batch 2900/3125\t Loss 2.093835 (2.277075)\n",
            "Batch 3000/3125\t Loss 2.278508 (2.273328)\n",
            "Batch 3100/3125\t Loss 3.189114 (2.271594)\n",
            "==> Epoch 7/30\n",
            "Batch 100/3125\t Loss 2.673011 (2.242681)\n",
            "Batch 200/3125\t Loss 2.333377 (2.239739)\n",
            "Batch 300/3125\t Loss 2.399778 (2.222601)\n",
            "Batch 400/3125\t Loss 3.212456 (2.240877)\n",
            "Batch 500/3125\t Loss 2.674825 (2.263617)\n",
            "Batch 600/3125\t Loss 3.463230 (2.273282)\n",
            "Batch 700/3125\t Loss 2.235547 (2.261000)\n",
            "Batch 800/3125\t Loss 2.455323 (2.267851)\n",
            "Batch 900/3125\t Loss 1.500064 (2.264797)\n",
            "Batch 1000/3125\t Loss 1.971266 (2.273330)\n",
            "Batch 1100/3125\t Loss 2.480143 (2.273916)\n",
            "Batch 1200/3125\t Loss 2.741525 (2.276506)\n",
            "Batch 1300/3125\t Loss 1.554322 (2.275809)\n",
            "Batch 1400/3125\t Loss 1.419991 (2.270555)\n",
            "Batch 1500/3125\t Loss 2.053487 (2.274404)\n",
            "Batch 1600/3125\t Loss 1.897341 (2.271450)\n",
            "Batch 1700/3125\t Loss 2.286317 (2.268362)\n",
            "Batch 1800/3125\t Loss 2.159326 (2.269445)\n",
            "Batch 1900/3125\t Loss 3.019320 (2.272538)\n",
            "Batch 2000/3125\t Loss 1.143815 (2.269155)\n",
            "Batch 2100/3125\t Loss 1.840355 (2.268472)\n",
            "Batch 2200/3125\t Loss 3.141562 (2.265015)\n",
            "Batch 2300/3125\t Loss 2.334120 (2.267638)\n",
            "Batch 2400/3125\t Loss 1.930802 (2.267932)\n",
            "Batch 2500/3125\t Loss 2.917481 (2.269207)\n",
            "Batch 2600/3125\t Loss 2.495012 (2.270164)\n",
            "Batch 2700/3125\t Loss 1.808731 (2.270182)\n",
            "Batch 2800/3125\t Loss 2.212082 (2.265583)\n",
            "Batch 2900/3125\t Loss 2.566068 (2.265556)\n",
            "Batch 3000/3125\t Loss 2.918479 (2.265993)\n",
            "Batch 3100/3125\t Loss 1.405985 (2.264373)\n",
            "==> Epoch 8/30\n",
            "Batch 100/3125\t Loss 2.242828 (2.249484)\n",
            "Batch 200/3125\t Loss 1.868147 (2.234416)\n",
            "Batch 300/3125\t Loss 1.830733 (2.252819)\n",
            "Batch 400/3125\t Loss 1.761504 (2.261357)\n",
            "Batch 500/3125\t Loss 2.868202 (2.245540)\n",
            "Batch 600/3125\t Loss 2.404491 (2.249275)\n",
            "Batch 700/3125\t Loss 3.538538 (2.266036)\n",
            "Batch 800/3125\t Loss 2.059899 (2.272122)\n",
            "Batch 900/3125\t Loss 2.941907 (2.278987)\n",
            "Batch 1000/3125\t Loss 2.760906 (2.280004)\n",
            "Batch 1100/3125\t Loss 1.509569 (2.273550)\n",
            "Batch 1200/3125\t Loss 3.318624 (2.270295)\n",
            "Batch 1300/3125\t Loss 1.858740 (2.264970)\n",
            "Batch 1400/3125\t Loss 1.509627 (2.261110)\n",
            "Batch 1500/3125\t Loss 2.381809 (2.261631)\n",
            "Batch 1600/3125\t Loss 2.553256 (2.257034)\n",
            "Batch 1700/3125\t Loss 1.683900 (2.257865)\n",
            "Batch 1800/3125\t Loss 1.478729 (2.259022)\n",
            "Batch 1900/3125\t Loss 2.377030 (2.256813)\n",
            "Batch 2000/3125\t Loss 2.299499 (2.255634)\n",
            "Batch 2100/3125\t Loss 2.563307 (2.251939)\n",
            "Batch 2200/3125\t Loss 2.577022 (2.251575)\n",
            "Batch 2300/3125\t Loss 2.098204 (2.248379)\n",
            "Batch 2400/3125\t Loss 3.046849 (2.250701)\n",
            "Batch 2500/3125\t Loss 1.547227 (2.250140)\n",
            "Batch 2600/3125\t Loss 1.654147 (2.248168)\n",
            "Batch 2700/3125\t Loss 1.730075 (2.249816)\n",
            "Batch 2800/3125\t Loss 2.751740 (2.253590)\n",
            "Batch 2900/3125\t Loss 2.566223 (2.253945)\n",
            "Batch 3000/3125\t Loss 2.545680 (2.257249)\n",
            "Batch 3100/3125\t Loss 1.323955 (2.257079)\n",
            "==> Epoch 9/30\n",
            "Batch 100/3125\t Loss 1.966345 (2.360847)\n",
            "Batch 200/3125\t Loss 1.773776 (2.282607)\n",
            "Batch 300/3125\t Loss 1.339097 (2.233583)\n",
            "Batch 400/3125\t Loss 1.423996 (2.217935)\n",
            "Batch 500/3125\t Loss 3.079684 (2.216985)\n",
            "Batch 600/3125\t Loss 1.180611 (2.247476)\n",
            "Batch 700/3125\t Loss 2.322860 (2.251776)\n",
            "Batch 800/3125\t Loss 2.487912 (2.258914)\n",
            "Batch 900/3125\t Loss 1.638269 (2.257851)\n",
            "Batch 1000/3125\t Loss 1.410502 (2.264176)\n",
            "Batch 1100/3125\t Loss 1.633974 (2.260262)\n",
            "Batch 1200/3125\t Loss 1.622113 (2.256554)\n",
            "Batch 1300/3125\t Loss 2.328188 (2.254205)\n",
            "Batch 1400/3125\t Loss 3.265930 (2.248269)\n",
            "Batch 1500/3125\t Loss 1.060336 (2.249112)\n",
            "Batch 1600/3125\t Loss 1.859354 (2.253718)\n",
            "Batch 1700/3125\t Loss 2.050368 (2.251980)\n",
            "Batch 1800/3125\t Loss 2.468982 (2.253785)\n",
            "Batch 1900/3125\t Loss 2.655177 (2.250914)\n",
            "Batch 2000/3125\t Loss 1.963949 (2.247251)\n",
            "Batch 2100/3125\t Loss 1.389610 (2.239668)\n",
            "Batch 2200/3125\t Loss 2.019127 (2.239613)\n",
            "Batch 2300/3125\t Loss 2.350048 (2.236246)\n",
            "Batch 2400/3125\t Loss 1.833732 (2.236272)\n",
            "Batch 2500/3125\t Loss 2.721005 (2.239563)\n",
            "Batch 2600/3125\t Loss 1.837367 (2.234946)\n",
            "Batch 2700/3125\t Loss 3.334107 (2.235747)\n",
            "Batch 2800/3125\t Loss 2.832096 (2.237185)\n",
            "Batch 2900/3125\t Loss 2.605340 (2.236797)\n",
            "Batch 3000/3125\t Loss 1.533389 (2.234884)\n",
            "Batch 3100/3125\t Loss 2.177374 (2.235875)\n",
            "==> Epoch 10/30\n",
            "Batch 100/3125\t Loss 2.610332 (2.183766)\n",
            "Batch 200/3125\t Loss 2.265302 (2.218086)\n",
            "Batch 300/3125\t Loss 2.382954 (2.189934)\n",
            "Batch 400/3125\t Loss 1.837936 (2.185307)\n",
            "Batch 500/3125\t Loss 1.702554 (2.185239)\n",
            "Batch 600/3125\t Loss 2.057458 (2.199228)\n",
            "Batch 700/3125\t Loss 1.451265 (2.204473)\n",
            "Batch 800/3125\t Loss 2.481792 (2.207963)\n",
            "Batch 900/3125\t Loss 2.431768 (2.220692)\n",
            "Batch 1000/3125\t Loss 1.343894 (2.222133)\n",
            "Batch 1100/3125\t Loss 1.716126 (2.229441)\n",
            "Batch 1200/3125\t Loss 2.837446 (2.230918)\n",
            "Batch 1300/3125\t Loss 1.205798 (2.235648)\n",
            "Batch 1400/3125\t Loss 3.415153 (2.232561)\n",
            "Batch 1500/3125\t Loss 1.728533 (2.234698)\n",
            "Batch 1600/3125\t Loss 3.429086 (2.236607)\n",
            "Batch 1700/3125\t Loss 2.267896 (2.235413)\n",
            "Batch 1800/3125\t Loss 2.145358 (2.235354)\n",
            "Batch 1900/3125\t Loss 2.950687 (2.238457)\n",
            "Batch 2000/3125\t Loss 2.456061 (2.232330)\n",
            "Batch 2100/3125\t Loss 2.018466 (2.236965)\n",
            "Batch 2200/3125\t Loss 2.510124 (2.234459)\n",
            "Batch 2300/3125\t Loss 2.568203 (2.234649)\n",
            "Batch 2400/3125\t Loss 3.085396 (2.232494)\n",
            "Batch 2500/3125\t Loss 2.196659 (2.234715)\n",
            "Batch 2600/3125\t Loss 2.474325 (2.236988)\n",
            "Batch 2700/3125\t Loss 1.969512 (2.235731)\n",
            "Batch 2800/3125\t Loss 2.654526 (2.237738)\n",
            "Batch 2900/3125\t Loss 2.553597 (2.240115)\n",
            "Batch 3000/3125\t Loss 1.900289 (2.238915)\n",
            "Batch 3100/3125\t Loss 2.413977 (2.242413)\n",
            "==> Test\n",
            "       TNR    AUROC  DTACC  AUIN   AUOUT \n",
            "Bas    11.100 68.172 63.775 71.165 63.721\n",
            "Acc: 60.40000\n",
            "==> Epoch 11/30\n",
            "Batch 100/3125\t Loss 4.181398 (2.154614)\n",
            "Batch 200/3125\t Loss 2.465376 (2.188843)\n",
            "Batch 300/3125\t Loss 1.803863 (2.218290)\n",
            "Batch 400/3125\t Loss 3.481838 (2.218369)\n",
            "Batch 500/3125\t Loss 1.321523 (2.233883)\n",
            "Batch 600/3125\t Loss 2.055233 (2.224861)\n",
            "Batch 700/3125\t Loss 1.640212 (2.220118)\n",
            "Batch 800/3125\t Loss 2.681131 (2.230443)\n",
            "Batch 900/3125\t Loss 1.449897 (2.243150)\n",
            "Batch 1000/3125\t Loss 2.668247 (2.243351)\n",
            "Batch 1100/3125\t Loss 1.760740 (2.238144)\n",
            "Batch 1200/3125\t Loss 3.464161 (2.230789)\n",
            "Batch 1300/3125\t Loss 2.302147 (2.220909)\n",
            "Batch 1400/3125\t Loss 2.895110 (2.218771)\n",
            "Batch 1500/3125\t Loss 1.992115 (2.220355)\n",
            "Batch 1600/3125\t Loss 1.351682 (2.217859)\n",
            "Batch 1700/3125\t Loss 2.084874 (2.219003)\n",
            "Batch 1800/3125\t Loss 2.465809 (2.224605)\n",
            "Batch 1900/3125\t Loss 1.928280 (2.223508)\n",
            "Batch 2000/3125\t Loss 1.781681 (2.221235)\n",
            "Batch 2100/3125\t Loss 3.305425 (2.222660)\n",
            "Batch 2200/3125\t Loss 3.010043 (2.227929)\n",
            "Batch 2300/3125\t Loss 0.913243 (2.227014)\n",
            "Batch 2400/3125\t Loss 3.466331 (2.225126)\n",
            "Batch 2500/3125\t Loss 3.378803 (2.230027)\n",
            "Batch 2600/3125\t Loss 1.939864 (2.231077)\n",
            "Batch 2700/3125\t Loss 2.525841 (2.231629)\n",
            "Batch 2800/3125\t Loss 3.112417 (2.233338)\n",
            "Batch 2900/3125\t Loss 2.190153 (2.230121)\n",
            "Batch 3000/3125\t Loss 1.511931 (2.228103)\n",
            "Batch 3100/3125\t Loss 2.867304 (2.227751)\n",
            "==> Epoch 12/30\n",
            "Batch 100/3125\t Loss 1.658575 (2.234715)\n",
            "Batch 200/3125\t Loss 2.605338 (2.228244)\n",
            "Batch 300/3125\t Loss 3.866663 (2.228224)\n",
            "Batch 400/3125\t Loss 1.972631 (2.225628)\n",
            "Batch 500/3125\t Loss 2.857050 (2.225517)\n",
            "Batch 600/3125\t Loss 1.788611 (2.222668)\n",
            "Batch 700/3125\t Loss 3.168079 (2.228181)\n",
            "Batch 800/3125\t Loss 2.596559 (2.225908)\n",
            "Batch 900/3125\t Loss 2.073044 (2.232837)\n",
            "Batch 1000/3125\t Loss 2.393509 (2.225784)\n",
            "Batch 1100/3125\t Loss 2.446220 (2.235174)\n",
            "Batch 1200/3125\t Loss 2.354465 (2.223911)\n",
            "Batch 1300/3125\t Loss 3.130348 (2.214254)\n",
            "Batch 1400/3125\t Loss 2.217910 (2.206724)\n",
            "Batch 1500/3125\t Loss 1.837877 (2.208764)\n",
            "Batch 1600/3125\t Loss 2.141830 (2.206649)\n",
            "Batch 1700/3125\t Loss 2.856742 (2.210939)\n",
            "Batch 1800/3125\t Loss 1.565139 (2.209347)\n",
            "Batch 1900/3125\t Loss 2.622976 (2.204636)\n",
            "Batch 2000/3125\t Loss 2.566772 (2.206114)\n",
            "Batch 2100/3125\t Loss 2.885876 (2.208533)\n",
            "Batch 2200/3125\t Loss 2.279035 (2.205467)\n",
            "Batch 2300/3125\t Loss 2.847894 (2.200726)\n",
            "Batch 2400/3125\t Loss 2.383980 (2.199442)\n",
            "Batch 2500/3125\t Loss 2.770178 (2.204648)\n",
            "Batch 2600/3125\t Loss 2.649786 (2.205362)\n",
            "Batch 2700/3125\t Loss 1.606753 (2.206882)\n",
            "Batch 2800/3125\t Loss 2.022755 (2.211011)\n",
            "Batch 2900/3125\t Loss 1.069899 (2.213921)\n",
            "Batch 3000/3125\t Loss 2.132458 (2.215906)\n",
            "Batch 3100/3125\t Loss 2.763253 (2.218599)\n",
            "==> Epoch 13/30\n",
            "Batch 100/3125\t Loss 1.814009 (2.116036)\n",
            "Batch 200/3125\t Loss 2.596760 (2.226451)\n",
            "Batch 300/3125\t Loss 1.787207 (2.290605)\n",
            "Batch 400/3125\t Loss 1.584600 (2.297102)\n",
            "Batch 500/3125\t Loss 1.362771 (2.306827)\n",
            "Batch 600/3125\t Loss 1.285713 (2.301461)\n",
            "Batch 700/3125\t Loss 2.317965 (2.289692)\n",
            "Batch 800/3125\t Loss 2.818606 (2.294536)\n",
            "Batch 900/3125\t Loss 1.647888 (2.284202)\n",
            "Batch 1000/3125\t Loss 2.935047 (2.280999)\n",
            "Batch 1100/3125\t Loss 3.377887 (2.287418)\n",
            "Batch 1200/3125\t Loss 2.406711 (2.294995)\n",
            "Batch 1300/3125\t Loss 2.922678 (2.286876)\n",
            "Batch 1400/3125\t Loss 2.410537 (2.284226)\n",
            "Batch 1500/3125\t Loss 2.744467 (2.278906)\n",
            "Batch 1600/3125\t Loss 2.818524 (2.281418)\n",
            "Batch 1700/3125\t Loss 2.344491 (2.280902)\n",
            "Batch 1800/3125\t Loss 2.645386 (2.274286)\n",
            "Batch 1900/3125\t Loss 2.791948 (2.273504)\n",
            "Batch 2000/3125\t Loss 1.034456 (2.269289)\n",
            "Batch 2100/3125\t Loss 2.554115 (2.265827)\n",
            "Batch 2200/3125\t Loss 2.555182 (2.268762)\n",
            "Batch 2300/3125\t Loss 2.294300 (2.266516)\n",
            "Batch 2400/3125\t Loss 3.449970 (2.263601)\n",
            "Batch 2500/3125\t Loss 1.909349 (2.263024)\n",
            "Batch 2600/3125\t Loss 2.994861 (2.264217)\n",
            "Batch 2700/3125\t Loss 1.634723 (2.264529)\n",
            "Batch 2800/3125\t Loss 1.501456 (2.260031)\n",
            "Batch 2900/3125\t Loss 1.270376 (2.258836)\n",
            "Batch 3000/3125\t Loss 1.606383 (2.254483)\n",
            "Batch 3100/3125\t Loss 2.030857 (2.253245)\n",
            "==> Epoch 14/30\n",
            "Batch 100/3125\t Loss 2.094308 (2.195008)\n",
            "Batch 200/3125\t Loss 2.218696 (2.184606)\n",
            "Batch 300/3125\t Loss 1.784796 (2.184759)\n",
            "Batch 400/3125\t Loss 1.871733 (2.228999)\n",
            "Batch 500/3125\t Loss 1.992414 (2.226267)\n",
            "Batch 600/3125\t Loss 2.220175 (2.223282)\n",
            "Batch 700/3125\t Loss 1.385317 (2.239789)\n",
            "Batch 800/3125\t Loss 2.221482 (2.233347)\n",
            "Batch 900/3125\t Loss 1.891314 (2.237859)\n",
            "Batch 1000/3125\t Loss 2.180920 (2.226388)\n",
            "Batch 1100/3125\t Loss 2.302260 (2.235047)\n",
            "Batch 1200/3125\t Loss 2.068720 (2.242467)\n",
            "Batch 1300/3125\t Loss 2.629091 (2.241076)\n",
            "Batch 1400/3125\t Loss 1.525642 (2.235946)\n",
            "Batch 1500/3125\t Loss 2.337095 (2.237533)\n",
            "Batch 1600/3125\t Loss 2.545643 (2.242470)\n",
            "Batch 1700/3125\t Loss 1.878529 (2.244964)\n",
            "Batch 1800/3125\t Loss 2.381902 (2.240226)\n",
            "Batch 1900/3125\t Loss 2.160240 (2.240930)\n",
            "Batch 2000/3125\t Loss 1.512088 (2.244950)\n",
            "Batch 2100/3125\t Loss 2.803628 (2.242431)\n",
            "Batch 2200/3125\t Loss 2.827096 (2.239113)\n",
            "Batch 2300/3125\t Loss 1.966794 (2.236996)\n",
            "Batch 2400/3125\t Loss 2.250139 (2.236553)\n",
            "Batch 2500/3125\t Loss 1.956901 (2.236208)\n",
            "Batch 2600/3125\t Loss 2.658338 (2.233605)\n",
            "Batch 2700/3125\t Loss 2.343113 (2.233467)\n",
            "Batch 2800/3125\t Loss 2.156574 (2.232129)\n",
            "Batch 2900/3125\t Loss 2.618521 (2.230290)\n",
            "Batch 3000/3125\t Loss 1.886829 (2.228768)\n",
            "Batch 3100/3125\t Loss 3.235008 (2.226267)\n",
            "==> Epoch 15/30\n",
            "Batch 100/3125\t Loss 2.321922 (2.105360)\n",
            "Batch 200/3125\t Loss 2.043848 (2.199547)\n",
            "Batch 300/3125\t Loss 2.321890 (2.211083)\n",
            "Batch 400/3125\t Loss 3.262437 (2.221279)\n",
            "Batch 500/3125\t Loss 2.021054 (2.211513)\n",
            "Batch 600/3125\t Loss 1.225111 (2.191388)\n",
            "Batch 700/3125\t Loss 2.101033 (2.199611)\n",
            "Batch 800/3125\t Loss 3.325133 (2.194288)\n",
            "Batch 900/3125\t Loss 2.388052 (2.201384)\n",
            "Batch 1000/3125\t Loss 1.864498 (2.203411)\n",
            "Batch 1100/3125\t Loss 2.076488 (2.199154)\n",
            "Batch 1200/3125\t Loss 2.622153 (2.206339)\n",
            "Batch 1300/3125\t Loss 3.029477 (2.206235)\n",
            "Batch 1400/3125\t Loss 2.431616 (2.207402)\n",
            "Batch 1500/3125\t Loss 2.328069 (2.200116)\n",
            "Batch 1600/3125\t Loss 1.984983 (2.203725)\n",
            "Batch 1700/3125\t Loss 1.414772 (2.208094)\n",
            "Batch 1800/3125\t Loss 1.878520 (2.208134)\n",
            "Batch 1900/3125\t Loss 1.407490 (2.212372)\n",
            "Batch 2000/3125\t Loss 2.607274 (2.214553)\n",
            "Batch 2100/3125\t Loss 2.372247 (2.211070)\n",
            "Batch 2200/3125\t Loss 1.453676 (2.209064)\n",
            "Batch 2300/3125\t Loss 2.065259 (2.210838)\n",
            "Batch 2400/3125\t Loss 3.182935 (2.213392)\n",
            "Batch 2500/3125\t Loss 0.671874 (2.212063)\n",
            "Batch 2600/3125\t Loss 2.249845 (2.213917)\n",
            "Batch 2700/3125\t Loss 2.904055 (2.218508)\n",
            "Batch 2800/3125\t Loss 2.160451 (2.214081)\n",
            "Batch 2900/3125\t Loss 2.502250 (2.213324)\n",
            "Batch 3000/3125\t Loss 1.555911 (2.213775)\n",
            "Batch 3100/3125\t Loss 3.175674 (2.215963)\n",
            "==> Test\n",
            "       TNR    AUROC  DTACC  AUIN   AUOUT \n",
            "Bas    17.710 75.271 69.130 77.415 71.539\n",
            "Acc: 70.43000\n",
            "Best Acc (%): 70.430\t\n",
            "==> Epoch 16/30\n",
            "Batch 100/3125\t Loss 1.047359 (2.248352)\n",
            "Batch 200/3125\t Loss 2.029053 (2.188189)\n",
            "Batch 300/3125\t Loss 2.034190 (2.214005)\n",
            "Batch 400/3125\t Loss 1.693434 (2.191530)\n",
            "Batch 500/3125\t Loss 2.752903 (2.197679)\n",
            "Batch 600/3125\t Loss 2.286772 (2.199220)\n",
            "Batch 700/3125\t Loss 2.525033 (2.191127)\n",
            "Batch 800/3125\t Loss 2.520369 (2.186745)\n",
            "Batch 900/3125\t Loss 1.565966 (2.184023)\n",
            "Batch 1000/3125\t Loss 2.278173 (2.182754)\n",
            "Batch 1100/3125\t Loss 1.011732 (2.174726)\n",
            "Batch 1200/3125\t Loss 1.043950 (2.177786)\n",
            "Batch 1300/3125\t Loss 2.149272 (2.175614)\n",
            "Batch 1400/3125\t Loss 1.991616 (2.183849)\n",
            "Batch 1500/3125\t Loss 2.546185 (2.190425)\n",
            "Batch 1600/3125\t Loss 2.848085 (2.191238)\n",
            "Batch 1700/3125\t Loss 1.897078 (2.187671)\n",
            "Batch 1800/3125\t Loss 1.603519 (2.191257)\n",
            "Batch 1900/3125\t Loss 3.323053 (2.187697)\n",
            "Batch 2000/3125\t Loss 2.568929 (2.184388)\n",
            "Batch 2100/3125\t Loss 2.461332 (2.190410)\n",
            "Batch 2200/3125\t Loss 1.671899 (2.193423)\n",
            "Batch 2300/3125\t Loss 2.517223 (2.190995)\n",
            "Batch 2400/3125\t Loss 0.893144 (2.189748)\n",
            "Batch 2500/3125\t Loss 1.950346 (2.192767)\n",
            "Batch 2600/3125\t Loss 2.293159 (2.195306)\n",
            "Batch 2700/3125\t Loss 1.902350 (2.196333)\n",
            "Batch 2800/3125\t Loss 2.968129 (2.197779)\n",
            "Batch 2900/3125\t Loss 2.379398 (2.195897)\n",
            "Batch 3000/3125\t Loss 2.724269 (2.195879)\n",
            "Batch 3100/3125\t Loss 2.307753 (2.198160)\n",
            "==> Epoch 17/30\n",
            "Batch 100/3125\t Loss 1.904874 (2.149996)\n",
            "Batch 200/3125\t Loss 2.822088 (2.136752)\n",
            "Batch 300/3125\t Loss 1.966944 (2.123528)\n",
            "Batch 400/3125\t Loss 2.491281 (2.151935)\n",
            "Batch 500/3125\t Loss 1.789747 (2.146642)\n",
            "Batch 600/3125\t Loss 2.317737 (2.156613)\n",
            "Batch 700/3125\t Loss 3.040651 (2.168636)\n",
            "Batch 800/3125\t Loss 2.899975 (2.179457)\n",
            "Batch 900/3125\t Loss 2.967797 (2.187987)\n",
            "Batch 1000/3125\t Loss 2.406984 (2.193472)\n",
            "Batch 1100/3125\t Loss 3.117961 (2.186905)\n",
            "Batch 1200/3125\t Loss 2.658428 (2.179574)\n",
            "Batch 1300/3125\t Loss 3.064050 (2.187271)\n",
            "Batch 1400/3125\t Loss 1.699862 (2.187541)\n",
            "Batch 1500/3125\t Loss 1.316229 (2.184163)\n",
            "Batch 1600/3125\t Loss 1.917111 (2.180222)\n",
            "Batch 1700/3125\t Loss 1.825297 (2.175089)\n",
            "Batch 1800/3125\t Loss 2.284312 (2.179992)\n",
            "Batch 1900/3125\t Loss 1.270450 (2.183262)\n",
            "Batch 2000/3125\t Loss 1.486114 (2.181348)\n",
            "Batch 2100/3125\t Loss 2.640628 (2.181600)\n",
            "Batch 2200/3125\t Loss 2.063472 (2.184256)\n",
            "Batch 2300/3125\t Loss 3.162477 (2.187024)\n",
            "Batch 2400/3125\t Loss 2.052096 (2.183972)\n",
            "Batch 2500/3125\t Loss 2.594857 (2.186626)\n",
            "Batch 2600/3125\t Loss 2.811452 (2.185369)\n",
            "Batch 2700/3125\t Loss 1.141156 (2.189329)\n",
            "Batch 2800/3125\t Loss 2.346359 (2.191560)\n",
            "Batch 2900/3125\t Loss 1.547722 (2.190114)\n",
            "Batch 3000/3125\t Loss 1.220409 (2.192527)\n",
            "Batch 3100/3125\t Loss 2.772222 (2.193367)\n",
            "==> Epoch 18/30\n",
            "Batch 100/3125\t Loss 1.881433 (2.176446)\n",
            "Batch 200/3125\t Loss 2.152391 (2.165030)\n",
            "Batch 300/3125\t Loss 2.600904 (2.160136)\n",
            "Batch 400/3125\t Loss 1.545451 (2.163241)\n",
            "Batch 500/3125\t Loss 1.768636 (2.158156)\n",
            "Batch 600/3125\t Loss 2.291192 (2.163522)\n",
            "Batch 700/3125\t Loss 1.944749 (2.171013)\n",
            "Batch 800/3125\t Loss 1.991784 (2.162941)\n",
            "Batch 900/3125\t Loss 1.303745 (2.164121)\n",
            "Batch 1000/3125\t Loss 3.517447 (2.169763)\n",
            "Batch 1100/3125\t Loss 1.172999 (2.167160)\n",
            "Batch 1200/3125\t Loss 2.651775 (2.174571)\n",
            "Batch 1300/3125\t Loss 2.199981 (2.179245)\n",
            "Batch 1400/3125\t Loss 1.690702 (2.182393)\n",
            "Batch 1500/3125\t Loss 2.410063 (2.178650)\n",
            "Batch 1600/3125\t Loss 2.922458 (2.184073)\n",
            "Batch 1700/3125\t Loss 1.969998 (2.185284)\n",
            "Batch 1800/3125\t Loss 1.379735 (2.188580)\n",
            "Batch 1900/3125\t Loss 3.031331 (2.188088)\n",
            "Batch 2000/3125\t Loss 1.984495 (2.186061)\n",
            "Batch 2100/3125\t Loss 1.348721 (2.184510)\n",
            "Batch 2200/3125\t Loss 2.724926 (2.185824)\n",
            "Batch 2300/3125\t Loss 2.048767 (2.188885)\n",
            "Batch 2400/3125\t Loss 2.705491 (2.194218)\n",
            "Batch 2500/3125\t Loss 1.129167 (2.194085)\n",
            "Batch 2600/3125\t Loss 2.076911 (2.197034)\n",
            "Batch 2700/3125\t Loss 3.483458 (2.198496)\n",
            "Batch 2800/3125\t Loss 2.274822 (2.198420)\n",
            "Batch 2900/3125\t Loss 1.182892 (2.198993)\n",
            "Batch 3000/3125\t Loss 2.026984 (2.199065)\n",
            "Batch 3100/3125\t Loss 1.850355 (2.200774)\n",
            "==> Epoch 19/30\n",
            "Batch 100/3125\t Loss 2.311407 (2.171341)\n",
            "Batch 200/3125\t Loss 2.558214 (2.144396)\n",
            "Batch 300/3125\t Loss 1.772894 (2.172203)\n",
            "Batch 400/3125\t Loss 1.891976 (2.187859)\n",
            "Batch 500/3125\t Loss 2.203457 (2.189553)\n",
            "Batch 600/3125\t Loss 1.987141 (2.197167)\n",
            "Batch 700/3125\t Loss 1.704757 (2.194548)\n",
            "Batch 800/3125\t Loss 1.449922 (2.186861)\n",
            "Batch 900/3125\t Loss 2.989854 (2.175273)\n",
            "Batch 1000/3125\t Loss 1.294428 (2.169273)\n",
            "Batch 1100/3125\t Loss 2.136195 (2.170845)\n",
            "Batch 1200/3125\t Loss 1.173884 (2.169351)\n",
            "Batch 1300/3125\t Loss 2.290413 (2.173274)\n",
            "Batch 1400/3125\t Loss 3.089358 (2.176919)\n",
            "Batch 1500/3125\t Loss 2.122026 (2.181440)\n",
            "Batch 1600/3125\t Loss 2.911905 (2.181764)\n",
            "Batch 1700/3125\t Loss 1.247339 (2.178417)\n",
            "Batch 1800/3125\t Loss 0.706493 (2.183334)\n",
            "Batch 1900/3125\t Loss 1.803398 (2.181547)\n",
            "Batch 2000/3125\t Loss 1.637457 (2.185043)\n",
            "Batch 2100/3125\t Loss 1.403436 (2.177679)\n",
            "Batch 2200/3125\t Loss 1.304596 (2.177883)\n",
            "Batch 2300/3125\t Loss 1.410325 (2.176951)\n",
            "Batch 2400/3125\t Loss 0.976133 (2.176022)\n",
            "Batch 2500/3125\t Loss 1.709727 (2.176236)\n",
            "Batch 2600/3125\t Loss 2.751207 (2.174810)\n",
            "Batch 2700/3125\t Loss 1.529370 (2.175318)\n",
            "Batch 2800/3125\t Loss 2.056735 (2.175479)\n",
            "Batch 2900/3125\t Loss 1.433000 (2.173681)\n",
            "Batch 3000/3125\t Loss 1.833366 (2.177122)\n",
            "Batch 3100/3125\t Loss 3.329717 (2.178214)\n",
            "==> Epoch 20/30\n",
            "Batch 100/3125\t Loss 2.634257 (2.240694)\n",
            "Batch 200/3125\t Loss 1.171173 (2.195824)\n",
            "Batch 300/3125\t Loss 3.120955 (2.205712)\n",
            "Batch 400/3125\t Loss 2.288039 (2.210261)\n",
            "Batch 500/3125\t Loss 1.941464 (2.219619)\n",
            "Batch 600/3125\t Loss 2.555404 (2.216509)\n",
            "Batch 700/3125\t Loss 2.246465 (2.224078)\n",
            "Batch 800/3125\t Loss 1.469915 (2.210397)\n",
            "Batch 900/3125\t Loss 2.068722 (2.214182)\n",
            "Batch 1000/3125\t Loss 2.827221 (2.213149)\n",
            "Batch 1100/3125\t Loss 1.634080 (2.209882)\n",
            "Batch 1200/3125\t Loss 2.024979 (2.200465)\n",
            "Batch 1300/3125\t Loss 1.778430 (2.201732)\n",
            "Batch 1400/3125\t Loss 2.752455 (2.201636)\n",
            "Batch 1500/3125\t Loss 1.580931 (2.199024)\n",
            "Batch 1600/3125\t Loss 2.234423 (2.197801)\n",
            "Batch 1700/3125\t Loss 1.460605 (2.196928)\n",
            "Batch 1800/3125\t Loss 2.200599 (2.199913)\n",
            "Batch 1900/3125\t Loss 2.708189 (2.197145)\n",
            "Batch 2000/3125\t Loss 2.317748 (2.199112)\n",
            "Batch 2100/3125\t Loss 4.168490 (2.197185)\n",
            "Batch 2200/3125\t Loss 1.922678 (2.197623)\n",
            "Batch 2300/3125\t Loss 1.234106 (2.195726)\n",
            "Batch 2400/3125\t Loss 1.436198 (2.192411)\n",
            "Batch 2500/3125\t Loss 2.343826 (2.192078)\n",
            "Batch 2600/3125\t Loss 1.635503 (2.190018)\n",
            "Batch 2700/3125\t Loss 2.146403 (2.193140)\n",
            "Batch 2800/3125\t Loss 1.482647 (2.192265)\n",
            "Batch 2900/3125\t Loss 1.517065 (2.187401)\n",
            "Batch 3000/3125\t Loss 2.767317 (2.189512)\n",
            "Batch 3100/3125\t Loss 1.193822 (2.188772)\n",
            "==> Test\n",
            "       TNR    AUROC  DTACC  AUIN   AUOUT \n",
            "Bas    15.810 72.988 67.080 74.960 69.375\n",
            "Acc: 70.00000\n",
            "==> Epoch 21/30\n",
            "Batch 100/3125\t Loss 1.774779 (2.197303)\n",
            "Batch 200/3125\t Loss 2.265041 (2.199117)\n",
            "Batch 300/3125\t Loss 3.655816 (2.194705)\n",
            "Batch 400/3125\t Loss 1.991506 (2.215824)\n",
            "Batch 500/3125\t Loss 1.845442 (2.211401)\n",
            "Batch 600/3125\t Loss 0.762184 (2.201764)\n",
            "Batch 700/3125\t Loss 0.716644 (2.203887)\n",
            "Batch 800/3125\t Loss 1.747156 (2.202490)\n",
            "Batch 900/3125\t Loss 1.723572 (2.199626)\n",
            "Batch 1000/3125\t Loss 2.438738 (2.200853)\n",
            "Batch 1100/3125\t Loss 2.358888 (2.190562)\n",
            "Batch 1200/3125\t Loss 1.791662 (2.191204)\n",
            "Batch 1300/3125\t Loss 1.822998 (2.193192)\n",
            "Batch 1400/3125\t Loss 2.289449 (2.193408)\n",
            "Batch 1500/3125\t Loss 2.771237 (2.189455)\n",
            "Batch 1600/3125\t Loss 2.481035 (2.184891)\n",
            "Batch 1700/3125\t Loss 2.289376 (2.183540)\n",
            "Batch 1800/3125\t Loss 1.606438 (2.184498)\n",
            "Batch 1900/3125\t Loss 2.469772 (2.188253)\n",
            "Batch 2000/3125\t Loss 3.934663 (2.187754)\n",
            "Batch 2100/3125\t Loss 2.197228 (2.183195)\n",
            "Batch 2200/3125\t Loss 2.826562 (2.181821)\n",
            "Batch 2300/3125\t Loss 1.983598 (2.182018)\n",
            "Batch 2400/3125\t Loss 2.359387 (2.180375)\n",
            "Batch 2500/3125\t Loss 2.932566 (2.180668)\n",
            "Batch 2600/3125\t Loss 1.764680 (2.178878)\n",
            "Batch 2700/3125\t Loss 1.927509 (2.176915)\n",
            "Batch 2800/3125\t Loss 2.114023 (2.175741)\n",
            "Batch 2900/3125\t Loss 2.316491 (2.175110)\n",
            "Batch 3000/3125\t Loss 1.146549 (2.172471)\n",
            "Batch 3100/3125\t Loss 1.378804 (2.176150)\n",
            "==> Epoch 22/30\n",
            "Batch 100/3125\t Loss 1.278533 (2.151812)\n",
            "Batch 200/3125\t Loss 2.824109 (2.164845)\n",
            "Batch 300/3125\t Loss 0.822825 (2.179053)\n",
            "Batch 400/3125\t Loss 1.628389 (2.190987)\n",
            "Batch 500/3125\t Loss 1.840343 (2.187600)\n",
            "Batch 600/3125\t Loss 2.916421 (2.185481)\n",
            "Batch 700/3125\t Loss 2.318145 (2.183820)\n",
            "Batch 800/3125\t Loss 2.433267 (2.173764)\n",
            "Batch 900/3125\t Loss 3.336151 (2.191187)\n",
            "Batch 1000/3125\t Loss 1.699947 (2.187209)\n",
            "Batch 1100/3125\t Loss 1.252452 (2.187079)\n",
            "Batch 1200/3125\t Loss 2.179657 (2.188137)\n",
            "Batch 1300/3125\t Loss 2.205638 (2.184375)\n",
            "Batch 1400/3125\t Loss 2.437740 (2.178119)\n",
            "Batch 1500/3125\t Loss 2.576659 (2.173661)\n",
            "Batch 1600/3125\t Loss 2.370817 (2.179597)\n",
            "Batch 1700/3125\t Loss 1.550309 (2.184861)\n",
            "Batch 1800/3125\t Loss 1.545777 (2.185571)\n",
            "Batch 1900/3125\t Loss 1.628145 (2.187655)\n",
            "Batch 2000/3125\t Loss 1.789514 (2.182724)\n",
            "Batch 2100/3125\t Loss 2.030056 (2.182335)\n",
            "Batch 2200/3125\t Loss 2.264670 (2.182615)\n",
            "Batch 2300/3125\t Loss 3.571312 (2.180444)\n",
            "Batch 2400/3125\t Loss 1.210895 (2.179802)\n",
            "Batch 2500/3125\t Loss 3.186464 (2.177887)\n",
            "Batch 2600/3125\t Loss 2.395512 (2.182787)\n",
            "Batch 2700/3125\t Loss 1.292281 (2.182218)\n",
            "Batch 2800/3125\t Loss 1.153519 (2.181998)\n",
            "Batch 2900/3125\t Loss 1.980870 (2.184923)\n",
            "Batch 3000/3125\t Loss 1.735439 (2.184307)\n",
            "Batch 3100/3125\t Loss 2.571734 (2.185592)\n",
            "==> Epoch 23/30\n",
            "Batch 100/3125\t Loss 1.107236 (2.118171)\n",
            "Batch 200/3125\t Loss 2.771741 (2.174483)\n",
            "Batch 300/3125\t Loss 2.272504 (2.196229)\n",
            "Batch 400/3125\t Loss 2.588262 (2.204388)\n",
            "Batch 500/3125\t Loss 2.721521 (2.207536)\n",
            "Batch 600/3125\t Loss 1.215651 (2.195841)\n",
            "Batch 700/3125\t Loss 2.417863 (2.203882)\n",
            "Batch 800/3125\t Loss 1.322784 (2.200984)\n",
            "Batch 900/3125\t Loss 2.391197 (2.194202)\n",
            "Batch 1000/3125\t Loss 1.804492 (2.197901)\n",
            "Batch 1100/3125\t Loss 1.897519 (2.191864)\n",
            "Batch 1200/3125\t Loss 2.416241 (2.187028)\n",
            "Batch 1300/3125\t Loss 1.908419 (2.197054)\n",
            "Batch 1400/3125\t Loss 3.318339 (2.200641)\n",
            "Batch 1500/3125\t Loss 1.329320 (2.194072)\n",
            "Batch 1600/3125\t Loss 1.402873 (2.195859)\n",
            "Batch 1700/3125\t Loss 2.130722 (2.196581)\n",
            "Batch 1800/3125\t Loss 2.029211 (2.191893)\n",
            "Batch 1900/3125\t Loss 2.157912 (2.189394)\n",
            "Batch 2000/3125\t Loss 1.541984 (2.193986)\n",
            "Batch 2100/3125\t Loss 1.671382 (2.195689)\n",
            "Batch 2200/3125\t Loss 1.934343 (2.194515)\n",
            "Batch 2300/3125\t Loss 2.907937 (2.193625)\n",
            "Batch 2400/3125\t Loss 2.217932 (2.192400)\n",
            "Batch 2500/3125\t Loss 2.534655 (2.191659)\n",
            "Batch 2600/3125\t Loss 2.561595 (2.190709)\n",
            "Batch 2700/3125\t Loss 2.214522 (2.190293)\n",
            "Batch 2800/3125\t Loss 2.227731 (2.190681)\n",
            "Batch 2900/3125\t Loss 2.982768 (2.188181)\n",
            "Batch 3000/3125\t Loss 3.225871 (2.191481)\n",
            "Batch 3100/3125\t Loss 2.409744 (2.190953)\n",
            "==> Epoch 24/30\n",
            "Batch 100/3125\t Loss 2.916232 (2.274453)\n",
            "Batch 200/3125\t Loss 1.361446 (2.148706)\n",
            "Batch 300/3125\t Loss 1.053556 (2.181464)\n",
            "Batch 400/3125\t Loss 2.058898 (2.187559)\n",
            "Batch 500/3125\t Loss 1.112621 (2.169424)\n",
            "Batch 600/3125\t Loss 2.772645 (2.172906)\n",
            "Batch 700/3125\t Loss 3.120204 (2.168121)\n",
            "Batch 800/3125\t Loss 2.213573 (2.172379)\n",
            "Batch 900/3125\t Loss 0.847389 (2.182183)\n",
            "Batch 1000/3125\t Loss 1.688605 (2.193876)\n",
            "Batch 1100/3125\t Loss 2.672624 (2.188648)\n",
            "Batch 1200/3125\t Loss 2.640050 (2.191314)\n",
            "Batch 1300/3125\t Loss 1.588676 (2.188262)\n",
            "Batch 1400/3125\t Loss 2.099587 (2.186732)\n",
            "Batch 1500/3125\t Loss 2.783111 (2.186235)\n",
            "Batch 1600/3125\t Loss 2.185787 (2.180592)\n",
            "Batch 1700/3125\t Loss 1.413771 (2.184547)\n",
            "Batch 1800/3125\t Loss 1.920072 (2.178298)\n",
            "Batch 1900/3125\t Loss 2.347092 (2.180979)\n",
            "Batch 2000/3125\t Loss 2.059050 (2.176827)\n",
            "Batch 2100/3125\t Loss 1.603981 (2.172542)\n",
            "Batch 2200/3125\t Loss 1.611705 (2.170759)\n",
            "Batch 2300/3125\t Loss 1.887958 (2.165547)\n",
            "Batch 2400/3125\t Loss 2.319484 (2.171553)\n",
            "Batch 2500/3125\t Loss 1.760873 (2.173690)\n",
            "Batch 2600/3125\t Loss 1.845344 (2.174322)\n",
            "Batch 2700/3125\t Loss 3.143115 (2.170390)\n",
            "Batch 2800/3125\t Loss 2.233402 (2.169500)\n",
            "Batch 2900/3125\t Loss 2.395298 (2.168364)\n",
            "Batch 3000/3125\t Loss 3.260653 (2.167446)\n",
            "Batch 3100/3125\t Loss 2.595884 (2.169987)\n",
            "==> Epoch 25/30\n",
            "Batch 100/3125\t Loss 1.985227 (2.176831)\n",
            "Batch 200/3125\t Loss 1.192353 (2.192626)\n",
            "Batch 300/3125\t Loss 2.096412 (2.198207)\n",
            "Batch 400/3125\t Loss 1.994019 (2.191526)\n",
            "Batch 500/3125\t Loss 2.045757 (2.188581)\n",
            "Batch 600/3125\t Loss 1.970586 (2.178536)\n",
            "Batch 700/3125\t Loss 2.567083 (2.189917)\n",
            "Batch 800/3125\t Loss 1.189548 (2.201574)\n",
            "Batch 900/3125\t Loss 1.702618 (2.195242)\n",
            "Batch 1000/3125\t Loss 1.809987 (2.188688)\n",
            "Batch 1100/3125\t Loss 2.014495 (2.182663)\n",
            "Batch 1200/3125\t Loss 2.079934 (2.177818)\n",
            "Batch 1300/3125\t Loss 1.438349 (2.181638)\n",
            "Batch 1400/3125\t Loss 3.253941 (2.183336)\n",
            "Batch 1500/3125\t Loss 1.922192 (2.184216)\n",
            "Batch 1600/3125\t Loss 2.904861 (2.189406)\n",
            "Batch 1700/3125\t Loss 3.536647 (2.187073)\n",
            "Batch 1800/3125\t Loss 2.201602 (2.183918)\n",
            "Batch 1900/3125\t Loss 2.479757 (2.182651)\n",
            "Batch 2000/3125\t Loss 2.550979 (2.180069)\n",
            "Batch 2100/3125\t Loss 3.311202 (2.176295)\n",
            "Batch 2200/3125\t Loss 1.355327 (2.176848)\n",
            "Batch 2300/3125\t Loss 1.108239 (2.175534)\n",
            "Batch 2400/3125\t Loss 2.775347 (2.174687)\n",
            "Batch 2500/3125\t Loss 2.699648 (2.172608)\n",
            "Batch 2600/3125\t Loss 1.648361 (2.175351)\n",
            "Batch 2700/3125\t Loss 1.971179 (2.179257)\n",
            "Batch 2800/3125\t Loss 2.311348 (2.180591)\n",
            "Batch 2900/3125\t Loss 1.941353 (2.180357)\n",
            "Batch 3000/3125\t Loss 2.400142 (2.180569)\n",
            "Batch 3100/3125\t Loss 2.139672 (2.181662)\n",
            "==> Test\n",
            "       TNR    AUROC  DTACC  AUIN   AUOUT \n",
            "Bas    16.030 72.787 67.110 74.022 69.406\n",
            "Acc: 70.57000\n",
            "Best Acc (%): 70.570\t\n",
            "==> Epoch 26/30\n",
            "Batch 100/3125\t Loss 1.275688 (2.167229)\n",
            "Batch 200/3125\t Loss 3.127447 (2.149425)\n",
            "Batch 300/3125\t Loss 1.718598 (2.190303)\n",
            "Batch 400/3125\t Loss 2.016580 (2.174180)\n",
            "Batch 500/3125\t Loss 1.538029 (2.169569)\n",
            "Batch 600/3125\t Loss 2.338576 (2.172172)\n",
            "Batch 700/3125\t Loss 1.859463 (2.186081)\n",
            "Batch 800/3125\t Loss 1.649558 (2.187116)\n",
            "Batch 900/3125\t Loss 2.290696 (2.186904)\n",
            "Batch 1000/3125\t Loss 2.076258 (2.183056)\n",
            "Batch 1100/3125\t Loss 1.458080 (2.195672)\n",
            "Batch 1200/3125\t Loss 1.945124 (2.192884)\n",
            "Batch 1300/3125\t Loss 2.048596 (2.198854)\n",
            "Batch 1400/3125\t Loss 1.996031 (2.206504)\n",
            "Batch 1500/3125\t Loss 1.445035 (2.206831)\n",
            "Batch 1600/3125\t Loss 1.568271 (2.202871)\n",
            "Batch 1700/3125\t Loss 2.088787 (2.200992)\n",
            "Batch 1800/3125\t Loss 2.041671 (2.202272)\n",
            "Batch 1900/3125\t Loss 2.786967 (2.199404)\n",
            "Batch 2000/3125\t Loss 2.236973 (2.198623)\n",
            "Batch 2100/3125\t Loss 2.838206 (2.195476)\n",
            "Batch 2200/3125\t Loss 2.441931 (2.193444)\n",
            "Batch 2300/3125\t Loss 1.577527 (2.188401)\n",
            "Batch 2400/3125\t Loss 2.267636 (2.190034)\n",
            "Batch 2500/3125\t Loss 1.961235 (2.192357)\n",
            "Batch 2600/3125\t Loss 1.862061 (2.189328)\n",
            "Batch 2700/3125\t Loss 2.250298 (2.190370)\n",
            "Batch 2800/3125\t Loss 1.639550 (2.187603)\n",
            "Batch 2900/3125\t Loss 3.101131 (2.185341)\n",
            "Batch 3000/3125\t Loss 1.400310 (2.182618)\n",
            "Batch 3100/3125\t Loss 1.934010 (2.180360)\n",
            "==> Epoch 27/30\n",
            "Batch 100/3125\t Loss 1.768562 (2.006993)\n",
            "Batch 200/3125\t Loss 2.173310 (2.099917)\n",
            "Batch 300/3125\t Loss 0.861007 (2.102315)\n",
            "Batch 400/3125\t Loss 2.474761 (2.107564)\n",
            "Batch 500/3125\t Loss 2.630165 (2.137902)\n",
            "Batch 600/3125\t Loss 1.494685 (2.137343)\n",
            "Batch 700/3125\t Loss 2.293776 (2.135893)\n",
            "Batch 800/3125\t Loss 2.835794 (2.140337)\n",
            "Batch 900/3125\t Loss 3.706032 (2.156104)\n",
            "Batch 1000/3125\t Loss 2.350913 (2.155638)\n",
            "Batch 1100/3125\t Loss 0.769454 (2.160549)\n",
            "Batch 1200/3125\t Loss 2.507543 (2.164840)\n",
            "Batch 1300/3125\t Loss 1.584173 (2.163458)\n",
            "Batch 1400/3125\t Loss 2.372413 (2.171160)\n",
            "Batch 1500/3125\t Loss 2.058528 (2.176286)\n",
            "Batch 1600/3125\t Loss 2.014329 (2.176536)\n",
            "Batch 1700/3125\t Loss 1.846121 (2.167359)\n",
            "Batch 1800/3125\t Loss 2.832599 (2.169132)\n",
            "Batch 1900/3125\t Loss 1.909339 (2.174891)\n",
            "Batch 2000/3125\t Loss 3.090769 (2.175348)\n",
            "Batch 2100/3125\t Loss 2.281509 (2.170946)\n",
            "Batch 2200/3125\t Loss 2.663777 (2.175519)\n",
            "Batch 2300/3125\t Loss 1.873392 (2.173484)\n",
            "Batch 2400/3125\t Loss 2.922855 (2.175151)\n",
            "Batch 2500/3125\t Loss 2.875043 (2.175877)\n",
            "Batch 2600/3125\t Loss 1.071319 (2.175858)\n",
            "Batch 2700/3125\t Loss 2.104558 (2.174861)\n",
            "Batch 2800/3125\t Loss 0.812660 (2.171824)\n",
            "Batch 2900/3125\t Loss 1.557897 (2.172153)\n",
            "Batch 3000/3125\t Loss 2.243654 (2.173358)\n",
            "Batch 3100/3125\t Loss 2.222073 (2.175524)\n",
            "==> Epoch 28/30\n",
            "Batch 100/3125\t Loss 1.161540 (2.084313)\n",
            "Batch 200/3125\t Loss 1.917289 (2.147082)\n",
            "Batch 300/3125\t Loss 2.316634 (2.130670)\n",
            "Batch 400/3125\t Loss 1.506665 (2.115814)\n",
            "Batch 500/3125\t Loss 2.235886 (2.135783)\n",
            "Batch 600/3125\t Loss 1.787428 (2.129617)\n",
            "Batch 700/3125\t Loss 1.646871 (2.128999)\n",
            "Batch 800/3125\t Loss 2.426424 (2.132911)\n",
            "Batch 900/3125\t Loss 1.929788 (2.125801)\n",
            "Batch 1000/3125\t Loss 1.513349 (2.129318)\n",
            "Batch 1100/3125\t Loss 2.756070 (2.138154)\n",
            "Batch 1200/3125\t Loss 2.483554 (2.140640)\n",
            "Batch 1300/3125\t Loss 2.220321 (2.148042)\n",
            "Batch 1400/3125\t Loss 2.032508 (2.146973)\n",
            "Batch 1500/3125\t Loss 2.442154 (2.147840)\n",
            "Batch 1600/3125\t Loss 2.430450 (2.151083)\n",
            "Batch 1700/3125\t Loss 2.216982 (2.156102)\n",
            "Batch 1800/3125\t Loss 1.870180 (2.157550)\n",
            "Batch 1900/3125\t Loss 2.590296 (2.159374)\n",
            "Batch 2000/3125\t Loss 2.768987 (2.161997)\n",
            "Batch 2100/3125\t Loss 1.905245 (2.161935)\n",
            "Batch 2200/3125\t Loss 1.409636 (2.162109)\n",
            "Batch 2300/3125\t Loss 2.027755 (2.158232)\n",
            "Batch 2400/3125\t Loss 2.848525 (2.154538)\n",
            "Batch 2500/3125\t Loss 1.295507 (2.153995)\n",
            "Batch 2600/3125\t Loss 1.356484 (2.151449)\n",
            "Batch 2700/3125\t Loss 2.801473 (2.155540)\n",
            "Batch 2800/3125\t Loss 1.265047 (2.157543)\n",
            "Batch 2900/3125\t Loss 2.630222 (2.156318)\n",
            "Batch 3000/3125\t Loss 1.634627 (2.159610)\n",
            "Batch 3100/3125\t Loss 2.215571 (2.159992)\n",
            "==> Epoch 29/30\n",
            "Batch 100/3125\t Loss 2.246039 (2.180364)\n",
            "Batch 200/3125\t Loss 1.777004 (2.135790)\n",
            "Batch 300/3125\t Loss 1.275191 (2.165973)\n",
            "Batch 400/3125\t Loss 3.308131 (2.170543)\n",
            "Batch 500/3125\t Loss 2.104568 (2.183223)\n",
            "Batch 600/3125\t Loss 3.456686 (2.186818)\n",
            "Batch 700/3125\t Loss 1.787723 (2.192167)\n",
            "Batch 800/3125\t Loss 3.465966 (2.182079)\n",
            "Batch 900/3125\t Loss 1.756767 (2.194621)\n",
            "Batch 1000/3125\t Loss 2.748121 (2.191427)\n",
            "Batch 1100/3125\t Loss 1.472765 (2.184303)\n",
            "Batch 1200/3125\t Loss 2.156886 (2.181764)\n",
            "Batch 1300/3125\t Loss 2.739975 (2.191922)\n",
            "Batch 1400/3125\t Loss 1.781864 (2.191905)\n",
            "Batch 1500/3125\t Loss 2.872262 (2.194374)\n",
            "Batch 1600/3125\t Loss 1.899221 (2.190661)\n",
            "Batch 1700/3125\t Loss 2.056412 (2.189837)\n",
            "Batch 1800/3125\t Loss 1.752030 (2.189014)\n",
            "Batch 1900/3125\t Loss 1.236965 (2.188628)\n",
            "Batch 2000/3125\t Loss 1.584897 (2.194536)\n",
            "Batch 2100/3125\t Loss 2.277313 (2.189729)\n",
            "Batch 2200/3125\t Loss 2.804338 (2.185668)\n",
            "Batch 2300/3125\t Loss 1.584744 (2.182242)\n",
            "Batch 2400/3125\t Loss 2.814564 (2.183563)\n",
            "Batch 2500/3125\t Loss 1.766301 (2.181841)\n",
            "Batch 2600/3125\t Loss 1.258208 (2.177701)\n",
            "Batch 2700/3125\t Loss 1.990815 (2.180764)\n",
            "Batch 2800/3125\t Loss 1.852623 (2.179230)\n",
            "Batch 2900/3125\t Loss 1.890366 (2.177823)\n",
            "Batch 3000/3125\t Loss 3.943363 (2.182018)\n",
            "Batch 3100/3125\t Loss 2.262941 (2.184165)\n",
            "==> Epoch 30/30\n",
            "Batch 100/3125\t Loss 1.555591 (2.163419)\n",
            "Batch 200/3125\t Loss 2.463867 (2.127193)\n",
            "Batch 300/3125\t Loss 2.858237 (2.104300)\n",
            "Batch 400/3125\t Loss 2.065798 (2.119947)\n",
            "Batch 500/3125\t Loss 2.244588 (2.140576)\n",
            "Batch 600/3125\t Loss 2.420384 (2.156696)\n",
            "Batch 700/3125\t Loss 1.713105 (2.160512)\n",
            "Batch 800/3125\t Loss 2.241740 (2.172713)\n",
            "Batch 900/3125\t Loss 1.767151 (2.164048)\n",
            "Batch 1000/3125\t Loss 3.264119 (2.164521)\n",
            "Batch 1100/3125\t Loss 1.717499 (2.167800)\n",
            "Batch 1200/3125\t Loss 2.121785 (2.168844)\n",
            "Batch 1300/3125\t Loss 1.769078 (2.162503)\n",
            "Batch 1400/3125\t Loss 3.021462 (2.165601)\n",
            "Batch 1500/3125\t Loss 2.043839 (2.164656)\n",
            "Batch 1600/3125\t Loss 1.530399 (2.167678)\n",
            "Batch 1700/3125\t Loss 3.183750 (2.169662)\n",
            "Batch 1800/3125\t Loss 1.878145 (2.164111)\n",
            "Batch 1900/3125\t Loss 3.358923 (2.155401)\n",
            "Batch 2000/3125\t Loss 2.144283 (2.157385)\n",
            "Batch 2100/3125\t Loss 2.191492 (2.160555)\n",
            "Batch 2200/3125\t Loss 2.229276 (2.161235)\n",
            "Batch 2300/3125\t Loss 2.191594 (2.162411)\n",
            "Batch 2400/3125\t Loss 1.802853 (2.161197)\n",
            "Batch 2500/3125\t Loss 2.327040 (2.160625)\n",
            "Batch 2600/3125\t Loss 2.375547 (2.164215)\n",
            "Batch 2700/3125\t Loss 0.811840 (2.166941)\n",
            "Batch 2800/3125\t Loss 2.119361 (2.164521)\n",
            "Batch 2900/3125\t Loss 2.147989 (2.164732)\n",
            "Batch 3000/3125\t Loss 2.994808 (2.163838)\n",
            "Batch 3100/3125\t Loss 2.427724 (2.164494)\n",
            "==> Test\n",
            "       TNR    AUROC  DTACC  AUIN   AUOUT \n",
            "Bas    18.050 74.429 68.260 75.863 71.116\n",
            "Acc: 71.85000\n",
            "Best Acc (%): 71.850\t\n",
            "Finished. Total elapsed time (h:m:s): 1:33:48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if options['eval']:\n",
        "        net, criterion = load_networks(net, options['outf'], file_name, criterion=criterion)\n",
        "        outloaders = Data.out_loaders\n",
        "        results = test(net, criterion, testloader, outloader, epoch=0, **options)\n",
        "        acc = results['ACC']\n",
        "        res = dict()\n",
        "        res['ACC'] = dict()\n",
        "        acc_res = []\n",
        "        for key in Data.out_keys:\n",
        "            results = test_robustness(net, criterion, outloaders[key], epoch=0, label=key, **options)\n",
        "            print('{} (%): {:.3f}\\t'.format(key, results['ACC']))\n",
        "            res['ACC'][key] = results['ACC']\n",
        "            acc_res.append(results['ACC'])\n",
        "        print('Mean ACC:', np.mean(acc_res))\n",
        "        print('Mean Error:', 100-np.mean(acc_res))"
      ],
      "metadata": {
        "id": "mqmZ2gN1vKcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e17f950-e6a9-4e9d-fae6-6604305fb05e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       TNR    AUROC  DTACC  AUIN   AUOUT \n",
            "Bas    18.050 74.429 68.260 75.863 71.116\n",
            "Acc: 71.85000\n",
            "gaussian_noise (%): 38.792\t\n",
            "shot_noise (%): 44.336\t\n",
            "impulse_noise (%): 38.070\t\n",
            "defocus_blur (%): 56.062\t\n",
            "glass_blur (%): 25.998\t\n",
            "motion_blur (%): 52.868\t\n",
            "zoom_blur (%): 47.950\t\n",
            "snow (%): 54.562\t\n",
            "frost (%): 52.212\t\n",
            "fog (%): 61.650\t\n",
            "brightness (%): 70.192\t\n",
            "contrast (%): 47.092\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "options['aug'] ='aprs'\n",
        "\n",
        "print(\"Creating model: {}\".format(options['model']))\n",
        "if 'wide_resnet' in options['model']:\n",
        "        print('wide_resnet') \n",
        "        net = WideResNet(40, num_classes, 2, 0.0)\n",
        "elif 'allconv' in options['model']:\n",
        "        print('allconv')\n",
        "        net = AllConvNet(num_classes)\n",
        "elif 'densenet' in options['model']:\n",
        "        print('densenet')\n",
        "        net = densenet(num_classes=num_classes)\n",
        "elif 'resnext' in options['model']:\n",
        "        print('resnext29')\n",
        "        net = resnext29(num_classes)\n",
        "else:\n",
        "        print('resnet18')\n",
        "        net = ResNet18(num_classes=num_classes)\n",
        "\n",
        "torch.manual_seed(options['seed'])\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = options['gpu']\n",
        "use_gpu = torch.cuda.is_available()\n",
        "if options['use_cpu']: use_gpu = False\n",
        "\n",
        "options.update({'use_gpu': use_gpu})\n",
        "options['use_gpu']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "if use_gpu:\n",
        "        net = nn.DataParallel(net, device_ids=[i for i in range(len(options['gpu'].split(',')))]).cuda()\n",
        "        criterion = criterion.cuda()\n",
        "file_name = '{}_{}_{}'.format(options['model'], options['dataset'], options['aug'])\n",
        "params_list = [{'params': net.parameters()},\n",
        "                {'params': criterion.parameters()}]\n",
        "optimizer = torch.optim.SGD(params_list, lr=options['lr'], momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
        "scheduler = lr_scheduler.MultiStepLR(optimizer, gamma=0.2, milestones=[60, 120, 160, 190])\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "best_acc = 0.0"
      ],
      "metadata": {
        "id": "tBUBN1zO08tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(options['max_epoch']):\n",
        "        print(\"==> Epoch {}/{}\".format(epoch+1, options['max_epoch']))\n",
        "\n",
        "        train(net, criterion, optimizer, trainloader, epoch=epoch, **options)\n",
        "\n",
        "        if options['eval_freq'] > 0 and (epoch+1) % options['eval_freq'] == 0 or (epoch+1) == options['max_epoch'] or epoch > 160:\n",
        "            print(\"==> Test\")\n",
        "            results = test(net, criterion, testloader, outloader, epoch=epoch, **options)\n",
        "\n",
        "            if best_acc < results['ACC']:\n",
        "                best_acc = results['ACC']\n",
        "                print(\"Best Acc (%): {:.3f}\\t\".format(best_acc))\n",
        "            \n",
        "            save_networks(net, options['outf'], file_name, criterion=criterion)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "elapsed = round(time.time() - start_time)\n",
        "elapsed = str(datetime.timedelta(seconds=elapsed))\n",
        "print(\"Finished. Total elapsed time (h:m:s): {}\".format(elapsed))"
      ],
      "metadata": {
        "id": "jmz0PTNOWowV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if options['eval']:\n",
        "        net, criterion = load_networks(net, options['outf'], file_name, criterion=criterion)\n",
        "        outloaders = Data.out_loaders\n",
        "        results = test(net, criterion, testloader, outloader, epoch=0, **options)\n",
        "        acc = results['ACC']\n",
        "        res = dict()\n",
        "        res['ACC'] = dict()\n",
        "        acc_res = []\n",
        "        for key in Data.out_keys:\n",
        "            results = test_robustness(net, criterion, outloaders[key], epoch=0, label=key, **options)\n",
        "            print('{} (%): {:.3f}\\t'.format(key, results['ACC']))\n",
        "            res['ACC'][key] = results['ACC']\n",
        "            acc_res.append(results['ACC'])\n",
        "        print('Mean ACC:', np.mean(acc_res))\n",
        "        print('Mean Error:', 100-np.mean(acc_res))"
      ],
      "metadata": {
        "id": "7tTiict2s8mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IRxS5jfTAQlV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}