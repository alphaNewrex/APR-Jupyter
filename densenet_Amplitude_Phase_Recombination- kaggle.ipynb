{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","gpuClass":"standard"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport argparse\nimport datetime\nimport time\nimport csv\nimport os.path as osp\nimport numpy as np\nimport warnings\nimport errno\nimport importlib\nimport pandas as pd","metadata":{"id":"v-E1ObShkKrG","execution":{"iopub.status.busy":"2023-04-24T19:30:54.652252Z","iopub.execute_input":"2023-04-24T19:30:54.652855Z","iopub.status.idle":"2023-04-24T19:30:54.658144Z","shell.execute_reply.started":"2023-04-24T19:30:54.652817Z","shell.execute_reply":"2023-04-24T19:30:54.657049Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings('ignore')","metadata":{"id":"_yp8KiUpkgzv","execution":{"iopub.status.busy":"2023-04-24T19:30:55.113832Z","iopub.execute_input":"2023-04-24T19:30:55.114871Z","iopub.status.idle":"2023-04-24T19:30:55.120116Z","shell.execute_reply.started":"2023-04-24T19:30:55.114816Z","shell.execute_reply":"2023-04-24T19:30:55.118759Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.optim import lr_scheduler\nimport torch.backends.cudnn as cudnn\nimport torchvision","metadata":{"id":"rBN4J-w8kmqG","execution":{"iopub.status.busy":"2023-04-24T19:30:55.699179Z","iopub.execute_input":"2023-04-24T19:30:55.699886Z","iopub.status.idle":"2023-04-24T19:30:57.947445Z","shell.execute_reply.started":"2023-04-24T19:30:55.699841Z","shell.execute_reply":"2023-04-24T19:30:57.946430Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\"\"\"Base augmentations operators.\"\"\"\n\nimport numpy as np\nfrom PIL import Image, ImageOps, ImageEnhance\n\n# ImageNet code should change this value\nIMAGE_SIZE = 32\n\n\ndef int_parameter(level, maxval):\n  \"\"\"Helper function to scale `val` between 0 and maxval .\n  Args:\n    level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n    maxval: Maximum value that the operation can have. This will be scaled to\n      level/PARAMETER_MAX.\n  Returns:\n    An int that results from scaling `maxval` according to `level`.\n  \"\"\"\n  return int(level * maxval / 10)\n\n\ndef float_parameter(level, maxval):\n  \"\"\"Helper function to scale `val` between 0 and maxval.\n  Args:\n    level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n    maxval: Maximum value that the operation can have. This will be scaled to\n      level/PARAMETER_MAX.\n  Returns:\n    A float that results from scaling `maxval` according to `level`.\n  \"\"\"\n  return float(level) * maxval / 10.\n\n\ndef sample_level(n):\n  return np.random.uniform(low=0.1, high=n)\n\n\ndef autocontrast(pil_img, _):\n  return ImageOps.autocontrast(pil_img)\n\n\ndef equalize(pil_img, _):\n  return ImageOps.equalize(pil_img)\n\n\ndef posterize(pil_img, level):\n  level = int_parameter(sample_level(level), 4)\n  return ImageOps.posterize(pil_img, 4 - level)\n\n\ndef rotate(pil_img, level):\n  degrees = int_parameter(sample_level(level), 30)\n  if np.random.uniform() > 0.5:\n    degrees = -degrees\n  return pil_img.rotate(degrees, resample=Image.BILINEAR)\n\n\ndef solarize(pil_img, level):\n  level = int_parameter(sample_level(level), 256)\n  return ImageOps.solarize(pil_img, 256 - level)\n\n\ndef shear_x(pil_img, level):\n  level = float_parameter(sample_level(level), 0.3)\n  if np.random.uniform() > 0.5:\n    level = -level\n  return pil_img.transform((IMAGE_SIZE, IMAGE_SIZE),\n                           Image.AFFINE, (1, level, 0, 0, 1, 0),\n                           resample=Image.BILINEAR)\n\n\ndef shear_y(pil_img, level):\n  level = float_parameter(sample_level(level), 0.3)\n  if np.random.uniform() > 0.5:\n    level = -level\n  return pil_img.transform((IMAGE_SIZE, IMAGE_SIZE),\n                           Image.AFFINE, (1, 0, 0, level, 1, 0),\n                           resample=Image.BILINEAR)\n\n\ndef translate_x(pil_img, level):\n  level = int_parameter(sample_level(level), IMAGE_SIZE / 3)\n  if np.random.random() > 0.5:\n    level = -level\n  return pil_img.transform((IMAGE_SIZE, IMAGE_SIZE),\n                           Image.AFFINE, (1, 0, level, 0, 1, 0),\n                           resample=Image.BILINEAR)\n\n\ndef translate_y(pil_img, level):\n  level = int_parameter(sample_level(level), IMAGE_SIZE / 3)\n  if np.random.random() > 0.5:\n    level = -level\n  return pil_img.transform((IMAGE_SIZE, IMAGE_SIZE),\n                           Image.AFFINE, (1, 0, 0, 0, 1, level),\n                           resample=Image.BILINEAR)\n\n\n# operation that overlaps with ImageNet-C's test set\ndef color(pil_img, level):\n    level = float_parameter(sample_level(level), 1.8) + 0.1\n    return ImageEnhance.Color(pil_img).enhance(level)\n\n\n# operation that overlaps with ImageNet-C's test set\ndef contrast(pil_img, level):\n    level = float_parameter(sample_level(level), 1.8) + 0.1\n    return ImageEnhance.Contrast(pil_img).enhance(level)\n\n\n# operation that overlaps with ImageNet-C's test set\ndef brightness(pil_img, level):\n    level = float_parameter(sample_level(level), 1.8) + 0.1\n    return ImageEnhance.Brightness(pil_img).enhance(level)\n\n\n# operation that overlaps with ImageNet-C's test set\ndef sharpness(pil_img, level):\n    level = float_parameter(sample_level(level), 1.8) + 0.1\n    return ImageEnhance.Sharpness(pil_img).enhance(level)\n\n\naugmentations = [\n    autocontrast, equalize, posterize, rotate, solarize, shear_x, shear_y,\n    translate_x, translate_y\n]\n\naugmentations_all = [\n    autocontrast, equalize, posterize, rotate, solarize, shear_x, shear_y,\n    translate_x, translate_y, color, contrast, brightness, sharpness\n]","metadata":{"id":"sNZr99VEtShx","execution":{"iopub.status.busy":"2023-04-24T19:30:57.950636Z","iopub.execute_input":"2023-04-24T19:30:57.951527Z","iopub.status.idle":"2023-04-24T19:30:57.970108Z","shell.execute_reply.started":"2023-04-24T19:30:57.951488Z","shell.execute_reply":"2023-04-24T19:30:57.969110Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import io\nimport random\nfrom PIL import Image\nimport numpy as np\n\nclass APRecombination(object):\n    def __init__(self, img_size=32, aug=None):\n        if aug is None:\n            augmentations.IMAGE_SIZE = img_size\n            self.aug_list = augmentations.augmentations\n        else:\n            self.aug_list = aug.augmentations\n\n    def __call__(self, x):\n        '''\n        :param img: (PIL Image): Image\n        :return: code img (PIL Image): Image\n        '''\n\n        op = np.random.choice(self.aug_list)\n        x = op(x, 3)\n\n        p = random.uniform(0, 1)\n        if p > 0.5:\n            return x\n\n        x_aug = x.copy()\n        op = np.random.choice(self.aug_list)\n        x_aug = op(x_aug, 3)\n\n        x = np.array(x).astype(np.uint8) \n        x_aug = np.array(x_aug).astype(np.uint8)\n        \n        fft_1 = np.fft.fftshift(np.fft.fftn(x))\n        fft_2 = np.fft.fftshift(np.fft.fftn(x_aug))\n        \n        abs_1, angle_1 = np.abs(fft_1), np.angle(fft_1)\n        abs_2, angle_2 = np.abs(fft_2), np.angle(fft_2)\n\n        fft_1 = abs_1*np.exp((1j) * angle_2)\n        fft_2 = abs_2*np.exp((1j) * angle_1)\n\n        p = random.uniform(0, 1)\n\n        if p > 0.5:\n            x = np.fft.ifftn(np.fft.ifftshift(fft_1))\n        else:\n            x = np.fft.ifftn(np.fft.ifftshift(fft_2))\n\n        x = x.astype(np.uint8)\n        x = Image.fromarray(x)\n        \n        return x","metadata":{"id":"iY7qbQxLrzc1","execution":{"iopub.status.busy":"2023-04-24T19:30:57.973106Z","iopub.execute_input":"2023-04-24T19:30:57.973807Z","iopub.status.idle":"2023-04-24T19:30:57.985528Z","shell.execute_reply.started":"2023-04-24T19:30:57.973763Z","shell.execute_reply":"2023-04-24T19:30:57.984531Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\n\nnormalize = transforms.Compose([\n        transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]),\n    ])\n\ndef train_transforms(_transforms):\n    transforms_list = []\n    if 'aprs' in _transforms:\n        print('APRecombination', _transforms)\n        transforms_list.extend([\n            transforms.RandomApply([APRecombination()], p=1.0),\n            transforms.RandomCrop(32, padding=4, fill=128),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n        ])\n    else:\n        transforms_list.extend([\n            transforms.RandomCrop(32, padding=4, fill=128),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n        ])\n\n    return transforms_list\n\n\ndef test_transforms():\n    test_transform = transforms.Compose([\n        transforms.ToTensor(),\n    ])\n\n    return test_transform","metadata":{"id":"MjjHKN8ktMKX","execution":{"iopub.status.busy":"2023-04-24T19:30:57.988515Z","iopub.execute_input":"2023-04-24T19:30:57.988882Z","iopub.status.idle":"2023-04-24T19:30:57.998941Z","shell.execute_reply.started":"2023-04-24T19:30:57.988840Z","shell.execute_reply":"2023-04-24T19:30:57.998015Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport pickle\nimport numpy as np\nfrom scipy import signal\nfrom PIL import Image\n\nimport torch\nfrom torchvision import transforms\nfrom torchvision.datasets import CIFAR10, CIFAR100, ImageFolder\n\n\nclass CIFARC(CIFAR10):\n    def __init__(\n            self,\n            root,\n            key = 'zoom_blur',\n            transform = None,\n            target_transform = None,\n    ):\n\n        super(CIFAR10, self).__init__(root, transform=transform,\n                                      target_transform=target_transform)\n\n        data_path = os.path.join(root, key+'.npy')\n        labels_path = os.path.join(root, 'labels.npy')\n\n        self.data = np.load(data_path)\n        self.targets = np.load(labels_path)\n\n    def __getitem__(self, index: int):\n        \"\"\"\n        Args:\n            index (int): Index\n        Returns:\n            tuple: (image, target) where target is index of the target class.\n        \"\"\"\n        img, target = self.data[index], self.targets[index]\n\n        img = Image.fromarray(img)\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target\n\nclass CIFAR10D(object):\n    def __init__(self, dataroot='', use_gpu=True, num_workers=4, batch_size=128, _transforms='', _eval=False):\n\n        transforms_list = train_transforms(_transforms)\n\n        train_transform = transforms.Compose(transforms_list)\n        test_transform = test_transforms()\n        self.train_transform = train_transform\n\n        pin_memory = True if use_gpu else False\n\n        data_root = os.path.join(dataroot, 'cifar10')\n\n        trainset = CIFAR10(root=data_root, train=True, download=True, transform=train_transform)\n        \n        self.train_loader = torch.utils.data.DataLoader(\n            trainset, batch_size=batch_size, shuffle=True,\n            num_workers=num_workers, pin_memory=pin_memory,\n        )\n        \n        testset = CIFAR10(root=data_root, train=False, download=True, transform=test_transform)\n        \n        self.test_loader = torch.utils.data.DataLoader(\n            testset, batch_size=batch_size, shuffle=False,\n            num_workers=num_workers, pin_memory=pin_memory,\n        )\n\n        if _eval:\n            self.out_loaders = dict()\n            self.out_keys = ['gaussian_noise', 'shot_noise', 'impulse_noise', 'defocus_blur',\n                            'glass_blur', 'motion_blur', 'zoom_blur', 'snow', 'frost', 'fog',\n                            'brightness', 'contrast', 'elastic_transform', 'pixelate',\n                            'jpeg_compression']\n\n            data_root = os.path.join(dataroot, 'CIFAR-10-C')\n            for key in self.out_keys:\n                outset = CIFARC(root=data_root, key=key, transform=test_transform)\n                out_loader = torch.utils.data.DataLoader(\n                    outset, batch_size=batch_size, shuffle=False,\n                    num_workers=num_workers, pin_memory=pin_memory,\n                )\n                self.out_loaders[key] = out_loader\n        \n        self.num_classes = 10\n\nclass CIFAR100D(object):\n    def __init__(self, dataroot='', use_gpu=True, num_workers=4, batch_size=128, _transforms='', _eval=False):\n\n        transforms_list = train_transforms(_transforms)\n\n        train_transform = transforms.Compose(transforms_list)\n        test_transform = test_transforms()\n        self.train_transform = train_transform\n\n        pin_memory = True if use_gpu else False\n\n        data_root = os.path.join(dataroot, 'cifar100')\n\n        trainset = CIFAR100(root=data_root, train=True, download=True, transform=train_transform)\n        \n        self.train_loader = torch.utils.data.DataLoader(\n            trainset, batch_size=batch_size, shuffle=True,\n            num_workers=num_workers, pin_memory=pin_memory,\n        )\n        \n        testset = CIFAR100(root=data_root, train=False, download=True, transform=test_transform)\n        \n        self.test_loader = torch.utils.data.DataLoader(\n            testset, batch_size=batch_size, shuffle=False,\n            num_workers=num_workers, pin_memory=pin_memory,\n        )\n\n        if _eval:\n            self.out_loaders = dict()\n            self.out_keys = ['gaussian_noise', 'shot_noise', 'impulse_noise', 'defocus_blur',\n                            'glass_blur', 'motion_blur', 'zoom_blur', 'snow', 'frost', 'fog',\n                            'brightness', 'contrast', 'elastic_transform', 'pixelate',\n                            'jpeg_compression']\n\n            data_root = os.path.join(dataroot, 'CIFAR-100-C')\n            for key in self.out_keys:\n                outset = CIFARC(root=data_root, key=key, transform=test_transform)\n                out_loader = torch.utils.data.DataLoader(\n                    outset, batch_size=batch_size, shuffle=False,\n                    num_workers=num_workers, pin_memory=pin_memory,\n                )\n                self.out_loaders[key] = out_loader\n\n        self.num_classes = 100","metadata":{"id":"0KTRpO0HtZxi","execution":{"iopub.status.busy":"2023-04-24T19:30:58.000605Z","iopub.execute_input":"2023-04-24T19:30:58.000964Z","iopub.status.idle":"2023-04-24T19:30:58.424387Z","shell.execute_reply.started":"2023-04-24T19:30:58.000929Z","shell.execute_reply":"2023-04-24T19:30:58.423394Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"'''ResNet in PyTorch.\nFor Pre-activation ResNet, see 'preact_resnet.py'.\nReference:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(\n            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion *\n                               planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512*block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x, rf=False, _eval=False):\n        if _eval:\n            self.eval()\n        else:\n            self.train()\n\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        y = self.linear(out)\n        if rf:\n            return out, y\n        return y\n\n\ndef ResNet18(num_classes):\n    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n\n\ndef ResNet34():\n    return ResNet(BasicBlock, [3, 4, 6, 3])\n\n\ndef ResNet50():\n    return ResNet(Bottleneck, [3, 4, 6, 3])\n\n\ndef ResNet101():\n    return ResNet(Bottleneck, [3, 4, 23, 3])\n\n\ndef ResNet152():\n    return ResNet(Bottleneck, [3, 8, 36, 3])","metadata":{"id":"TnzLuRymtioH","execution":{"iopub.status.busy":"2023-04-24T19:30:58.425916Z","iopub.execute_input":"2023-04-24T19:30:58.426503Z","iopub.status.idle":"2023-04-24T19:30:58.451678Z","shell.execute_reply.started":"2023-04-24T19:30:58.426465Z","shell.execute_reply":"2023-04-24T19:30:58.450607Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"AllConv implementation (https://arxiv.org/abs/1412.6806).\"\"\"\nimport math\nimport torch\nimport torch.nn as nn\n\n\nclass GELU(nn.Module):\n\n  def forward(self, x):\n    return torch.sigmoid(1.702 * x) * x\n\n\ndef make_layers(cfg):\n  \"\"\"Create a single layer.\"\"\"\n  layers = []\n  in_channels = 3\n  for v in cfg:\n    if v == 'Md':\n      layers += [nn.MaxPool2d(kernel_size=2, stride=2), nn.Dropout(p=0.5)]\n    elif v == 'A':\n      layers += [nn.AvgPool2d(kernel_size=8)]\n    elif v == 'NIN':\n      conv2d = nn.Conv2d(in_channels, in_channels, kernel_size=1, padding=1)\n      layers += [conv2d, nn.BatchNorm2d(in_channels), GELU()]\n    elif v == 'nopad':\n      conv2d = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=0)\n      layers += [conv2d, nn.BatchNorm2d(in_channels), GELU()]\n    else:\n      conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n      layers += [conv2d, nn.BatchNorm2d(v), GELU()]\n      in_channels = v\n  return nn.Sequential(*layers)\n\n\nclass AllConvNet(nn.Module):\n  \"\"\"AllConvNet main class.\"\"\"\n\n  def __init__(self, num_classes):\n    super(AllConvNet, self).__init__()\n\n    self.num_classes = num_classes\n    self.width1, w1 = 96, 96\n    self.width2, w2 = 192, 192\n\n    self.features = make_layers(\n        [w1, w1, w1, 'Md', w2, w2, w2, 'Md', 'nopad', 'NIN', 'NIN', 'A'])\n    self.classifier = nn.Linear(self.width2, num_classes)\n\n    for m in self.modules():\n      if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))  # He initialization\n      elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n      elif isinstance(m, nn.Linear):\n        m.bias.data.zero_()\n\n  def forward(self, x, rf=False, _eval=False):\n    if _eval:\n        # switch to eval mode\n        self.eval()\n    else:\n        self.train()\n\n    x = self.features(x)\n    x = x.view(x.size(0), -1)\n    y = self.classifier(x)\n    if rf:\n        return x, y\n    return y","metadata":{"id":"pDXheT0_soxd","execution":{"iopub.status.busy":"2023-04-24T19:30:58.454648Z","iopub.execute_input":"2023-04-24T19:30:58.455098Z","iopub.status.idle":"2023-04-24T19:30:58.471953Z","shell.execute_reply.started":"2023-04-24T19:30:58.455043Z","shell.execute_reply":"2023-04-24T19:30:58.470839Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\"\"\"DenseNet implementation (https://arxiv.org/abs/1608.06993).\"\"\"\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Bottleneck(nn.Module):\n  \"\"\"Bottleneck block for DenseNet.\"\"\"\n\n  def __init__(self, n_channels, growth_rate):\n    super(Bottleneck, self).__init__()\n    inter_channels = 4 * growth_rate\n    self.bn1 = nn.BatchNorm2d(n_channels)\n    self.conv1 = nn.Conv2d(\n        n_channels, inter_channels, kernel_size=1, bias=False)\n    self.bn2 = nn.BatchNorm2d(inter_channels)\n    self.conv2 = nn.Conv2d(\n        inter_channels, growth_rate, kernel_size=3, padding=1, bias=False)\n\n  def forward(self, x):\n    out = self.conv1(F.relu(self.bn1(x)))\n    out = self.conv2(F.relu(self.bn2(out)))\n    out = torch.cat((x, out), 1)\n    return out\n\n\nclass SingleLayer(nn.Module):\n  \"\"\"Layer container for blocks.\"\"\"\n\n  def __init__(self, n_channels, growth_rate):\n    super(SingleLayer, self).__init__()\n    self.bn1 = nn.BatchNorm2d(n_channels)\n    self.conv1 = nn.Conv2d(\n        n_channels, growth_rate, kernel_size=3, padding=1, bias=False)\n\n  def forward(self, x):\n    out = self.conv1(F.relu(self.bn1(x)))\n    out = torch.cat((x, out), 1)\n    return out\n\n\nclass Transition(nn.Module):\n  \"\"\"Transition block.\"\"\"\n\n  def __init__(self, n_channels, n_out_channels):\n    super(Transition, self).__init__()\n    self.bn1 = nn.BatchNorm2d(n_channels)\n    self.conv1 = nn.Conv2d(\n        n_channels, n_out_channels, kernel_size=1, bias=False)\n\n  def forward(self, x):\n    out = self.conv1(F.relu(self.bn1(x)))\n    out = F.avg_pool2d(out, 2)\n    return out\n\n\nclass DenseNet(nn.Module):\n  \"\"\"DenseNet main class.\"\"\"\n\n  def __init__(self, growth_rate, depth, reduction, n_classes, bottleneck):\n    super(DenseNet, self).__init__()\n\n    if bottleneck:\n      n_dense_blocks = int((depth - 4) / 6)\n    else:\n      n_dense_blocks = int((depth - 4) / 3)\n\n    n_channels = 2 * growth_rate\n    self.conv1 = nn.Conv2d(3, n_channels, kernel_size=3, padding=1, bias=False)\n\n    self.dense1 = self._make_dense(n_channels, growth_rate, n_dense_blocks,\n                                   bottleneck)\n    n_channels += n_dense_blocks * growth_rate\n    n_out_channels = int(math.floor(n_channels * reduction))\n    self.trans1 = Transition(n_channels, n_out_channels)\n\n    n_channels = n_out_channels\n    self.dense2 = self._make_dense(n_channels, growth_rate, n_dense_blocks,\n                                   bottleneck)\n    n_channels += n_dense_blocks * growth_rate\n    n_out_channels = int(math.floor(n_channels * reduction))\n    self.trans2 = Transition(n_channels, n_out_channels)\n\n    n_channels = n_out_channels\n    self.dense3 = self._make_dense(n_channels, growth_rate, n_dense_blocks,\n                                   bottleneck)\n    n_channels += n_dense_blocks * growth_rate\n\n    self.bn1 = nn.BatchNorm2d(n_channels)\n    self.fc = nn.Linear(n_channels, n_classes)\n\n    for m in self.modules():\n      if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n      elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n      elif isinstance(m, nn.Linear):\n        m.bias.data.zero_()\n\n  def _make_dense(self, n_channels, growth_rate, n_dense_blocks, bottleneck):\n    layers = []\n    for _ in range(int(n_dense_blocks)):\n      if bottleneck:\n        layers.append(Bottleneck(n_channels, growth_rate))\n      else:\n        layers.append(SingleLayer(n_channels, growth_rate))\n      n_channels += growth_rate\n    return nn.Sequential(*layers)\n\n  def forward(self, x, rf=False, _eval=False):\n    if _eval:\n        # switch to eval mode\n        self.eval()\n    else:\n        self.train()\n    out = self.conv1(x)\n    out = self.trans1(self.dense1(out))\n    out = self.trans2(self.dense2(out))\n    out = self.dense3(out)\n    out = torch.squeeze(F.avg_pool2d(F.relu(self.bn1(out)), 8))\n    y = self.fc(out)\n    if rf:\n        return out, y\n    return y\n\ndef densenet(growth_rate=12, depth=40, num_classes=10):\n  model = DenseNet(growth_rate, depth, 1., num_classes, False)\n  return model","metadata":{"id":"HA6nd4hassc-","execution":{"iopub.status.busy":"2023-04-24T19:30:58.475583Z","iopub.execute_input":"2023-04-24T19:30:58.475879Z","iopub.status.idle":"2023-04-24T19:30:58.500285Z","shell.execute_reply.started":"2023-04-24T19:30:58.475852Z","shell.execute_reply":"2023-04-24T19:30:58.497826Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"\"\"\"ResNeXt implementation (https://arxiv.org/abs/1611.05431).\"\"\"\nimport math\nimport torch.nn as nn\nfrom torch.nn import init\nimport torch.nn.functional as F\n\n\nclass ResNeXtBottleneck(nn.Module):\n  \"\"\"ResNeXt Bottleneck Block type C (https://github.com/facebookresearch/ResNeXt/blob/master/models/resnext.lua).\"\"\"\n  expansion = 4\n\n  def __init__(self,\n               inplanes,\n               planes,\n               cardinality,\n               base_width,\n               stride=1,\n               downsample=None):\n    super(ResNeXtBottleneck, self).__init__()\n\n    dim = int(math.floor(planes * (base_width / 64.0)))\n\n    self.conv_reduce = nn.Conv2d(\n        inplanes,\n        dim * cardinality,\n        kernel_size=1,\n        stride=1,\n        padding=0,\n        bias=False)\n    self.bn_reduce = nn.BatchNorm2d(dim * cardinality)\n\n    self.conv_conv = nn.Conv2d(\n        dim * cardinality,\n        dim * cardinality,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        groups=cardinality,\n        bias=False)\n    self.bn = nn.BatchNorm2d(dim * cardinality)\n\n    self.conv_expand = nn.Conv2d(\n        dim * cardinality,\n        planes * 4,\n        kernel_size=1,\n        stride=1,\n        padding=0,\n        bias=False)\n    self.bn_expand = nn.BatchNorm2d(planes * 4)\n\n    self.downsample = downsample\n\n  def forward(self, x):\n    residual = x\n\n    bottleneck = self.conv_reduce(x)\n    bottleneck = F.relu(self.bn_reduce(bottleneck), inplace=True)\n\n    bottleneck = self.conv_conv(bottleneck)\n    bottleneck = F.relu(self.bn(bottleneck), inplace=True)\n\n    bottleneck = self.conv_expand(bottleneck)\n    bottleneck = self.bn_expand(bottleneck)\n\n    if self.downsample is not None:\n      residual = self.downsample(x)\n\n    return F.relu(residual + bottleneck, inplace=True)\n\n\nclass CifarResNeXt(nn.Module):\n  \"\"\"ResNext optimized for the Cifar dataset, as specified in https://arxiv.org/pdf/1611.05431.pdf.\"\"\"\n\n  def __init__(self, block, depth, cardinality, base_width, num_classes):\n    super(CifarResNeXt, self).__init__()\n\n    # Model type specifies number of layers for CIFAR-10 and CIFAR-100 model\n    assert (depth - 2) % 9 == 0, 'depth should be one of 29, 38, 47, 56, 101'\n    layer_blocks = (depth - 2) // 9\n\n    self.cardinality = cardinality\n    self.base_width = base_width\n    self.num_classes = num_classes\n\n    self.conv_1_3x3 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n    self.bn_1 = nn.BatchNorm2d(64)\n\n    self.inplanes = 64\n    self.stage_1 = self._make_layer(block, 64, layer_blocks, 1)\n    self.stage_2 = self._make_layer(block, 128, layer_blocks, 2)\n    self.stage_3 = self._make_layer(block, 256, layer_blocks, 2)\n    self.avgpool = nn.AvgPool2d(8)\n    self.classifier = nn.Linear(256 * block.expansion, num_classes)\n\n    for m in self.modules():\n      if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n      elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n      elif isinstance(m, nn.Linear):\n        init.kaiming_normal(m.weight)\n        m.bias.data.zero_()\n\n  def _make_layer(self, block, planes, blocks, stride=1):\n    downsample = None\n    if stride != 1 or self.inplanes != planes * block.expansion:\n      downsample = nn.Sequential(\n          nn.Conv2d(\n              self.inplanes,\n              planes * block.expansion,\n              kernel_size=1,\n              stride=stride,\n              bias=False),\n          nn.BatchNorm2d(planes * block.expansion),\n      )\n\n    layers = []\n    layers.append(\n        block(self.inplanes, planes, self.cardinality, self.base_width, stride,\n              downsample))\n    self.inplanes = planes * block.expansion\n    for _ in range(1, blocks):\n      layers.append(\n          block(self.inplanes, planes, self.cardinality, self.base_width))\n\n    return nn.Sequential(*layers)\n\n  def forward(self, x, rf=False, _eval=False):\n    if _eval:\n        # switch to eval mode\n        self.eval()\n    else:\n        self.train()\n    x = self.conv_1_3x3(x)\n    x = F.relu(self.bn_1(x), inplace=True)\n    x = self.stage_1(x)\n    x = self.stage_2(x)\n    x = self.stage_3(x)\n    x = self.avgpool(x)\n    x = x.view(x.size(0), -1)\n    y = self.classifier(x)\n\n    if rf:\n        return x, y\n    return y\n\ndef resnext29(num_classes=10, cardinality=4, base_width=32):\n  model = CifarResNeXt(ResNeXtBottleneck, 29, cardinality, base_width,\n                       num_classes)\n  return model","metadata":{"id":"FLqt2DoJsvPZ","execution":{"iopub.status.busy":"2023-04-24T19:30:58.502074Z","iopub.execute_input":"2023-04-24T19:30:58.503005Z","iopub.status.idle":"2023-04-24T19:30:58.526583Z","shell.execute_reply.started":"2023-04-24T19:30:58.502968Z","shell.execute_reply":"2023-04-24T19:30:58.525651Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"\"\"\"WideResNet implementation (https://arxiv.org/abs/1605.07146).\"\"\"\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicBlock(nn.Module):\n  \"\"\"Basic ResNet block.\"\"\"\n\n  def __init__(self, in_planes, out_planes, stride, drop_rate=0.0):\n    super(BasicBlock, self).__init__()\n    self.bn1 = nn.BatchNorm2d(in_planes)\n    self.relu1 = nn.ReLU(inplace=True)\n    self.conv1 = nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        bias=False)\n    self.bn2 = nn.BatchNorm2d(out_planes)\n    self.relu2 = nn.ReLU(inplace=True)\n    self.conv2 = nn.Conv2d(\n        out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\n    self.drop_rate = drop_rate\n    self.is_in_equal_out = (in_planes == out_planes)\n    self.conv_shortcut = (not self.is_in_equal_out) and nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=1,\n        stride=stride,\n        padding=0,\n        bias=False) or None\n\n  def forward(self, x):\n    if not self.is_in_equal_out:\n      x = self.relu1(self.bn1(x))\n    else:\n      out = self.relu1(self.bn1(x))\n    if self.is_in_equal_out:\n      out = self.relu2(self.bn2(self.conv1(out)))\n    else:\n      out = self.relu2(self.bn2(self.conv1(x)))\n    if self.drop_rate > 0:\n      out = F.dropout(out, p=self.drop_rate, training=self.training)\n    out = self.conv2(out)\n    if not self.is_in_equal_out:\n      return torch.add(self.conv_shortcut(x), out)\n    else:\n      return torch.add(x, out)\n\n\nclass NetworkBlock(nn.Module):\n  \"\"\"Layer container for blocks.\"\"\"\n\n  def __init__(self,\n               nb_layers,\n               in_planes,\n               out_planes,\n               block,\n               stride,\n               drop_rate=0.0):\n    super(NetworkBlock, self).__init__()\n    self.layer = self._make_layer(block, in_planes, out_planes, nb_layers,\n                                  stride, drop_rate)\n\n  def _make_layer(self, block, in_planes, out_planes, nb_layers, stride,\n                  drop_rate):\n    layers = []\n    for i in range(nb_layers):\n      layers.append(\n          block(i == 0 and in_planes or out_planes, out_planes,\n                i == 0 and stride or 1, drop_rate))\n    return nn.Sequential(*layers)\n\n  def forward(self, x):\n    return self.layer(x)\n\n\nclass WideResNet(nn.Module):\n  \"\"\"WideResNet class.\"\"\"\n\n  def __init__(self, depth, num_classes, widen_factor=1, drop_rate=0.0):\n    super(WideResNet, self).__init__()\n    n_channels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n    assert (depth - 4) % 6 == 0\n    n = (depth - 4) // 6\n    block = BasicBlock\n    # 1st conv before any network block\n    self.conv1 = nn.Conv2d(\n        3, n_channels[0], kernel_size=3, stride=1, padding=1, bias=False)\n    # 1st block\n    self.block1 = NetworkBlock(n, n_channels[0], n_channels[1], block, 1,\n                               drop_rate)\n    # 2nd block\n    self.block2 = NetworkBlock(n, n_channels[1], n_channels[2], block, 2,\n                               drop_rate)\n    # 3rd block\n    self.block3 = NetworkBlock(n, n_channels[2], n_channels[3], block, 2,\n                               drop_rate)\n    # global average pooling and classifier\n    self.bn1 = nn.BatchNorm2d(n_channels[3])\n    self.relu = nn.ReLU(inplace=True)\n    self.fc = nn.Linear(n_channels[3], num_classes)\n    self.n_channels = n_channels[3]\n\n    for m in self.modules():\n      if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n      elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n      elif isinstance(m, nn.Linear):\n        m.bias.data.zero_()\n\n  def forward(self, x, rf=False, _eval=False):\n    if _eval:\n        # switch to eval mode\n        self.eval()\n    else:\n        self.train()\n\n    out = self.conv1(x)\n    out = self.block1(out)\n    out = self.block2(out)\n    out = self.block3(out)\n    out = self.relu(self.bn1(out))\n    out = F.avg_pool2d(out, 8)\n    out = out.view(-1, self.n_channels)\n    y =  self.fc(out)\n\n    if rf:\n        return out, y\n    return y","metadata":{"id":"4gfUmcamsxhr","execution":{"iopub.status.busy":"2023-04-24T19:30:58.531500Z","iopub.execute_input":"2023-04-24T19:30:58.531793Z","iopub.status.idle":"2023-04-24T19:30:58.558100Z","shell.execute_reply.started":"2023-04-24T19:30:58.531765Z","shell.execute_reply":"2023-04-24T19:30:58.557118Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport os.path as osp\nimport numpy as np\nfrom PIL import Image\nimport torch.fft\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\n\ndef test(net, criterion, testloader, outloader, attack=None, epoch=None, **options):\n    net.eval()\n    correct, total, adv_correct = 0, 0, 0\n\n    torch.cuda.empty_cache()\n\n    _pred_k, _pred_u, _labels = [], [], []\n\n    with torch.no_grad():\n        for data, labels in testloader:\n            if options['use_gpu']:\n                data, labels = data.cuda(), labels.cuda()             \n                data = normalize(data)\n                logits = net(data, _eval=True)\n                predictions = logits.data.max(1)[1]\n                total += labels.size(0)\n                correct += (predictions == labels.data).sum()\n            \n                _pred_k.append(logits.data.cpu().numpy())\n                _labels.append(labels.data.cpu().numpy())\n\n        for batch_idx, (data, labels) in enumerate(outloader):\n            if options['use_gpu']:\n                data, labels = data.cuda(), labels.cuda()\n                data = normalize(data)\n            with torch.set_grad_enabled(False):\n                logits = net(data, _eval=True)\n                _pred_u.append(logits.data.cpu().numpy())\n\n    _pred_k = np.concatenate(_pred_k, 0)\n    _pred_u = np.concatenate(_pred_u, 0)\n    _labels = np.concatenate(_labels, 0)\n    \n    # # Out-of-Distribution detction evaluation\n    x1, x2 = np.max(_pred_k, axis=1), np.max(_pred_u, axis=1)\n    results = metric_ood(x1, x2)['Bas']\n\n    # Accuracy\n    acc = float(correct) * 100. / float(total)\n    results['ACC'] = acc\n\n    print('Acc: {:.5f}'.format(acc))\n\n    return results\n\ndef test_robustness(net, criterion, testloader, epoch=None, label='', **options):\n    net.eval()\n    results = dict()\n    correct, total = 0, 0\n\n    torch.cuda.empty_cache()\n\n    with torch.no_grad():\n        for data, labels in testloader:\n            if options['use_gpu']:\n                data, labels = data.cuda(), labels.cuda()\n                data = normalize(data)\n            with torch.set_grad_enabled(False):\n                logits = net(data, _eval=True)\n                predictions = logits.data.max(1)[1]\n                total += labels.size(0)\n                correct += (predictions == labels.data).sum()\n\n    # Accuracy\n    acc = float(correct) * 100. / float(total)\n    results['ACC'] = acc\n\n    return results","metadata":{"id":"tbsgoVV10myW","execution":{"iopub.status.busy":"2023-04-24T19:30:58.559662Z","iopub.execute_input":"2023-04-24T19:30:58.560171Z","iopub.status.idle":"2023-04-24T19:30:58.576328Z","shell.execute_reply.started":"2023-04-24T19:30:58.560135Z","shell.execute_reply":"2023-04-24T19:30:58.575242Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\n\ndef get_curve_online(known, novel, stypes = ['Bas']):\n    tp, fp = dict(), dict()\n    tnr_at_tpr95 = dict()\n    for stype in stypes:\n        known.sort()\n        novel.sort()\n        end = np.max([np.max(known), np.max(novel)])\n        start = np.min([np.min(known),np.min(novel)])\n        num_k = known.shape[0]\n        num_n = novel.shape[0]\n        tp[stype] = -np.ones([num_k+num_n+1], dtype=int)\n        fp[stype] = -np.ones([num_k+num_n+1], dtype=int)\n        tp[stype][0], fp[stype][0] = num_k, num_n\n        k, n = 0, 0\n        for l in range(num_k+num_n):\n            if k == num_k:\n                tp[stype][l+1:] = tp[stype][l]\n                fp[stype][l+1:] = np.arange(fp[stype][l]-1, -1, -1)\n                break\n            elif n == num_n:\n                tp[stype][l+1:] = np.arange(tp[stype][l]-1, -1, -1)\n                fp[stype][l+1:] = fp[stype][l]\n                break\n            else:\n                if novel[n] < known[k]:\n                    n += 1\n                    tp[stype][l+1] = tp[stype][l]\n                    fp[stype][l+1] = fp[stype][l] - 1\n                else:\n                    k += 1\n                    tp[stype][l+1] = tp[stype][l] - 1\n                    fp[stype][l+1] = fp[stype][l]\n        tpr95_pos = np.abs(tp[stype] / num_k - .95).argmin()\n        tnr_at_tpr95[stype] = 1. - fp[stype][tpr95_pos] / num_n\n    return tp, fp, tnr_at_tpr95\n\ndef metric_ood(x1, x2, stypes = ['Bas'], verbose=True):\n    tp, fp, tnr_at_tpr95 = get_curve_online(x1, x2, stypes)\n    results = dict()\n    mtypes = ['TNR', 'AUROC', 'DTACC', 'AUIN', 'AUOUT']\n    if verbose:\n        print('      ', end='')\n        for mtype in mtypes:\n            print(' {mtype:6s}'.format(mtype=mtype), end='')\n        print('')\n        \n    for stype in stypes:\n        if verbose:\n            print('{stype:5s} '.format(stype=stype), end='')\n        results[stype] = dict()\n        \n        # TNR\n        mtype = 'TNR'\n        results[stype][mtype] = 100.*tnr_at_tpr95[stype]\n        if verbose:\n            print(' {val:6.3f}'.format(val=results[stype][mtype]), end='')\n        \n        # AUROC\n        mtype = 'AUROC'\n        tpr = np.concatenate([[1.], tp[stype]/tp[stype][0], [0.]])\n        fpr = np.concatenate([[1.], fp[stype]/fp[stype][0], [0.]])\n        results[stype][mtype] = 100.*(-np.trapz(1.-fpr, tpr))\n        if verbose:\n            print(' {val:6.3f}'.format(val=results[stype][mtype]), end='')\n        \n        # DTACC\n        mtype = 'DTACC'\n        results[stype][mtype] = 100.*(.5 * (tp[stype]/tp[stype][0] + 1.-fp[stype]/fp[stype][0]).max())\n        if verbose:\n            print(' {val:6.3f}'.format(val=results[stype][mtype]), end='')\n        \n        # AUIN\n        mtype = 'AUIN'\n        denom = tp[stype]+fp[stype]\n        denom[denom == 0.] = -1.\n        pin_ind = np.concatenate([[True], denom > 0., [True]])\n        pin = np.concatenate([[.5], tp[stype]/denom, [0.]])\n        results[stype][mtype] = 100.*(-np.trapz(pin[pin_ind], tpr[pin_ind]))\n        if verbose:\n            print(' {val:6.3f}'.format(val=results[stype][mtype]), end='')\n        \n        # AUOUT\n        mtype = 'AUOUT'\n        denom = tp[stype][0]-tp[stype]+fp[stype][0]-fp[stype]\n        denom[denom == 0.] = -1.\n        pout_ind = np.concatenate([[True], denom > 0., [True]])\n        pout = np.concatenate([[0.], (fp[stype][0]-fp[stype])/denom, [.5]])\n        results[stype][mtype] = 100.*(np.trapz(pout[pout_ind], 1.-fpr[pout_ind]))\n        if verbose:\n            print(' {val:6.3f}'.format(val=results[stype][mtype]), end='')\n            print('')\n    \n    return results\n\ndef compute_oscr(pred_k, pred_u, labels):\n    x1, x2 = np.max(pred_k, axis=1), np.max(pred_u, axis=1)\n    pred = np.argmax(pred_k, axis=1)\n    correct = (pred == labels)\n    m_x1 = np.zeros(len(x1))\n    m_x1[pred == labels] = 1\n    k_target = np.concatenate((m_x1, np.zeros(len(x2))), axis=0)\n    u_target = np.concatenate((np.zeros(len(x1)), np.ones(len(x2))), axis=0)\n    predict = np.concatenate((x1, x2), axis=0)\n    n = len(predict)\n\n    # Cutoffs are of prediction values\n    \n    CCR = [0 for x in range(n+2)]\n    FPR = [0 for x in range(n+2)] \n\n    idx = predict.argsort()\n\n    s_k_target = k_target[idx]\n    s_u_target = u_target[idx]\n\n    for k in range(n-1):\n        CC = s_k_target[k+1:].sum()\n        FP = s_u_target[k:].sum()\n\n        # True\tPositive Rate\n        CCR[k] = float(CC) / float(len(x1))\n        # False Positive Rate\n        FPR[k] = float(FP) / float(len(x2))\n\n    CCR[n] = 0.0\n    FPR[n] = 0.0\n    CCR[n+1] = 1.0\n    FPR[n+1] = 1.0\n\n    # Positions of ROC curve (FPR, TPR)\n    ROC = sorted(zip(FPR, CCR), reverse=True)\n\n    OSCR = 0\n\n    # Compute AUROC Using Trapezoidal Rule\n    for j in range(n+1):\n        h =   ROC[j][0] - ROC[j+1][0]\n        w =  (ROC[j][1] + ROC[j+1][1]) / 2.0\n\n        OSCR = OSCR + h*w\n\n    return OSCR\n","metadata":{"id":"NC3UcZuv0wOh","execution":{"iopub.status.busy":"2023-04-24T19:30:58.577708Z","iopub.execute_input":"2023-04-24T19:30:58.578249Z","iopub.status.idle":"2023-04-24T19:30:58.610231Z","shell.execute_reply.started":"2023-04-24T19:30:58.578202Z","shell.execute_reply":"2023-04-24T19:30:58.609317Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def mkdir_if_missing(directory):\n    if not osp.exists(directory):\n        try:\n            os.makedirs(directory)\n        except OSError as e:\n            if e.errno != errno.EEXIST:\n                raise\ndef save_networks(networks, result_dir, name='', loss='', criterion=None):\n    mkdir_if_missing(osp.join(result_dir, 'checkpoints'))\n    weights = networks.state_dict()\n    filename = '{}/checkpoints/{}_{}.pth'.format(result_dir, name, loss)\n    torch.save(weights, filename)\n    if criterion:\n        weights = criterion.state_dict()\n        filename = '{}/checkpoints/{}_{}_criterion.pth'.format(result_dir, name, loss)\n        torch.save(weights, filename)\n\ndef load_networks(networks, result_dir, name='', loss='', criterion=None):\n    weights = networks.state_dict()\n    filename = '{}/checkpoints/{}_{}.pth'.format(result_dir, name, loss)\n    networks.load_state_dict(torch.load(filename))\n    if criterion:\n        weights = criterion.state_dict()\n        filename = '{}/checkpoints/{}_{}_criterion.pth'.format(result_dir, name, loss)\n        criterion.load_state_dict(torch.load(filename))\n\n    return networks, criterion","metadata":{"id":"PJYT7Nd10ALr","execution":{"iopub.status.busy":"2023-04-24T19:30:58.613854Z","iopub.execute_input":"2023-04-24T19:30:58.614862Z","iopub.status.idle":"2023-04-24T19:30:58.625089Z","shell.execute_reply.started":"2023-04-24T19:30:58.614832Z","shell.execute_reply":"2023-04-24T19:30:58.624008Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torchvision.utils as vutils\n\nimport numpy as np\nimport random\n\ndef mix_data(x, use_cuda=True, prob=0.6):\n    '''Returns mixed inputs, pairs of targets, and lambda'''\n\n    p = random.uniform(0, 1)\n\n    if p > prob:\n        return x\n\n    batch_size = x.size()[0]\n    if use_cuda:\n        index = torch.randperm(batch_size).cuda()\n    else:\n        index = torch.randperm(batch_size)\n\n    fft_1 = torch.fft.fftn(x, dim=(1,2,3))\n    abs_1, angle_1 = torch.abs(fft_1), torch.angle(fft_1)\n\n    fft_2 = torch.fft.fftn(x[index, :], dim=(1,2,3))\n    abs_2, angle_2 = torch.abs(fft_2), torch.angle(fft_2)\n\n    fft_1 = abs_2*torch.exp((1j) * angle_1)\n\n    mixed_x = torch.fft.ifftn(fft_1, dim=(1,2,3)).float()\n\n    return mixed_x\n\n\ndef train(net, criterion, optimizer, trainloader, epoch=None, **options):\n    net.train()\n    losses = AverageMeter()\n\n    torch.cuda.empty_cache()\n    \n    loss_all = 0\n    for batch_idx, (data, labels) in enumerate(trainloader):\n        if options['use_gpu']:\n            inputs, targets = data.cuda(), labels.cuda()\n\n        inputs_mix = mix_data(inputs)\n        inputs_mix = Variable(inputs_mix)\n        batch_size = inputs.size(0)\n        inputs, inputs_mix = normalize(inputs), normalize(inputs_mix)\n\n        inputs = torch.cat([inputs, inputs_mix], 0)\n\n        with torch.set_grad_enabled(True):\n            optimizer.zero_grad()\n\n            _, y = net(inputs, True)\n            loss = criterion(y[:batch_size], targets) + criterion(y[batch_size:], targets)\n            \n            loss.backward()\n            optimizer.step()\n        \n        losses.update(loss.item(), targets.size(0))\n\n        if (batch_idx+1) % options['print_freq'] == 0:\n            print(\"Batch {}/{}\\t Loss {:.6f} ({:.6f})\" \\\n                  .format(batch_idx+1, len(trainloader), losses.val, losses.avg))\n        \n        loss_all += losses.avg\n\n    return loss_all","metadata":{"id":"vUp5pEUm2Xk1","execution":{"iopub.status.busy":"2023-04-24T19:30:58.766910Z","iopub.execute_input":"2023-04-24T19:30:58.767815Z","iopub.status.idle":"2023-04-24T19:30:58.781917Z","shell.execute_reply.started":"2023-04-24T19:30:58.767775Z","shell.execute_reply":"2023-04-24T19:30:58.780542Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value.\n       \n       Code imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n    \"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\nclass Logger(object):\n    \"\"\"\n    Write console output to external text file.\n    \n    Code imported from https://github.com/Cysu/open-reid/blob/master/reid/utils/logging.py.\n    \"\"\"\n    def __init__(self, fpath=None):\n        self.console = sys.stdout\n        self.file = None\n        if fpath is not None:\n            mkdir_if_missing(os.path.dirname(fpath))\n            self.file = open(fpath, 'w')\n\n    def __del__(self):\n        self.close()\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, *args):\n        self.close()\n\n    def write(self, msg):\n        self.console.write(msg)\n        if self.file is not None:\n            self.file.write(msg)\n\n    def flush(self):\n        self.console.flush()\n        if self.file is not None:\n            self.file.flush()\n            os.fsync(self.file.fileno())\n\n    def close(self):\n        self.console.close()\n        if self.file is not None:\n            self.file.close()","metadata":{"id":"8laTrZuZ2eJc","execution":{"iopub.status.busy":"2023-04-24T19:30:58.954096Z","iopub.execute_input":"2023-04-24T19:30:58.955193Z","iopub.status.idle":"2023-04-24T19:30:58.966619Z","shell.execute_reply.started":"2023-04-24T19:30:58.955148Z","shell.execute_reply":"2023-04-24T19:30:58.965361Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"!mkdir ./outf\n!mkdir ./data\n\n!wget https://zenodo.org/record/2535967/files/CIFAR-10-C.tar","metadata":{"id":"_TJg-jZAhwmG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"19ca4a39-7533-4137-81f8-762ab48735bc","execution":{"iopub.status.busy":"2023-04-24T19:30:59.994108Z","iopub.execute_input":"2023-04-24T19:30:59.995163Z","iopub.status.idle":"2023-04-24T19:59:48.522461Z","shell.execute_reply.started":"2023-04-24T19:30:59.995107Z","shell.execute_reply":"2023-04-24T19:59:48.521199Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"--2023-04-24 19:31:02--  https://zenodo.org/record/2535967/files/CIFAR-10-C.tar\nResolving zenodo.org (zenodo.org)... 188.185.124.72\nConnecting to zenodo.org (zenodo.org)|188.185.124.72|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2918471680 (2.7G) [application/octet-stream]\nSaving to: ‘CIFAR-10-C.tar’\n\nCIFAR-10-C.tar      100%[===================>]   2.72G   907KB/s    in 28m 43s \n\n2023-04-24 19:59:48 (1.62 MB/s) - ‘CIFAR-10-C.tar’ saved [2918471680/2918471680]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!tar -xvf CIFAR-10-C.tar -C ./data","metadata":{"id":"w5gRM-dwntwX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3809d56c-be02-42a8-ec02-405f68d42711","execution":{"iopub.status.busy":"2023-04-24T19:59:48.526224Z","iopub.execute_input":"2023-04-24T19:59:48.526615Z","iopub.status.idle":"2023-04-24T19:59:54.202844Z","shell.execute_reply.started":"2023-04-24T19:59:48.526580Z","shell.execute_reply":"2023-04-24T19:59:54.200581Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"CIFAR-10-C/\nCIFAR-10-C/fog.npy\nCIFAR-10-C/jpeg_compression.npy\nCIFAR-10-C/zoom_blur.npy\nCIFAR-10-C/speckle_noise.npy\nCIFAR-10-C/glass_blur.npy\nCIFAR-10-C/spatter.npy\nCIFAR-10-C/shot_noise.npy\nCIFAR-10-C/defocus_blur.npy\nCIFAR-10-C/elastic_transform.npy\nCIFAR-10-C/gaussian_blur.npy\nCIFAR-10-C/frost.npy\nCIFAR-10-C/saturate.npy\nCIFAR-10-C/brightness.npy\nCIFAR-10-C/snow.npy\nCIFAR-10-C/gaussian_noise.npy\nCIFAR-10-C/motion_blur.npy\nCIFAR-10-C/contrast.npy\nCIFAR-10-C/impulse_noise.npy\nCIFAR-10-C/labels.npy\nCIFAR-10-C/pixelate.npy\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls ./data/CIFAR-10-C/","metadata":{"id":"3J0OI71FmZwy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a670e59d-8110-490e-dffd-e44ad20bffd7","execution":{"iopub.status.busy":"2023-04-24T19:59:54.204911Z","iopub.execute_input":"2023-04-24T19:59:54.205331Z","iopub.status.idle":"2023-04-24T19:59:55.325000Z","shell.execute_reply.started":"2023-04-24T19:59:54.205284Z","shell.execute_reply":"2023-04-24T19:59:55.323725Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"brightness.npy\t       gaussian_noise.npy    saturate.npy\ncontrast.npy\t       glass_blur.npy\t     shot_noise.npy\ndefocus_blur.npy       impulse_noise.npy     snow.npy\nelastic_transform.npy  jpeg_compression.npy  spatter.npy\nfog.npy\t\t       labels.npy\t     speckle_noise.npy\nfrost.npy\t       motion_blur.npy\t     zoom_blur.npy\ngaussian_blur.npy      pixelate.npy\n","output_type":"stream"}]},{"cell_type":"code","source":"options = {\n    'data': './data',\n    'outf': './results',\n    'dataset': 'cifar10',\n    'workers': 8,\n    'batch_size': 16,\n    'lr': 0.1,\n    'max_epoch': 30,\n    'stepsize': 10,\n    'aug': 'none',\n    'model': 'densenet',\n    'eval_freq': 5,\n    'print_freq': 100,\n    'gpu': '0',\n    'seed': 0,\n    'use_cpu': False,\n    'eval': True,\n    'epsilon': 0.0157,\n    'alpha': 0.00784,\n    'k': 10,\n    'perturbation_type': 'linf'\n}\n","metadata":{"id":"kpqP-MrH0Ald","execution":{"iopub.status.busy":"2023-04-24T19:59:55.328350Z","iopub.execute_input":"2023-04-24T19:59:55.328673Z","iopub.status.idle":"2023-04-24T19:59:55.993724Z","shell.execute_reply.started":"2023-04-24T19:59:55.328640Z","shell.execute_reply":"2023-04-24T19:59:55.992585Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"if 'cifar10' == options['dataset']:\n        Data = CIFAR10D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['aug'], _eval=options['eval'])\n        OODData = CIFAR100D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['aug'])\nelse:\n        Data = CIFAR100D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['aug'], _eval=options['eval'])\n        OODData = CIFAR10D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['aug'])","metadata":{"id":"PsH_MKlSjd-_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1c7858bb-0336-47c2-dccc-ea8f87222eb1","execution":{"iopub.status.busy":"2023-04-24T19:59:55.997460Z","iopub.execute_input":"2023-04-24T19:59:55.997763Z","iopub.status.idle":"2023-04-24T20:00:35.273282Z","shell.execute_reply.started":"2023-04-24T19:59:55.997728Z","shell.execute_reply":"2023-04-24T20:00:35.272229Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar10/cifar-10-python.tar.gz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/170498071 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0fa4ddc31754dd9a52e8a099624af28"}},"metadata":{}},{"name":"stdout","text":"Extracting ./data/cifar10/cifar-10-python.tar.gz to ./data/cifar10\nFiles already downloaded and verified\nDownloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar100/cifar-100-python.tar.gz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/169001437 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42eef5be63e84cf4aa6f3fd83b5a7be5"}},"metadata":{}},{"name":"stdout","text":"Extracting ./data/cifar100/cifar-100-python.tar.gz to ./data/cifar100\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"trainloader, testloader, outloader = Data.train_loader, Data.test_loader, OODData.test_loader\nnum_classes = Data.num_classes\n\n\n\"\"\"DenseNet implementation (https://arxiv.org/abs/1608.06993).\"\"\"\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Bottleneck(nn.Module):\n  \"\"\"Bottleneck block for DenseNet.\"\"\"\n\n  def __init__(self, n_channels, growth_rate):\n    super(Bottleneck, self).__init__()\n    inter_channels = 4 * growth_rate\n    self.bn1 = nn.BatchNorm2d(n_channels)\n    self.conv1 = nn.Conv2d(\n        n_channels, inter_channels, kernel_size=1, bias=False)\n    self.bn2 = nn.BatchNorm2d(inter_channels)\n    self.conv2 = nn.Conv2d(\n        inter_channels, growth_rate, kernel_size=3, padding=1, bias=False)\n\n  def forward(self, x):\n    out = self.conv1(F.relu(self.bn1(x)))\n    out = self.conv2(F.relu(self.bn2(out)))\n    out = torch.cat((x, out), 1)\n    return out\n\n\nclass SingleLayer(nn.Module):\n  \"\"\"Layer container for blocks.\"\"\"\n\n  def __init__(self, n_channels, growth_rate):\n    super(SingleLayer, self).__init__()\n    self.bn1 = nn.BatchNorm2d(n_channels)\n    self.conv1 = nn.Conv2d(\n        n_channels, growth_rate, kernel_size=3, padding=1, bias=False)\n\n  def forward(self, x):\n    out = self.conv1(F.relu(self.bn1(x)))\n    out = torch.cat((x, out), 1)\n    return out\n\n\nclass Transition(nn.Module):\n  \"\"\"Transition block.\"\"\"\n\n  def __init__(self, n_channels, n_out_channels):\n    super(Transition, self).__init__()\n    self.bn1 = nn.BatchNorm2d(n_channels)\n    self.conv1 = nn.Conv2d(\n        n_channels, n_out_channels, kernel_size=1, bias=False)\n\n  def forward(self, x):\n    out = self.conv1(F.relu(self.bn1(x)))\n    out = F.avg_pool2d(out, 2)\n    return out\n\n\nclass DenseNet(nn.Module):\n  \"\"\"DenseNet main class.\"\"\"\n\n  def __init__(self, growth_rate, depth, reduction, n_classes, bottleneck):\n    super(DenseNet, self).__init__()\n\n    if bottleneck:\n      n_dense_blocks = int((depth - 4) / 6)\n    else:\n      n_dense_blocks = int((depth - 4) / 3)\n\n    n_channels = 2 * growth_rate\n    self.conv1 = nn.Conv2d(3, n_channels, kernel_size=3, padding=1, bias=False)\n\n    self.dense1 = self._make_dense(n_channels, growth_rate, n_dense_blocks,\n                                   bottleneck)\n    n_channels += n_dense_blocks * growth_rate\n    n_out_channels = int(math.floor(n_channels * reduction))\n    self.trans1 = Transition(n_channels, n_out_channels)\n\n    n_channels = n_out_channels\n    self.dense2 = self._make_dense(n_channels, growth_rate, n_dense_blocks,\n                                   bottleneck)\n    n_channels += n_dense_blocks * growth_rate\n    n_out_channels = int(math.floor(n_channels * reduction))\n    self.trans2 = Transition(n_channels, n_out_channels)\n\n    n_channels = n_out_channels\n    self.dense3 = self._make_dense(n_channels, growth_rate, n_dense_blocks,\n                                   bottleneck)\n    n_channels += n_dense_blocks * growth_rate\n\n    self.bn1 = nn.BatchNorm2d(n_channels)\n    self.fc = nn.Linear(n_channels, n_classes)\n\n    for m in self.modules():\n      if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n      elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n      elif isinstance(m, nn.Linear):\n        m.bias.data.zero_()\n\n  def _make_dense(self, n_channels, growth_rate, n_dense_blocks, bottleneck):\n    layers = []\n    for _ in range(int(n_dense_blocks)):\n      if bottleneck:\n        layers.append(Bottleneck(n_channels, growth_rate))\n      else:\n        layers.append(SingleLayer(n_channels, growth_rate))\n      n_channels += growth_rate\n    return nn.Sequential(*layers)\n\n  def forward(self, x, rf=False, _eval=False):\n    if _eval:\n        # switch to eval mode\n        self.eval()\n    else:\n        self.train()\n    out = self.conv1(x)\n    out = self.trans1(self.dense1(out))\n    out = self.trans2(self.dense2(out))\n    out = self.dense3(out)\n    out = torch.squeeze(F.avg_pool2d(F.relu(self.bn1(out)), 8))\n    y = self.fc(out)\n    if rf:\n        return out, y\n    return y\n\ndef densenet(growth_rate=12, depth=40, num_classes=10):\n  model = DenseNet(growth_rate, depth, 1., num_classes, False)\n  return model","metadata":{"id":"cGGLtRXoobo1","execution":{"iopub.status.busy":"2023-04-24T20:00:35.274799Z","iopub.execute_input":"2023-04-24T20:00:35.275688Z","iopub.status.idle":"2023-04-24T20:00:35.299797Z","shell.execute_reply.started":"2023-04-24T20:00:35.275648Z","shell.execute_reply":"2023-04-24T20:00:35.298734Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"print(\"Creating model: {}\".format(options['model']))\nif 'wide_resnet' in options['model']:\n        print('wide_resnet')\n        net = WideResNet(40, num_classes, 2, 0.0)\nelif 'allconv' in options['model']:\n        print('allconv')\n        net = AllConvNet(num_classes)\nelif 'densenet' in options['model']:\n        print('densenet')\n        net = densenet(num_classes=num_classes)\nelif 'resnext' in options['model']:\n        print('resnext29')\n        net = resnext29(num_classes)\nelse:\n        print('resnet18')\n        net = ResNet18(num_classes=num_classes)\n","metadata":{"id":"4NmhUnj5opN6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a1a5e431-a912-4431-9c91-3b60cd6e8d50","execution":{"iopub.status.busy":"2023-04-24T20:00:35.301461Z","iopub.execute_input":"2023-04-24T20:00:35.301868Z","iopub.status.idle":"2023-04-24T20:00:35.383321Z","shell.execute_reply.started":"2023-04-24T20:00:35.301803Z","shell.execute_reply":"2023-04-24T20:00:35.382246Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Creating model: densenet\ndensenet\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.manual_seed(options['seed'])\nos.environ['CUDA_VISIBLE_DEVICES'] = options['gpu']\nuse_gpu = torch.cuda.is_available()\nif options['use_cpu']: use_gpu = False\n\noptions.update({'use_gpu': use_gpu})\noptions['use_gpu']","metadata":{"id":"c_XXanTQdxDh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"53eb52d5-3e2a-490b-bf17-537b73d97e6b","execution":{"iopub.status.busy":"2023-04-24T20:00:35.384633Z","iopub.execute_input":"2023-04-24T20:00:35.385767Z","iopub.status.idle":"2023-04-24T20:00:35.451801Z","shell.execute_reply.started":"2023-04-24T20:00:35.385711Z","shell.execute_reply":"2023-04-24T20:00:35.451110Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss().cuda()\n\nif use_gpu:\n        net = nn.DataParallel(net, device_ids=[i for i in range(len(options['gpu'].split(',')))]).cuda()\n        criterion = criterion.cuda()","metadata":{"id":"UJ8nlpIcpzci","execution":{"iopub.status.busy":"2023-04-24T20:00:35.452986Z","iopub.execute_input":"2023-04-24T20:00:35.453592Z","iopub.status.idle":"2023-04-24T20:00:37.934861Z","shell.execute_reply.started":"2023-04-24T20:00:35.453554Z","shell.execute_reply":"2023-04-24T20:00:37.933830Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"file_name = '{}_{}_{}'.format(options['model'], options['dataset'], options['aug'])\n","metadata":{"id":"-3eUpU7ztjaM","execution":{"iopub.status.busy":"2023-04-24T20:00:37.938553Z","iopub.execute_input":"2023-04-24T20:00:37.939024Z","iopub.status.idle":"2023-04-24T20:00:37.944231Z","shell.execute_reply.started":"2023-04-24T20:00:37.938982Z","shell.execute_reply":"2023-04-24T20:00:37.942875Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"params_list = [{'params': net.parameters()},\n                {'params': criterion.parameters()}]","metadata":{"id":"OJlKwuxb04v-","execution":{"iopub.status.busy":"2023-04-24T20:00:37.945682Z","iopub.execute_input":"2023-04-24T20:00:37.946349Z","iopub.status.idle":"2023-04-24T20:00:37.954366Z","shell.execute_reply.started":"2023-04-24T20:00:37.946309Z","shell.execute_reply":"2023-04-24T20:00:37.953294Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.SGD(params_list, lr=options['lr'], momentum=0.9, nesterov=True, weight_decay=5e-4)\nscheduler = lr_scheduler.MultiStepLR(optimizer, gamma=0.2, milestones=[60, 120, 160, 190])\n\nstart_time = time.time()\n\nbest_acc = 0.0","metadata":{"id":"y9Ho8WZQ08LG","execution":{"iopub.status.busy":"2023-04-24T20:00:37.955946Z","iopub.execute_input":"2023-04-24T20:00:37.956342Z","iopub.status.idle":"2023-04-24T20:00:37.965112Z","shell.execute_reply.started":"2023-04-24T20:00:37.956305Z","shell.execute_reply":"2023-04-24T20:00:37.963934Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"for epoch in range(options['max_epoch']):\n        print(\"==> Epoch {}/{}\".format(epoch+1, options['max_epoch']))\n\n        train(net, criterion, optimizer, trainloader, epoch=epoch, **options)\n\n        if options['eval_freq'] > 0 and (epoch+1) % options['eval_freq'] == 0 or (epoch+1) == options['max_epoch'] or epoch > 160:\n            print(\"==> Test\")\n            results = test(net, criterion, testloader, outloader, epoch=epoch, **options)\n\n            if best_acc < results['ACC']:\n                best_acc = results['ACC']\n                print(\"Best Acc (%): {:.3f}\\t\".format(best_acc))\n            \n            save_networks(net, options['outf'], file_name, criterion=criterion)\n\n        scheduler.step()\n\nelapsed = round(time.time() - start_time)\nelapsed = str(datetime.timedelta(seconds=elapsed))\nprint(\"Finished. Total elapsed time (h:m:s): {}\".format(elapsed))","metadata":{"id":"IEFaTaqm1r_6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ed7d4c93-e9a6-4370-c7f2-8b930ed50a3d","execution":{"iopub.status.busy":"2023-04-24T20:00:37.968777Z","iopub.execute_input":"2023-04-24T20:00:37.969112Z","iopub.status.idle":"2023-04-24T21:23:59.022993Z","shell.execute_reply.started":"2023-04-24T20:00:37.969077Z","shell.execute_reply":"2023-04-24T21:23:59.020885Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"==> Epoch 1/30\nBatch 100/3125\t Loss 4.235605 (5.462429)\nBatch 200/3125\t Loss 4.393431 (4.872405)\nBatch 300/3125\t Loss 4.242622 (4.657397)\nBatch 400/3125\t Loss 4.686236 (4.526082)\nBatch 500/3125\t Loss 4.167071 (4.438787)\nBatch 600/3125\t Loss 3.931071 (4.385849)\nBatch 700/3125\t Loss 3.120471 (4.330513)\nBatch 800/3125\t Loss 4.129355 (4.283418)\nBatch 900/3125\t Loss 3.648975 (4.244224)\nBatch 1000/3125\t Loss 3.638278 (4.208969)\nBatch 1100/3125\t Loss 3.286736 (4.179218)\nBatch 1200/3125\t Loss 3.158395 (4.149884)\nBatch 1300/3125\t Loss 4.018410 (4.122935)\nBatch 1400/3125\t Loss 4.106685 (4.103152)\nBatch 1500/3125\t Loss 3.512030 (4.078481)\nBatch 1600/3125\t Loss 3.150679 (4.046856)\nBatch 1700/3125\t Loss 3.503009 (4.027922)\nBatch 1800/3125\t Loss 3.558290 (4.001999)\nBatch 1900/3125\t Loss 3.930381 (3.981598)\nBatch 2000/3125\t Loss 3.376851 (3.964149)\nBatch 2100/3125\t Loss 3.133754 (3.944517)\nBatch 2200/3125\t Loss 3.420199 (3.923387)\nBatch 2300/3125\t Loss 3.441493 (3.899154)\nBatch 2400/3125\t Loss 3.661265 (3.876143)\nBatch 2500/3125\t Loss 3.689844 (3.856266)\nBatch 2600/3125\t Loss 3.935500 (3.834494)\nBatch 2700/3125\t Loss 3.291731 (3.816092)\nBatch 2800/3125\t Loss 2.762142 (3.790826)\nBatch 2900/3125\t Loss 2.954118 (3.770638)\nBatch 3000/3125\t Loss 3.849776 (3.748142)\nBatch 3100/3125\t Loss 3.132042 (3.725661)\n==> Epoch 2/30\nBatch 100/3125\t Loss 3.155469 (3.006095)\nBatch 200/3125\t Loss 2.687585 (3.000343)\nBatch 300/3125\t Loss 3.042131 (3.042417)\nBatch 400/3125\t Loss 3.345652 (3.012508)\nBatch 500/3125\t Loss 3.403691 (3.000691)\nBatch 600/3125\t Loss 3.751887 (3.003049)\nBatch 700/3125\t Loss 1.748106 (2.979814)\nBatch 800/3125\t Loss 2.317893 (2.971446)\nBatch 900/3125\t Loss 2.194973 (2.952360)\nBatch 1000/3125\t Loss 2.006755 (2.940825)\nBatch 1100/3125\t Loss 2.223973 (2.919880)\nBatch 1200/3125\t Loss 2.711937 (2.908483)\nBatch 1300/3125\t Loss 1.875708 (2.900064)\nBatch 1400/3125\t Loss 3.153467 (2.888686)\nBatch 1500/3125\t Loss 2.569727 (2.877019)\nBatch 1600/3125\t Loss 4.015274 (2.871710)\nBatch 1700/3125\t Loss 2.897505 (2.860554)\nBatch 1800/3125\t Loss 3.542358 (2.850073)\nBatch 1900/3125\t Loss 3.377030 (2.845312)\nBatch 2000/3125\t Loss 2.659892 (2.839095)\nBatch 2100/3125\t Loss 2.441777 (2.834257)\nBatch 2200/3125\t Loss 2.504733 (2.830619)\nBatch 2300/3125\t Loss 2.494455 (2.821797)\nBatch 2400/3125\t Loss 2.087603 (2.811378)\nBatch 2500/3125\t Loss 2.397852 (2.805573)\nBatch 2600/3125\t Loss 1.538683 (2.795877)\nBatch 2700/3125\t Loss 3.369843 (2.786024)\nBatch 2800/3125\t Loss 2.644537 (2.778690)\nBatch 2900/3125\t Loss 2.507034 (2.767874)\nBatch 3000/3125\t Loss 3.365312 (2.763467)\nBatch 3100/3125\t Loss 2.622231 (2.754806)\n==> Epoch 3/30\nBatch 100/3125\t Loss 2.594475 (2.511679)\nBatch 200/3125\t Loss 2.138798 (2.526959)\nBatch 300/3125\t Loss 2.376999 (2.557192)\nBatch 400/3125\t Loss 2.664914 (2.555726)\nBatch 500/3125\t Loss 3.094889 (2.556046)\nBatch 600/3125\t Loss 2.301960 (2.561119)\nBatch 700/3125\t Loss 1.670181 (2.552279)\nBatch 800/3125\t Loss 3.088007 (2.546978)\nBatch 900/3125\t Loss 1.941098 (2.528932)\nBatch 1000/3125\t Loss 1.951956 (2.520152)\nBatch 1100/3125\t Loss 2.243314 (2.524042)\nBatch 1200/3125\t Loss 2.526840 (2.509159)\nBatch 1300/3125\t Loss 2.183044 (2.510111)\nBatch 1400/3125\t Loss 3.438847 (2.502103)\nBatch 1500/3125\t Loss 2.229099 (2.498012)\nBatch 1600/3125\t Loss 2.209038 (2.494071)\nBatch 1700/3125\t Loss 2.898744 (2.491127)\nBatch 1800/3125\t Loss 2.788607 (2.486169)\nBatch 1900/3125\t Loss 2.385855 (2.484566)\nBatch 2000/3125\t Loss 2.271934 (2.481829)\nBatch 2100/3125\t Loss 3.061116 (2.479182)\nBatch 2200/3125\t Loss 2.538958 (2.483163)\nBatch 2300/3125\t Loss 2.437358 (2.481058)\nBatch 2400/3125\t Loss 3.707185 (2.476919)\nBatch 2500/3125\t Loss 2.748845 (2.480424)\nBatch 2600/3125\t Loss 3.260368 (2.477881)\nBatch 2700/3125\t Loss 2.359137 (2.474944)\nBatch 2800/3125\t Loss 2.145397 (2.471338)\nBatch 2900/3125\t Loss 2.267092 (2.464258)\nBatch 3000/3125\t Loss 2.541038 (2.460865)\nBatch 3100/3125\t Loss 3.383815 (2.462133)\n==> Epoch 4/30\nBatch 100/3125\t Loss 3.729750 (2.468953)\nBatch 200/3125\t Loss 2.523220 (2.431169)\nBatch 300/3125\t Loss 1.433229 (2.437129)\nBatch 400/3125\t Loss 3.249281 (2.424651)\nBatch 500/3125\t Loss 2.649387 (2.438342)\nBatch 600/3125\t Loss 3.014754 (2.436135)\nBatch 700/3125\t Loss 1.657424 (2.426172)\nBatch 800/3125\t Loss 2.747247 (2.428541)\nBatch 900/3125\t Loss 4.009242 (2.421017)\nBatch 1000/3125\t Loss 3.991982 (2.419321)\nBatch 1100/3125\t Loss 2.025598 (2.409991)\nBatch 1200/3125\t Loss 2.031815 (2.415751)\nBatch 1300/3125\t Loss 2.610337 (2.412446)\nBatch 1400/3125\t Loss 2.251131 (2.410684)\nBatch 1500/3125\t Loss 3.899262 (2.406407)\nBatch 1600/3125\t Loss 1.564050 (2.396555)\nBatch 1700/3125\t Loss 1.752828 (2.389724)\nBatch 1800/3125\t Loss 2.642997 (2.385645)\nBatch 1900/3125\t Loss 1.251731 (2.384028)\nBatch 2000/3125\t Loss 2.549692 (2.379879)\nBatch 2100/3125\t Loss 1.105454 (2.378660)\nBatch 2200/3125\t Loss 1.962968 (2.377225)\nBatch 2300/3125\t Loss 2.593523 (2.373767)\nBatch 2400/3125\t Loss 1.611714 (2.368103)\nBatch 2500/3125\t Loss 2.364798 (2.363588)\nBatch 2600/3125\t Loss 2.192126 (2.361667)\nBatch 2700/3125\t Loss 2.144352 (2.360819)\nBatch 2800/3125\t Loss 3.624976 (2.358486)\nBatch 2900/3125\t Loss 2.883125 (2.356285)\nBatch 3000/3125\t Loss 1.571533 (2.351127)\nBatch 3100/3125\t Loss 3.099417 (2.353171)\n==> Epoch 5/30\nBatch 100/3125\t Loss 2.538603 (2.392675)\nBatch 200/3125\t Loss 0.970625 (2.363451)\nBatch 300/3125\t Loss 2.541080 (2.336673)\nBatch 400/3125\t Loss 2.341717 (2.311465)\nBatch 500/3125\t Loss 2.785191 (2.306607)\nBatch 600/3125\t Loss 2.429029 (2.309575)\nBatch 700/3125\t Loss 2.212740 (2.308014)\nBatch 800/3125\t Loss 2.228006 (2.304894)\nBatch 900/3125\t Loss 1.582311 (2.303138)\nBatch 1000/3125\t Loss 1.503771 (2.295862)\nBatch 1100/3125\t Loss 1.771852 (2.292307)\nBatch 1200/3125\t Loss 2.173877 (2.298738)\nBatch 1300/3125\t Loss 1.702327 (2.296186)\nBatch 1400/3125\t Loss 2.237257 (2.290373)\nBatch 1500/3125\t Loss 2.987154 (2.295417)\nBatch 1600/3125\t Loss 3.630147 (2.297457)\nBatch 1700/3125\t Loss 1.361774 (2.292504)\nBatch 1800/3125\t Loss 2.764048 (2.289560)\nBatch 1900/3125\t Loss 2.053424 (2.284236)\nBatch 2000/3125\t Loss 2.373934 (2.284723)\nBatch 2100/3125\t Loss 2.012907 (2.286058)\nBatch 2200/3125\t Loss 2.162403 (2.290006)\nBatch 2300/3125\t Loss 1.539120 (2.293330)\nBatch 2400/3125\t Loss 2.223374 (2.289625)\nBatch 2500/3125\t Loss 3.158969 (2.289578)\nBatch 2600/3125\t Loss 3.410975 (2.289151)\nBatch 2700/3125\t Loss 1.966861 (2.289163)\nBatch 2800/3125\t Loss 2.214525 (2.292710)\nBatch 2900/3125\t Loss 2.774333 (2.289891)\nBatch 3000/3125\t Loss 2.444032 (2.290633)\nBatch 3100/3125\t Loss 1.828540 (2.292328)\n==> Test\n       TNR    AUROC  DTACC  AUIN   AUOUT \nBas    15.210 69.259 64.060 70.401 66.890\nAcc: 63.65000\nBest Acc (%): 63.650\t\n==> Epoch 6/30\nBatch 100/3125\t Loss 1.777548 (2.314657)\nBatch 200/3125\t Loss 3.312471 (2.326935)\nBatch 300/3125\t Loss 2.533229 (2.294487)\nBatch 400/3125\t Loss 2.798030 (2.282024)\nBatch 500/3125\t Loss 1.258341 (2.254307)\nBatch 600/3125\t Loss 2.669059 (2.254098)\nBatch 700/3125\t Loss 2.434671 (2.260994)\nBatch 800/3125\t Loss 2.718306 (2.261396)\nBatch 900/3125\t Loss 2.393556 (2.253750)\nBatch 1000/3125\t Loss 2.412771 (2.258181)\nBatch 1100/3125\t Loss 1.479961 (2.254463)\nBatch 1200/3125\t Loss 3.198392 (2.252247)\nBatch 1300/3125\t Loss 3.018960 (2.254080)\nBatch 1400/3125\t Loss 1.627989 (2.258718)\nBatch 1500/3125\t Loss 2.367701 (2.262272)\nBatch 1600/3125\t Loss 3.305742 (2.265767)\nBatch 1700/3125\t Loss 2.635667 (2.269149)\nBatch 1800/3125\t Loss 2.374377 (2.268398)\nBatch 1900/3125\t Loss 2.546176 (2.274869)\nBatch 2000/3125\t Loss 2.600001 (2.273698)\nBatch 2100/3125\t Loss 2.190880 (2.270437)\nBatch 2200/3125\t Loss 3.519824 (2.269780)\nBatch 2300/3125\t Loss 2.170392 (2.264884)\nBatch 2400/3125\t Loss 3.504532 (2.261976)\nBatch 2500/3125\t Loss 3.364467 (2.263911)\nBatch 2600/3125\t Loss 2.714830 (2.264831)\nBatch 2700/3125\t Loss 2.009276 (2.263353)\nBatch 2800/3125\t Loss 2.117178 (2.262162)\nBatch 2900/3125\t Loss 2.313848 (2.260672)\nBatch 3000/3125\t Loss 1.134151 (2.257936)\nBatch 3100/3125\t Loss 2.746248 (2.257602)\n==> Epoch 7/30\nBatch 100/3125\t Loss 2.007078 (2.138445)\nBatch 200/3125\t Loss 1.741479 (2.181772)\nBatch 300/3125\t Loss 2.216632 (2.196622)\nBatch 400/3125\t Loss 2.396250 (2.206058)\nBatch 500/3125\t Loss 1.785071 (2.220471)\nBatch 600/3125\t Loss 2.756037 (2.230366)\nBatch 700/3125\t Loss 2.633512 (2.234474)\nBatch 800/3125\t Loss 2.366869 (2.239441)\nBatch 900/3125\t Loss 2.233793 (2.246604)\nBatch 1000/3125\t Loss 1.881732 (2.236399)\nBatch 1100/3125\t Loss 2.214567 (2.243142)\nBatch 1200/3125\t Loss 2.017065 (2.233638)\nBatch 1300/3125\t Loss 2.242388 (2.233310)\nBatch 1400/3125\t Loss 2.548755 (2.235477)\nBatch 1500/3125\t Loss 1.571864 (2.240619)\nBatch 1600/3125\t Loss 2.081148 (2.233822)\nBatch 1700/3125\t Loss 2.363002 (2.237741)\nBatch 1800/3125\t Loss 1.715211 (2.237256)\nBatch 1900/3125\t Loss 2.740146 (2.236621)\nBatch 2000/3125\t Loss 3.604606 (2.238987)\nBatch 2100/3125\t Loss 2.754875 (2.238715)\nBatch 2200/3125\t Loss 2.021046 (2.242752)\nBatch 2300/3125\t Loss 2.692970 (2.244583)\nBatch 2400/3125\t Loss 3.688558 (2.244166)\nBatch 2500/3125\t Loss 3.192699 (2.249725)\nBatch 2600/3125\t Loss 1.609307 (2.247667)\nBatch 2700/3125\t Loss 2.456224 (2.246108)\nBatch 2800/3125\t Loss 2.087156 (2.247774)\nBatch 2900/3125\t Loss 3.558675 (2.247523)\nBatch 3000/3125\t Loss 0.867662 (2.247294)\nBatch 3100/3125\t Loss 2.179701 (2.249861)\n==> Epoch 8/30\nBatch 100/3125\t Loss 1.441775 (2.213303)\nBatch 200/3125\t Loss 2.090998 (2.211985)\nBatch 300/3125\t Loss 1.820609 (2.209855)\nBatch 400/3125\t Loss 1.880097 (2.197858)\nBatch 500/3125\t Loss 2.158548 (2.208799)\nBatch 600/3125\t Loss 2.949439 (2.203960)\nBatch 700/3125\t Loss 1.579175 (2.208330)\nBatch 800/3125\t Loss 1.890834 (2.217250)\nBatch 900/3125\t Loss 2.438428 (2.211765)\nBatch 1000/3125\t Loss 2.096724 (2.208134)\nBatch 1100/3125\t Loss 1.841623 (2.212191)\nBatch 1200/3125\t Loss 2.106560 (2.207227)\nBatch 1300/3125\t Loss 2.160929 (2.202821)\nBatch 1400/3125\t Loss 1.246337 (2.208199)\nBatch 1500/3125\t Loss 1.145844 (2.207703)\nBatch 1600/3125\t Loss 2.567150 (2.206271)\nBatch 1700/3125\t Loss 1.862239 (2.204332)\nBatch 1800/3125\t Loss 2.964388 (2.202324)\nBatch 1900/3125\t Loss 1.303808 (2.201999)\nBatch 2000/3125\t Loss 2.435121 (2.208986)\nBatch 2100/3125\t Loss 2.189876 (2.207167)\nBatch 2200/3125\t Loss 2.142861 (2.209703)\nBatch 2300/3125\t Loss 1.996958 (2.211963)\nBatch 2400/3125\t Loss 2.082644 (2.213551)\nBatch 2500/3125\t Loss 2.787920 (2.212480)\nBatch 2600/3125\t Loss 1.089908 (2.209678)\nBatch 2700/3125\t Loss 3.133106 (2.206112)\nBatch 2800/3125\t Loss 2.517711 (2.206510)\nBatch 2900/3125\t Loss 2.475912 (2.205784)\nBatch 3000/3125\t Loss 2.164565 (2.208300)\nBatch 3100/3125\t Loss 1.912318 (2.205720)\n==> Epoch 9/30\nBatch 100/3125\t Loss 1.941672 (2.306540)\nBatch 200/3125\t Loss 1.560067 (2.281922)\nBatch 300/3125\t Loss 2.119165 (2.225886)\nBatch 400/3125\t Loss 2.139863 (2.208813)\nBatch 500/3125\t Loss 2.638411 (2.201418)\nBatch 600/3125\t Loss 2.255055 (2.211319)\nBatch 700/3125\t Loss 1.374030 (2.205285)\nBatch 800/3125\t Loss 2.397686 (2.189722)\nBatch 900/3125\t Loss 2.637532 (2.198506)\nBatch 1000/3125\t Loss 1.883381 (2.203035)\nBatch 1100/3125\t Loss 1.904986 (2.198123)\nBatch 1200/3125\t Loss 2.042648 (2.202533)\nBatch 1300/3125\t Loss 1.658957 (2.207837)\nBatch 1400/3125\t Loss 2.762987 (2.203985)\nBatch 1500/3125\t Loss 1.787970 (2.203799)\nBatch 1600/3125\t Loss 2.879144 (2.207996)\nBatch 1700/3125\t Loss 2.673599 (2.209189)\nBatch 1800/3125\t Loss 1.920784 (2.212276)\nBatch 1900/3125\t Loss 3.872135 (2.209292)\nBatch 2000/3125\t Loss 1.675933 (2.207139)\nBatch 2100/3125\t Loss 2.064306 (2.206664)\nBatch 2200/3125\t Loss 3.479942 (2.206921)\nBatch 2300/3125\t Loss 2.740061 (2.208001)\nBatch 2400/3125\t Loss 2.442543 (2.209281)\nBatch 2500/3125\t Loss 2.169291 (2.208874)\nBatch 2600/3125\t Loss 1.531590 (2.208148)\nBatch 2700/3125\t Loss 2.056040 (2.204438)\nBatch 2800/3125\t Loss 3.206968 (2.204018)\nBatch 2900/3125\t Loss 2.418868 (2.204660)\nBatch 3000/3125\t Loss 1.497491 (2.204870)\nBatch 3100/3125\t Loss 2.448465 (2.206809)\n==> Epoch 10/30\nBatch 100/3125\t Loss 2.486062 (2.170356)\nBatch 200/3125\t Loss 1.204418 (2.182363)\nBatch 300/3125\t Loss 2.353178 (2.179689)\nBatch 400/3125\t Loss 1.284620 (2.179538)\nBatch 500/3125\t Loss 2.717241 (2.170736)\nBatch 600/3125\t Loss 1.794300 (2.184724)\nBatch 700/3125\t Loss 1.259105 (2.174378)\nBatch 800/3125\t Loss 2.430944 (2.169804)\nBatch 900/3125\t Loss 1.141326 (2.160046)\nBatch 1000/3125\t Loss 2.333555 (2.169666)\nBatch 1100/3125\t Loss 2.152837 (2.171906)\nBatch 1200/3125\t Loss 2.149566 (2.176703)\nBatch 1300/3125\t Loss 2.439418 (2.180025)\nBatch 1400/3125\t Loss 1.904158 (2.178483)\nBatch 1500/3125\t Loss 2.418774 (2.180175)\nBatch 1600/3125\t Loss 1.180347 (2.180570)\nBatch 1700/3125\t Loss 2.635194 (2.180545)\nBatch 1800/3125\t Loss 2.217356 (2.179683)\nBatch 1900/3125\t Loss 1.582138 (2.179557)\nBatch 2000/3125\t Loss 2.282408 (2.180141)\nBatch 2100/3125\t Loss 1.104757 (2.181603)\nBatch 2200/3125\t Loss 1.973397 (2.177753)\nBatch 2300/3125\t Loss 1.972048 (2.183837)\nBatch 2400/3125\t Loss 2.805800 (2.184102)\nBatch 2500/3125\t Loss 2.706427 (2.188238)\nBatch 2600/3125\t Loss 1.985068 (2.193763)\nBatch 2700/3125\t Loss 2.207513 (2.195831)\nBatch 2800/3125\t Loss 2.739908 (2.198220)\nBatch 2900/3125\t Loss 1.978964 (2.198026)\nBatch 3000/3125\t Loss 3.402340 (2.196887)\nBatch 3100/3125\t Loss 3.415576 (2.196870)\n==> Test\n       TNR    AUROC  DTACC  AUIN   AUOUT \nBas    12.210 69.786 64.965 71.707 65.560\nAcc: 63.66000\nBest Acc (%): 63.660\t\n==> Epoch 11/30\nBatch 100/3125\t Loss 1.711452 (2.233635)\nBatch 200/3125\t Loss 1.860604 (2.223239)\nBatch 300/3125\t Loss 2.808959 (2.195689)\nBatch 400/3125\t Loss 3.267138 (2.206094)\nBatch 500/3125\t Loss 1.173286 (2.204139)\nBatch 600/3125\t Loss 1.711577 (2.186581)\nBatch 700/3125\t Loss 1.991232 (2.189768)\nBatch 800/3125\t Loss 2.009033 (2.206295)\nBatch 900/3125\t Loss 1.495409 (2.190898)\nBatch 1000/3125\t Loss 2.394629 (2.195801)\nBatch 1100/3125\t Loss 2.394467 (2.194361)\nBatch 1200/3125\t Loss 2.199919 (2.192338)\nBatch 1300/3125\t Loss 2.140401 (2.198483)\nBatch 1400/3125\t Loss 2.448743 (2.198238)\nBatch 1500/3125\t Loss 3.427709 (2.200826)\nBatch 1600/3125\t Loss 2.098933 (2.200347)\nBatch 1700/3125\t Loss 1.048311 (2.202191)\nBatch 1800/3125\t Loss 2.459299 (2.200593)\nBatch 1900/3125\t Loss 2.042114 (2.193956)\nBatch 2000/3125\t Loss 2.578890 (2.197937)\nBatch 2100/3125\t Loss 2.670040 (2.196031)\nBatch 2200/3125\t Loss 2.308511 (2.193506)\nBatch 2300/3125\t Loss 2.359658 (2.197021)\nBatch 2400/3125\t Loss 1.408450 (2.196923)\nBatch 2500/3125\t Loss 2.286876 (2.195818)\nBatch 2600/3125\t Loss 2.936723 (2.194705)\nBatch 2700/3125\t Loss 1.247040 (2.195553)\nBatch 2800/3125\t Loss 1.794632 (2.196998)\nBatch 2900/3125\t Loss 1.212527 (2.193878)\nBatch 3000/3125\t Loss 3.809122 (2.193998)\nBatch 3100/3125\t Loss 1.884374 (2.194570)\n==> Epoch 12/30\nBatch 100/3125\t Loss 0.947172 (2.273288)\nBatch 200/3125\t Loss 1.122346 (2.252276)\nBatch 300/3125\t Loss 0.948747 (2.225018)\nBatch 400/3125\t Loss 2.339916 (2.217945)\nBatch 500/3125\t Loss 2.265210 (2.215784)\nBatch 600/3125\t Loss 2.341855 (2.205217)\nBatch 700/3125\t Loss 1.688126 (2.196958)\nBatch 800/3125\t Loss 0.886489 (2.201463)\nBatch 900/3125\t Loss 1.192269 (2.189666)\nBatch 1000/3125\t Loss 1.529515 (2.185662)\nBatch 1100/3125\t Loss 1.620471 (2.178248)\nBatch 1200/3125\t Loss 2.286958 (2.174287)\nBatch 1300/3125\t Loss 1.074145 (2.173311)\nBatch 1400/3125\t Loss 2.062615 (2.170016)\nBatch 1500/3125\t Loss 1.713635 (2.163964)\nBatch 1600/3125\t Loss 1.961199 (2.166535)\nBatch 1700/3125\t Loss 1.208379 (2.171634)\nBatch 1800/3125\t Loss 2.614980 (2.176039)\nBatch 1900/3125\t Loss 1.774004 (2.176038)\nBatch 2000/3125\t Loss 2.285419 (2.180204)\nBatch 2100/3125\t Loss 1.426746 (2.183122)\nBatch 2200/3125\t Loss 1.484849 (2.182388)\nBatch 2300/3125\t Loss 2.897222 (2.181745)\nBatch 2400/3125\t Loss 3.227069 (2.180544)\nBatch 2500/3125\t Loss 2.934784 (2.182269)\nBatch 2600/3125\t Loss 2.532733 (2.183451)\nBatch 2700/3125\t Loss 2.166214 (2.182204)\nBatch 2800/3125\t Loss 2.357132 (2.182610)\nBatch 2900/3125\t Loss 3.125509 (2.180578)\nBatch 3000/3125\t Loss 2.429808 (2.183916)\nBatch 3100/3125\t Loss 2.217748 (2.182013)\n==> Epoch 13/30\nBatch 100/3125\t Loss 2.666994 (2.144662)\nBatch 200/3125\t Loss 1.378440 (2.148441)\nBatch 300/3125\t Loss 1.505572 (2.176489)\nBatch 400/3125\t Loss 1.551593 (2.177294)\nBatch 500/3125\t Loss 2.334177 (2.162449)\nBatch 600/3125\t Loss 3.288791 (2.164689)\nBatch 700/3125\t Loss 2.484479 (2.163080)\nBatch 800/3125\t Loss 1.404565 (2.164644)\nBatch 900/3125\t Loss 2.228640 (2.169318)\nBatch 1000/3125\t Loss 1.586556 (2.169235)\nBatch 1100/3125\t Loss 1.677433 (2.160854)\nBatch 1200/3125\t Loss 2.970079 (2.163513)\nBatch 1300/3125\t Loss 2.064128 (2.164117)\nBatch 1400/3125\t Loss 2.544044 (2.171344)\nBatch 1500/3125\t Loss 3.281885 (2.174291)\nBatch 1600/3125\t Loss 1.759601 (2.171908)\nBatch 1700/3125\t Loss 3.044223 (2.171555)\nBatch 1800/3125\t Loss 1.312345 (2.171410)\nBatch 1900/3125\t Loss 1.722272 (2.175201)\nBatch 2000/3125\t Loss 2.224729 (2.175229)\nBatch 2100/3125\t Loss 1.758207 (2.175945)\nBatch 2200/3125\t Loss 2.243424 (2.177100)\nBatch 2300/3125\t Loss 1.269468 (2.174297)\nBatch 2400/3125\t Loss 1.234411 (2.173393)\nBatch 2500/3125\t Loss 2.483850 (2.169238)\nBatch 2600/3125\t Loss 1.277834 (2.172079)\nBatch 2700/3125\t Loss 3.377680 (2.174174)\nBatch 2800/3125\t Loss 2.818256 (2.175365)\nBatch 2900/3125\t Loss 1.655269 (2.174043)\nBatch 3000/3125\t Loss 3.077469 (2.177409)\nBatch 3100/3125\t Loss 3.042427 (2.179997)\n==> Epoch 14/30\nBatch 100/3125\t Loss 2.175132 (2.121411)\nBatch 200/3125\t Loss 1.942586 (2.113361)\nBatch 300/3125\t Loss 1.746242 (2.149621)\nBatch 400/3125\t Loss 1.946440 (2.151026)\nBatch 500/3125\t Loss 3.515533 (2.141481)\nBatch 600/3125\t Loss 2.522666 (2.132213)\nBatch 700/3125\t Loss 2.372581 (2.126050)\nBatch 800/3125\t Loss 2.777681 (2.119710)\nBatch 900/3125\t Loss 2.649639 (2.131331)\nBatch 1000/3125\t Loss 1.937308 (2.134734)\nBatch 1100/3125\t Loss 1.982429 (2.138460)\nBatch 1200/3125\t Loss 3.451221 (2.137843)\nBatch 1300/3125\t Loss 1.346548 (2.140172)\nBatch 1400/3125\t Loss 2.544357 (2.149030)\nBatch 1500/3125\t Loss 2.022030 (2.152721)\nBatch 1600/3125\t Loss 2.566906 (2.154197)\nBatch 1700/3125\t Loss 1.775427 (2.148440)\nBatch 1800/3125\t Loss 2.176306 (2.152753)\nBatch 1900/3125\t Loss 2.535952 (2.149362)\nBatch 2000/3125\t Loss 2.319699 (2.149984)\nBatch 2100/3125\t Loss 3.430854 (2.151644)\nBatch 2200/3125\t Loss 1.771518 (2.149896)\nBatch 2300/3125\t Loss 3.122879 (2.152364)\nBatch 2400/3125\t Loss 3.006124 (2.153899)\nBatch 2500/3125\t Loss 1.749832 (2.152504)\nBatch 2600/3125\t Loss 1.941146 (2.156164)\nBatch 2700/3125\t Loss 1.745185 (2.152302)\nBatch 2800/3125\t Loss 1.720111 (2.149305)\nBatch 2900/3125\t Loss 2.042849 (2.149133)\nBatch 3000/3125\t Loss 3.048041 (2.147381)\nBatch 3100/3125\t Loss 2.128661 (2.147246)\n==> Epoch 15/30\nBatch 100/3125\t Loss 3.202229 (2.176604)\nBatch 200/3125\t Loss 1.533250 (2.152570)\nBatch 300/3125\t Loss 2.454280 (2.149726)\nBatch 400/3125\t Loss 0.898280 (2.142459)\nBatch 500/3125\t Loss 1.989240 (2.139214)\nBatch 600/3125\t Loss 2.042518 (2.140369)\nBatch 700/3125\t Loss 1.904510 (2.153698)\nBatch 800/3125\t Loss 2.979527 (2.144560)\nBatch 900/3125\t Loss 0.797556 (2.143937)\nBatch 1000/3125\t Loss 1.733607 (2.151162)\nBatch 1100/3125\t Loss 2.298696 (2.153405)\nBatch 1200/3125\t Loss 2.864766 (2.153517)\nBatch 1300/3125\t Loss 1.924639 (2.150759)\nBatch 1400/3125\t Loss 2.349325 (2.154648)\nBatch 1500/3125\t Loss 3.122326 (2.154263)\nBatch 1600/3125\t Loss 1.945625 (2.159926)\nBatch 1700/3125\t Loss 2.000636 (2.159671)\nBatch 1800/3125\t Loss 2.252822 (2.166343)\nBatch 1900/3125\t Loss 1.974489 (2.167358)\nBatch 2000/3125\t Loss 1.351542 (2.165701)\nBatch 2100/3125\t Loss 1.256353 (2.165498)\nBatch 2200/3125\t Loss 3.088275 (2.165161)\nBatch 2300/3125\t Loss 2.078382 (2.165852)\nBatch 2400/3125\t Loss 3.808980 (2.168113)\nBatch 2500/3125\t Loss 2.885215 (2.170187)\nBatch 2600/3125\t Loss 1.749716 (2.169708)\nBatch 2700/3125\t Loss 2.752134 (2.166973)\nBatch 2800/3125\t Loss 2.417642 (2.167563)\nBatch 2900/3125\t Loss 1.166853 (2.164272)\nBatch 3000/3125\t Loss 2.359858 (2.166963)\nBatch 3100/3125\t Loss 1.810987 (2.165751)\n==> Test\n       TNR    AUROC  DTACC  AUIN   AUOUT \nBas    17.880 73.891 68.080 75.391 70.513\nAcc: 71.59000\nBest Acc (%): 71.590\t\n==> Epoch 16/30\nBatch 100/3125\t Loss 2.074588 (2.152821)\nBatch 200/3125\t Loss 1.341769 (2.186121)\nBatch 300/3125\t Loss 2.089905 (2.162703)\nBatch 700/3125\t Loss 1.630715 (2.186382)\nBatch 800/3125\t Loss 2.394800 (2.179370)\nBatch 900/3125\t Loss 2.998629 (2.186443)\nBatch 1000/3125\t Loss 1.812132 (2.187467)\nBatch 1100/3125\t Loss 3.141808 (2.180122)\nBatch 1200/3125\t Loss 1.461812 (2.174082)\nBatch 1300/3125\t Loss 1.434208 (2.175165)\nBatch 1400/3125\t Loss 2.303365 (2.173536)\nBatch 1500/3125\t Loss 1.422356 (2.178014)\nBatch 1600/3125\t Loss 1.766806 (2.178512)\nBatch 1700/3125\t Loss 2.713031 (2.174940)\nBatch 1800/3125\t Loss 2.837346 (2.166946)\nBatch 1900/3125\t Loss 1.877762 (2.166951)\nBatch 2000/3125\t Loss 2.095557 (2.167578)\nBatch 2100/3125\t Loss 1.708628 (2.166677)\nBatch 2200/3125\t Loss 2.139872 (2.164173)\nBatch 2300/3125\t Loss 1.092084 (2.160663)\nBatch 2400/3125\t Loss 2.469494 (2.160495)\nBatch 2500/3125\t Loss 2.296159 (2.162581)\nBatch 2600/3125\t Loss 2.331768 (2.161757)\nBatch 2700/3125\t Loss 3.097464 (2.162529)\nBatch 2800/3125\t Loss 1.586073 (2.161921)\nBatch 2900/3125\t Loss 1.123424 (2.160673)\nBatch 3000/3125\t Loss 1.893100 (2.159516)\nBatch 3100/3125\t Loss 2.109668 (2.159232)\n==> Epoch 17/30\nBatch 100/3125\t Loss 2.872890 (2.162693)\nBatch 200/3125\t Loss 2.028959 (2.219079)\nBatch 300/3125\t Loss 1.700014 (2.185108)\nBatch 400/3125\t Loss 2.389661 (2.213791)\nBatch 500/3125\t Loss 1.821483 (2.182606)\nBatch 600/3125\t Loss 1.922358 (2.191205)\nBatch 700/3125\t Loss 2.781726 (2.177978)\nBatch 800/3125\t Loss 2.281677 (2.187509)\nBatch 900/3125\t Loss 1.666784 (2.191075)\nBatch 1000/3125\t Loss 2.430493 (2.187133)\nBatch 1100/3125\t Loss 1.899135 (2.187167)\nBatch 1200/3125\t Loss 1.691809 (2.183161)\nBatch 1300/3125\t Loss 1.916229 (2.176800)\nBatch 1400/3125\t Loss 2.306420 (2.177161)\nBatch 1500/3125\t Loss 3.525079 (2.175333)\nBatch 1600/3125\t Loss 2.824797 (2.177652)\nBatch 1700/3125\t Loss 1.925763 (2.183112)\nBatch 1800/3125\t Loss 2.819272 (2.180905)\nBatch 1900/3125\t Loss 2.180031 (2.176662)\nBatch 2000/3125\t Loss 2.088870 (2.177658)\nBatch 2100/3125\t Loss 3.359735 (2.178784)\nBatch 2200/3125\t Loss 3.257881 (2.174703)\nBatch 2300/3125\t Loss 1.916924 (2.172739)\nBatch 2400/3125\t Loss 3.409927 (2.172211)\nBatch 2500/3125\t Loss 2.648676 (2.172988)\nBatch 2600/3125\t Loss 2.510325 (2.167809)\nBatch 2700/3125\t Loss 2.333713 (2.169598)\nBatch 2800/3125\t Loss 1.704224 (2.168493)\nBatch 2900/3125\t Loss 1.916467 (2.168544)\nBatch 3000/3125\t Loss 3.018022 (2.169375)\nBatch 3100/3125\t Loss 3.815188 (2.170882)\n==> Epoch 18/30\nBatch 100/3125\t Loss 1.500214 (2.091501)\nBatch 200/3125\t Loss 3.292477 (2.147926)\nBatch 300/3125\t Loss 2.202623 (2.146432)\nBatch 400/3125\t Loss 2.355327 (2.142723)\nBatch 500/3125\t Loss 2.382941 (2.142066)\nBatch 600/3125\t Loss 1.581326 (2.135611)\nBatch 700/3125\t Loss 1.341883 (2.136422)\nBatch 800/3125\t Loss 1.730103 (2.145665)\nBatch 900/3125\t Loss 1.910617 (2.168071)\nBatch 1000/3125\t Loss 1.953401 (2.164381)\nBatch 1100/3125\t Loss 2.033497 (2.164053)\nBatch 1200/3125\t Loss 1.359643 (2.154549)\nBatch 1300/3125\t Loss 0.884741 (2.152779)\nBatch 1400/3125\t Loss 1.895513 (2.152069)\nBatch 1500/3125\t Loss 1.434205 (2.151386)\nBatch 1600/3125\t Loss 2.189951 (2.159476)\nBatch 1700/3125\t Loss 1.277062 (2.159280)\nBatch 1800/3125\t Loss 2.315466 (2.155233)\nBatch 1900/3125\t Loss 1.175392 (2.155897)\nBatch 2000/3125\t Loss 2.556437 (2.159604)\nBatch 2100/3125\t Loss 3.798805 (2.161668)\nBatch 2200/3125\t Loss 1.432974 (2.162978)\nBatch 2300/3125\t Loss 1.396178 (2.162236)\nBatch 2400/3125\t Loss 2.033354 (2.164162)\nBatch 2500/3125\t Loss 2.186031 (2.163770)\nBatch 2600/3125\t Loss 2.526170 (2.163594)\nBatch 2700/3125\t Loss 2.819590 (2.166752)\nBatch 2800/3125\t Loss 1.040962 (2.169123)\nBatch 2900/3125\t Loss 1.188990 (2.167113)\nBatch 3000/3125\t Loss 2.601942 (2.168767)\nBatch 3100/3125\t Loss 1.499650 (2.168281)\n==> Epoch 19/30\nBatch 100/3125\t Loss 2.315708 (2.204672)\nBatch 200/3125\t Loss 1.747492 (2.229673)\nBatch 300/3125\t Loss 1.155660 (2.163172)\nBatch 400/3125\t Loss 1.328065 (2.152097)\nBatch 500/3125\t Loss 1.670334 (2.152057)\nBatch 600/3125\t Loss 3.616968 (2.149747)\nBatch 700/3125\t Loss 2.148842 (2.152480)\nBatch 800/3125\t Loss 2.305497 (2.135431)\nBatch 900/3125\t Loss 1.925446 (2.127592)\nBatch 1000/3125\t Loss 2.661845 (2.134220)\nBatch 1100/3125\t Loss 1.517044 (2.142175)\nBatch 1200/3125\t Loss 2.560365 (2.143182)\nBatch 1300/3125\t Loss 2.786915 (2.140641)\nBatch 1400/3125\t Loss 1.506507 (2.139264)\nBatch 1500/3125\t Loss 1.263509 (2.133341)\nBatch 1600/3125\t Loss 2.506923 (2.130524)\nBatch 1700/3125\t Loss 2.990511 (2.137064)\nBatch 1800/3125\t Loss 1.580543 (2.133042)\nBatch 1900/3125\t Loss 2.488206 (2.140359)\nBatch 2000/3125\t Loss 2.028640 (2.143047)\nBatch 2100/3125\t Loss 2.230984 (2.142119)\nBatch 2200/3125\t Loss 1.738701 (2.143850)\nBatch 2300/3125\t Loss 2.052968 (2.144335)\nBatch 2400/3125\t Loss 2.299090 (2.147191)\nBatch 2500/3125\t Loss 2.343694 (2.142280)\nBatch 2600/3125\t Loss 3.114941 (2.145506)\nBatch 2700/3125\t Loss 2.236995 (2.147159)\nBatch 2800/3125\t Loss 1.435297 (2.146030)\nBatch 2900/3125\t Loss 1.552228 (2.146452)\nBatch 3000/3125\t Loss 1.767172 (2.148499)\nBatch 3100/3125\t Loss 2.703079 (2.147181)\n==> Epoch 20/30\nBatch 100/3125\t Loss 1.799403 (2.141619)\nBatch 200/3125\t Loss 2.648531 (2.140912)\nBatch 300/3125\t Loss 2.212842 (2.115423)\nBatch 400/3125\t Loss 2.215005 (2.134224)\nBatch 500/3125\t Loss 1.922520 (2.111714)\nBatch 600/3125\t Loss 0.699988 (2.112131)\nBatch 700/3125\t Loss 1.692169 (2.099483)\nBatch 800/3125\t Loss 2.261580 (2.106043)\nBatch 900/3125\t Loss 3.690712 (2.112179)\nBatch 1000/3125\t Loss 1.532789 (2.114055)\nBatch 1100/3125\t Loss 2.611313 (2.113545)\nBatch 1200/3125\t Loss 2.084961 (2.115888)\nBatch 1300/3125\t Loss 2.302711 (2.122732)\nBatch 1400/3125\t Loss 1.261113 (2.119238)\nBatch 1500/3125\t Loss 1.175444 (2.125243)\nBatch 1600/3125\t Loss 2.348430 (2.130030)\nBatch 1700/3125\t Loss 2.701302 (2.137919)\nBatch 1800/3125\t Loss 2.174138 (2.144069)\nBatch 1900/3125\t Loss 2.082864 (2.142929)\nBatch 2000/3125\t Loss 1.893121 (2.138958)\nBatch 2100/3125\t Loss 2.042832 (2.137738)\nBatch 2200/3125\t Loss 2.232608 (2.142203)\nBatch 2300/3125\t Loss 3.170618 (2.143396)\nBatch 2400/3125\t Loss 2.228614 (2.143580)\nBatch 2500/3125\t Loss 1.464591 (2.147865)\nBatch 2600/3125\t Loss 2.066391 (2.146107)\nBatch 2700/3125\t Loss 3.654024 (2.150029)\nBatch 2800/3125\t Loss 1.101477 (2.149743)\nBatch 2900/3125\t Loss 2.162211 (2.148620)\nBatch 3000/3125\t Loss 2.984654 (2.149074)\nBatch 3100/3125\t Loss 2.461969 (2.149194)\n==> Test\n       TNR    AUROC  DTACC  AUIN   AUOUT \nBas    14.280 71.003 65.635 73.226 67.115\nAcc: 68.40000\n==> Epoch 21/30\nBatch 100/3125\t Loss 1.797545 (2.189024)\nBatch 200/3125\t Loss 2.027755 (2.140449)\nBatch 300/3125\t Loss 2.130271 (2.156948)\nBatch 400/3125\t Loss 2.871426 (2.115567)\nBatch 500/3125\t Loss 1.681041 (2.126353)\nBatch 600/3125\t Loss 2.001747 (2.135789)\nBatch 700/3125\t Loss 1.722253 (2.123928)\nBatch 800/3125\t Loss 2.601871 (2.140857)\nBatch 900/3125\t Loss 2.910452 (2.155303)\nBatch 1000/3125\t Loss 1.605956 (2.146317)\nBatch 1100/3125\t Loss 2.188959 (2.151187)\nBatch 1200/3125\t Loss 1.748252 (2.147542)\nBatch 1300/3125\t Loss 3.166068 (2.155549)\nBatch 1400/3125\t Loss 3.644852 (2.152100)\nBatch 1500/3125\t Loss 1.917457 (2.157213)\nBatch 1600/3125\t Loss 3.868982 (2.152147)\nBatch 1700/3125\t Loss 3.068975 (2.153236)\nBatch 1800/3125\t Loss 2.559354 (2.152749)\nBatch 1900/3125\t Loss 1.778887 (2.156457)\nBatch 2000/3125\t Loss 2.571918 (2.158590)\nBatch 2100/3125\t Loss 2.163416 (2.163540)\nBatch 2200/3125\t Loss 2.248453 (2.161720)\nBatch 2300/3125\t Loss 1.164878 (2.163352)\nBatch 2400/3125\t Loss 1.259078 (2.160289)\nBatch 2500/3125\t Loss 2.987633 (2.158997)\nBatch 2600/3125\t Loss 2.637405 (2.157585)\nBatch 2700/3125\t Loss 2.017724 (2.160825)\nBatch 2800/3125\t Loss 2.073324 (2.161003)\nBatch 2900/3125\t Loss 1.496866 (2.159663)\nBatch 3000/3125\t Loss 1.109694 (2.157480)\nBatch 3100/3125\t Loss 2.287180 (2.154521)\n==> Epoch 22/30\nBatch 100/3125\t Loss 1.596187 (2.118753)\nBatch 200/3125\t Loss 2.262554 (2.151286)\nBatch 300/3125\t Loss 2.462772 (2.191791)\nBatch 400/3125\t Loss 2.808549 (2.165155)\nBatch 500/3125\t Loss 1.221421 (2.163086)\nBatch 600/3125\t Loss 2.482321 (2.161095)\nBatch 700/3125\t Loss 2.647053 (2.154027)\nBatch 800/3125\t Loss 2.180530 (2.150699)\nBatch 900/3125\t Loss 1.586959 (2.149636)\nBatch 1000/3125\t Loss 1.447473 (2.150048)\nBatch 1100/3125\t Loss 0.838303 (2.152154)\nBatch 1200/3125\t Loss 2.637150 (2.152636)\nBatch 1300/3125\t Loss 2.169230 (2.157584)\nBatch 1400/3125\t Loss 2.294317 (2.163622)\nBatch 1500/3125\t Loss 0.796244 (2.158957)\nBatch 1600/3125\t Loss 2.966344 (2.155270)\nBatch 1700/3125\t Loss 1.920027 (2.152387)\nBatch 1800/3125\t Loss 2.946338 (2.150321)\nBatch 1900/3125\t Loss 2.839650 (2.154718)\nBatch 2000/3125\t Loss 2.374669 (2.155292)\nBatch 2100/3125\t Loss 1.619845 (2.156289)\nBatch 2200/3125\t Loss 2.531532 (2.158816)\nBatch 2300/3125\t Loss 2.857292 (2.160664)\nBatch 2400/3125\t Loss 1.739803 (2.155871)\nBatch 2500/3125\t Loss 2.212800 (2.157756)\nBatch 2600/3125\t Loss 2.573721 (2.156936)\nBatch 2700/3125\t Loss 1.670807 (2.158551)\nBatch 2800/3125\t Loss 1.400166 (2.160880)\nBatch 2900/3125\t Loss 1.334271 (2.159610)\nBatch 3000/3125\t Loss 2.320982 (2.161151)\nBatch 3100/3125\t Loss 3.224941 (2.157198)\n==> Epoch 23/30\nBatch 100/3125\t Loss 1.940207 (2.301545)\nBatch 200/3125\t Loss 2.253131 (2.261548)\nBatch 300/3125\t Loss 1.048633 (2.215677)\nBatch 400/3125\t Loss 1.169903 (2.219636)\nBatch 500/3125\t Loss 2.739233 (2.208101)\nBatch 600/3125\t Loss 1.625046 (2.178839)\nBatch 700/3125\t Loss 1.891858 (2.170642)\nBatch 800/3125\t Loss 1.990549 (2.169481)\nBatch 900/3125\t Loss 1.492135 (2.172233)\nBatch 1000/3125\t Loss 2.337273 (2.175479)\nBatch 1100/3125\t Loss 1.596107 (2.170040)\nBatch 1200/3125\t Loss 3.214538 (2.165426)\nBatch 1300/3125\t Loss 3.372575 (2.160498)\nBatch 1400/3125\t Loss 2.145559 (2.164618)\nBatch 1500/3125\t Loss 2.178162 (2.159279)\nBatch 1600/3125\t Loss 2.451147 (2.152410)\nBatch 1700/3125\t Loss 1.680480 (2.152743)\nBatch 1800/3125\t Loss 2.621112 (2.158556)\nBatch 1900/3125\t Loss 2.268465 (2.152195)\nBatch 2000/3125\t Loss 2.458382 (2.146731)\nBatch 2100/3125\t Loss 1.759041 (2.155086)\nBatch 2200/3125\t Loss 2.190795 (2.156815)\nBatch 2300/3125\t Loss 3.471756 (2.158783)\nBatch 2400/3125\t Loss 1.852414 (2.158822)\nBatch 2500/3125\t Loss 1.779336 (2.160438)\nBatch 2600/3125\t Loss 2.528475 (2.159980)\nBatch 2700/3125\t Loss 1.771688 (2.159937)\nBatch 2800/3125\t Loss 2.757453 (2.158944)\nBatch 2900/3125\t Loss 2.001225 (2.158241)\nBatch 3000/3125\t Loss 2.380333 (2.156406)\nBatch 3100/3125\t Loss 3.233577 (2.153334)\n==> Epoch 24/30\nBatch 100/3125\t Loss 2.053346 (2.184156)\nBatch 200/3125\t Loss 1.843183 (2.136729)\nBatch 300/3125\t Loss 2.701357 (2.126261)\nBatch 400/3125\t Loss 2.653691 (2.156176)\nBatch 500/3125\t Loss 2.388342 (2.153313)\nBatch 600/3125\t Loss 2.025937 (2.145240)\nBatch 700/3125\t Loss 0.830333 (2.160238)\nBatch 800/3125\t Loss 2.623979 (2.139846)\nBatch 900/3125\t Loss 1.869682 (2.150386)\nBatch 1000/3125\t Loss 2.272894 (2.141691)\nBatch 1100/3125\t Loss 2.023673 (2.143717)\nBatch 1200/3125\t Loss 2.688763 (2.144982)\nBatch 1300/3125\t Loss 1.401058 (2.149148)\nBatch 1400/3125\t Loss 1.969941 (2.157379)\nBatch 1500/3125\t Loss 1.469464 (2.159112)\nBatch 1600/3125\t Loss 2.886251 (2.160894)\nBatch 1700/3125\t Loss 1.786449 (2.162598)\nBatch 1800/3125\t Loss 2.262179 (2.158236)\nBatch 1900/3125\t Loss 2.646763 (2.159838)\nBatch 2000/3125\t Loss 2.129194 (2.156193)\nBatch 2100/3125\t Loss 2.425225 (2.152600)\nBatch 2200/3125\t Loss 2.105379 (2.151872)\nBatch 2300/3125\t Loss 2.925542 (2.152865)\nBatch 2400/3125\t Loss 1.854914 (2.157855)\nBatch 2500/3125\t Loss 4.207526 (2.157210)\nBatch 2600/3125\t Loss 1.476029 (2.157104)\nBatch 2700/3125\t Loss 3.085100 (2.158854)\nBatch 2800/3125\t Loss 2.141291 (2.156111)\nBatch 2900/3125\t Loss 1.890413 (2.155414)\nBatch 3000/3125\t Loss 2.128524 (2.158726)\nBatch 3100/3125\t Loss 3.405985 (2.160351)\n==> Epoch 25/30\nBatch 100/3125\t Loss 3.842129 (2.134294)\nBatch 200/3125\t Loss 2.217124 (2.145707)\nBatch 300/3125\t Loss 3.246534 (2.126883)\nBatch 400/3125\t Loss 2.498552 (2.139257)\nBatch 500/3125\t Loss 2.215419 (2.127282)\nBatch 600/3125\t Loss 2.289166 (2.115639)\nBatch 700/3125\t Loss 1.854794 (2.127351)\nBatch 800/3125\t Loss 2.280242 (2.119867)\nBatch 900/3125\t Loss 1.865237 (2.125138)\nBatch 1000/3125\t Loss 1.981417 (2.136715)\nBatch 1100/3125\t Loss 2.512951 (2.140128)\nBatch 1200/3125\t Loss 1.351223 (2.141900)\nBatch 1300/3125\t Loss 2.107733 (2.142001)\nBatch 1400/3125\t Loss 3.256506 (2.143071)\nBatch 1500/3125\t Loss 1.199616 (2.139706)\nBatch 1600/3125\t Loss 2.325919 (2.139447)\nBatch 1700/3125\t Loss 1.236996 (2.141560)\nBatch 1800/3125\t Loss 2.668466 (2.145836)\nBatch 1900/3125\t Loss 1.583019 (2.143136)\nBatch 2000/3125\t Loss 2.200111 (2.145345)\nBatch 2100/3125\t Loss 2.446137 (2.143596)\nBatch 2200/3125\t Loss 1.754918 (2.142376)\nBatch 2300/3125\t Loss 1.285600 (2.146867)\nBatch 2400/3125\t Loss 1.737888 (2.141604)\nBatch 2500/3125\t Loss 1.845140 (2.141303)\nBatch 2600/3125\t Loss 2.086272 (2.145829)\nBatch 2700/3125\t Loss 1.360590 (2.145546)\nBatch 2800/3125\t Loss 2.053541 (2.144057)\nBatch 2900/3125\t Loss 3.394546 (2.143368)\nBatch 3000/3125\t Loss 1.951827 (2.140275)\nBatch 3100/3125\t Loss 2.853680 (2.141132)\n==> Test\n       TNR    AUROC  DTACC  AUIN   AUOUT \nBas    17.370 71.928 65.840 73.768 69.126\nAcc: 69.86000\n==> Epoch 26/30\nBatch 100/3125\t Loss 2.175737 (2.152460)\nBatch 200/3125\t Loss 1.487550 (2.149856)\nBatch 300/3125\t Loss 2.728879 (2.114373)\nBatch 400/3125\t Loss 1.912425 (2.122911)\nBatch 500/3125\t Loss 2.182209 (2.127357)\nBatch 600/3125\t Loss 2.901343 (2.135365)\nBatch 700/3125\t Loss 2.629666 (2.142423)\nBatch 800/3125\t Loss 2.171002 (2.134322)\nBatch 900/3125\t Loss 1.864000 (2.132620)\nBatch 1000/3125\t Loss 2.140969 (2.130583)\nBatch 1100/3125\t Loss 2.429086 (2.128759)\nBatch 1200/3125\t Loss 2.503236 (2.133951)\nBatch 1300/3125\t Loss 2.225228 (2.138832)\nBatch 1400/3125\t Loss 2.457202 (2.139559)\nBatch 1500/3125\t Loss 1.888054 (2.142284)\nBatch 1600/3125\t Loss 1.882764 (2.141034)\nBatch 1700/3125\t Loss 2.149488 (2.138359)\nBatch 1800/3125\t Loss 2.231719 (2.139474)\nBatch 1900/3125\t Loss 2.250281 (2.146135)\nBatch 2000/3125\t Loss 2.174731 (2.142100)\nBatch 2100/3125\t Loss 2.367337 (2.147354)\nBatch 2200/3125\t Loss 2.919176 (2.148229)\nBatch 2300/3125\t Loss 2.124878 (2.148131)\nBatch 2400/3125\t Loss 1.420956 (2.146869)\nBatch 2500/3125\t Loss 3.320258 (2.150098)\nBatch 2600/3125\t Loss 1.389901 (2.147968)\nBatch 2700/3125\t Loss 1.440586 (2.149425)\nBatch 2800/3125\t Loss 1.667742 (2.152265)\nBatch 2900/3125\t Loss 2.628455 (2.150927)\nBatch 3000/3125\t Loss 2.410691 (2.151941)\nBatch 3100/3125\t Loss 2.812724 (2.150427)\n==> Epoch 27/30\nBatch 100/3125\t Loss 1.838477 (2.120753)\nBatch 200/3125\t Loss 2.642582 (2.154066)\nBatch 300/3125\t Loss 2.436214 (2.187446)\nBatch 400/3125\t Loss 2.675453 (2.198071)\nBatch 500/3125\t Loss 2.881881 (2.178516)\nBatch 600/3125\t Loss 1.514513 (2.168874)\nBatch 700/3125\t Loss 2.816712 (2.163902)\nBatch 800/3125\t Loss 1.746734 (2.150739)\nBatch 900/3125\t Loss 3.001457 (2.157054)\nBatch 1000/3125\t Loss 2.408858 (2.154678)\nBatch 1100/3125\t Loss 2.490002 (2.156131)\nBatch 1200/3125\t Loss 1.647856 (2.140081)\nBatch 1300/3125\t Loss 1.299230 (2.137417)\nBatch 1400/3125\t Loss 0.553639 (2.137790)\nBatch 1500/3125\t Loss 3.093874 (2.139334)\nBatch 1600/3125\t Loss 1.523806 (2.138545)\nBatch 1700/3125\t Loss 2.644223 (2.142936)\nBatch 1800/3125\t Loss 2.155288 (2.143126)\nBatch 1900/3125\t Loss 3.374738 (2.141321)\nBatch 2000/3125\t Loss 2.504234 (2.144282)\nBatch 2100/3125\t Loss 1.710382 (2.148949)\nBatch 2200/3125\t Loss 2.273855 (2.146744)\nBatch 2300/3125\t Loss 1.881582 (2.150215)\nBatch 2400/3125\t Loss 3.141065 (2.149172)\nBatch 2500/3125\t Loss 3.267483 (2.148800)\nBatch 2600/3125\t Loss 2.340204 (2.150098)\nBatch 2700/3125\t Loss 0.713190 (2.147721)\nBatch 2800/3125\t Loss 2.682554 (2.146014)\nBatch 2900/3125\t Loss 2.509022 (2.145973)\nBatch 3000/3125\t Loss 2.243618 (2.145437)\nBatch 3100/3125\t Loss 1.626961 (2.145777)\n==> Epoch 28/30\nBatch 100/3125\t Loss 1.955513 (2.068667)\nBatch 200/3125\t Loss 1.795888 (2.077395)\nBatch 300/3125\t Loss 2.460425 (2.131652)\nBatch 400/3125\t Loss 2.374922 (2.137032)\nBatch 500/3125\t Loss 2.977987 (2.147157)\nBatch 600/3125\t Loss 1.288130 (2.139995)\nBatch 700/3125\t Loss 1.936860 (2.137969)\nBatch 800/3125\t Loss 1.508832 (2.139004)\nBatch 900/3125\t Loss 2.685817 (2.138352)\nBatch 1000/3125\t Loss 1.662069 (2.138467)\nBatch 1100/3125\t Loss 2.350898 (2.141887)\nBatch 1200/3125\t Loss 2.201218 (2.131131)\nBatch 1300/3125\t Loss 1.958725 (2.131905)\nBatch 1400/3125\t Loss 1.130922 (2.133270)\nBatch 1500/3125\t Loss 2.309218 (2.137297)\nBatch 1600/3125\t Loss 3.906576 (2.134574)\nBatch 1700/3125\t Loss 2.466928 (2.138460)\nBatch 1800/3125\t Loss 2.038372 (2.140865)\nBatch 1900/3125\t Loss 2.354058 (2.144840)\nBatch 2000/3125\t Loss 2.128179 (2.146913)\nBatch 2100/3125\t Loss 1.822803 (2.143413)\nBatch 2200/3125\t Loss 0.815331 (2.145398)\nBatch 2300/3125\t Loss 1.355747 (2.145159)\nBatch 2400/3125\t Loss 2.033563 (2.143598)\nBatch 2500/3125\t Loss 2.187447 (2.146714)\nBatch 2600/3125\t Loss 1.727118 (2.148424)\nBatch 2700/3125\t Loss 2.001775 (2.149047)\nBatch 2800/3125\t Loss 1.966852 (2.148355)\nBatch 2900/3125\t Loss 2.165485 (2.151177)\nBatch 3000/3125\t Loss 3.271541 (2.151873)\nBatch 3100/3125\t Loss 1.466293 (2.155129)\n==> Epoch 29/30\nBatch 100/3125\t Loss 2.269311 (2.124780)\nBatch 200/3125\t Loss 2.822308 (2.145547)\nBatch 300/3125\t Loss 3.309831 (2.154462)\nBatch 400/3125\t Loss 2.249149 (2.152518)\nBatch 500/3125\t Loss 1.878850 (2.153414)\nBatch 600/3125\t Loss 1.603894 (2.157067)\nBatch 700/3125\t Loss 1.874451 (2.176403)\nBatch 800/3125\t Loss 1.011523 (2.172870)\nBatch 900/3125\t Loss 2.393848 (2.163402)\nBatch 1000/3125\t Loss 1.436739 (2.168418)\nBatch 1100/3125\t Loss 2.707468 (2.165630)\nBatch 1200/3125\t Loss 1.209093 (2.170986)\nBatch 1300/3125\t Loss 1.926606 (2.163733)\nBatch 1400/3125\t Loss 1.046842 (2.161717)\nBatch 1500/3125\t Loss 1.434916 (2.160038)\nBatch 1600/3125\t Loss 2.444819 (2.162982)\nBatch 1700/3125\t Loss 2.263825 (2.165542)\nBatch 1800/3125\t Loss 1.552384 (2.165879)\nBatch 1900/3125\t Loss 2.647225 (2.170809)\nBatch 2000/3125\t Loss 2.458208 (2.168923)\nBatch 2100/3125\t Loss 2.811092 (2.170434)\nBatch 2200/3125\t Loss 3.668136 (2.173159)\nBatch 2300/3125\t Loss 2.924856 (2.172367)\nBatch 2400/3125\t Loss 1.174214 (2.174975)\nBatch 2500/3125\t Loss 2.423827 (2.174212)\nBatch 2600/3125\t Loss 3.336492 (2.171000)\nBatch 2700/3125\t Loss 1.361554 (2.171590)\nBatch 2800/3125\t Loss 1.832362 (2.169891)\nBatch 2900/3125\t Loss 2.288002 (2.169927)\nBatch 3000/3125\t Loss 1.631545 (2.168736)\nBatch 3100/3125\t Loss 1.358314 (2.168229)\n==> Epoch 30/30\nBatch 100/3125\t Loss 1.799142 (2.105404)\nBatch 200/3125\t Loss 1.206392 (2.179344)\nBatch 300/3125\t Loss 1.313356 (2.213126)\nBatch 400/3125\t Loss 1.693481 (2.163924)\nBatch 500/3125\t Loss 2.084673 (2.145240)\nBatch 600/3125\t Loss 2.583265 (2.158231)\nBatch 700/3125\t Loss 2.602699 (2.155370)\nBatch 800/3125\t Loss 0.914807 (2.169117)\nBatch 900/3125\t Loss 2.522370 (2.175889)\nBatch 1000/3125\t Loss 1.666243 (2.177273)\nBatch 1100/3125\t Loss 1.472691 (2.166038)\nBatch 1200/3125\t Loss 2.538565 (2.163866)\nBatch 1300/3125\t Loss 2.291114 (2.158218)\nBatch 1400/3125\t Loss 1.658879 (2.153558)\nBatch 1500/3125\t Loss 2.201398 (2.156373)\nBatch 1600/3125\t Loss 3.267065 (2.156980)\nBatch 1700/3125\t Loss 2.210014 (2.158205)\nBatch 1800/3125\t Loss 2.491858 (2.157836)\nBatch 1900/3125\t Loss 2.254130 (2.149981)\nBatch 2000/3125\t Loss 2.801037 (2.153199)\nBatch 2100/3125\t Loss 2.450985 (2.151473)\nBatch 2200/3125\t Loss 1.369109 (2.154164)\nBatch 2300/3125\t Loss 2.548664 (2.158985)\nBatch 2400/3125\t Loss 2.488386 (2.155611)\nBatch 2500/3125\t Loss 1.456980 (2.156292)\nBatch 2600/3125\t Loss 2.079086 (2.157662)\nBatch 2700/3125\t Loss 2.736659 (2.155655)\nBatch 2800/3125\t Loss 2.211969 (2.157156)\nBatch 2900/3125\t Loss 2.564271 (2.158622)\nBatch 3000/3125\t Loss 2.437144 (2.156962)\nBatch 3100/3125\t Loss 0.644128 (2.154653)\n==> Test\n       TNR    AUROC  DTACC  AUIN   AUOUT \nBas    14.730 69.872 64.575 71.635 66.728\nAcc: 65.71000\nFinished. Total elapsed time (h:m:s): 1:23:21\n","output_type":"stream"}]},{"cell_type":"code","source":"if options['eval']:\n        net, criterion = load_networks(net, options['outf'], file_name, criterion=criterion)\n        outloaders = Data.out_loaders\n        results = test(net, criterion, testloader, outloader, epoch=0, **options)\n        acc = results['ACC']\n        res = dict()\n        res['ACC'] = dict()\n        acc_res = []\n        for key in Data.out_keys:\n            results = test_robustness(net, criterion, outloaders[key], epoch=0, label=key, **options)\n            print('{} (%): {:.3f}\\t'.format(key, results['ACC']))\n            res['ACC'][key] = results['ACC']\n            acc_res.append(results['ACC'])\n        print('Mean ACC:', np.mean(acc_res))\n        print('Mean Error:', 100-np.mean(acc_res))","metadata":{"id":"mqmZ2gN1vKcf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4e17f950-e6a9-4e9d-fae6-6604305fb05e","execution":{"iopub.status.busy":"2023-04-24T21:23:59.025357Z","iopub.execute_input":"2023-04-24T21:23:59.026127Z","iopub.status.idle":"2023-04-24T21:33:24.831758Z","shell.execute_reply.started":"2023-04-24T21:23:59.026077Z","shell.execute_reply":"2023-04-24T21:33:24.829592Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"       TNR    AUROC  DTACC  AUIN   AUOUT \nBas    14.730 69.872 64.575 71.635 66.728\nAcc: 65.71000\ngaussian_noise (%): 36.410\t\nshot_noise (%): 41.300\t\nimpulse_noise (%): 36.588\t\ndefocus_blur (%): 62.954\t\nglass_blur (%): 30.180\t\nmotion_blur (%): 60.454\t\nzoom_blur (%): 61.184\t\nsnow (%): 47.986\t\nfrost (%): 47.800\t\nfog (%): 63.774\t\nbrightness (%): 63.072\t\ncontrast (%): 54.340\t\nelastic_transform (%): 59.110\t\npixelate (%): 56.938\t\njpeg_compression (%): 55.724\t\nMean ACC: 51.854266666666675\nMean Error: 48.145733333333325\n","output_type":"stream"}]},{"cell_type":"code","source":"options['aug'] ='aprs'\n\nprint(\"Creating model: {}\".format(options['model']))\nif 'wide_resnet' in options['model']:\n        print('wide_resnet') \n        net = WideResNet(40, num_classes, 2, 0.0)\nelif 'allconv' in options['model']:\n        print('allconv')\n        net = AllConvNet(num_classes)\nelif 'densenet' in options['model']:\n        print('densenet')\n        net = densenet(num_classes=num_classes)\nelif 'resnext' in options['model']:\n        print('resnext29')\n        net = resnext29(num_classes)\nelse:\n        print('resnet18')\n        net = ResNet18(num_classes=num_classes)\n\ntorch.manual_seed(options['seed'])\nos.environ['CUDA_VISIBLE_DEVICES'] = options['gpu']\nuse_gpu = torch.cuda.is_available()\nif options['use_cpu']: use_gpu = False\n\noptions.update({'use_gpu': use_gpu})\noptions['use_gpu']\n\ncriterion = nn.CrossEntropyLoss().cuda()\n\nif use_gpu:\n        net = nn.DataParallel(net, device_ids=[i for i in range(len(options['gpu'].split(',')))]).cuda()\n        criterion = criterion.cuda()\nfile_name = '{}_{}_{}'.format(options['model'], options['dataset'], options['aug'])\nparams_list = [{'params': net.parameters()},\n                {'params': criterion.parameters()}]\noptimizer = torch.optim.SGD(params_list, lr=options['lr'], momentum=0.9, nesterov=True, weight_decay=5e-4)\nscheduler = lr_scheduler.MultiStepLR(optimizer, gamma=0.2, milestones=[60, 120, 160, 190])\n\nstart_time = time.time()\n\nbest_acc = 0.0","metadata":{"id":"tBUBN1zO08tZ","execution":{"iopub.status.busy":"2023-04-24T21:33:24.833759Z","iopub.execute_input":"2023-04-24T21:33:24.834523Z","iopub.status.idle":"2023-04-24T21:33:24.888809Z","shell.execute_reply.started":"2023-04-24T21:33:24.834475Z","shell.execute_reply":"2023-04-24T21:33:24.887621Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Creating model: densenet\ndensenet\n","output_type":"stream"}]},{"cell_type":"code","source":"for epoch in range(options['max_epoch']):\n        print(\"==> Epoch {}/{}\".format(epoch+1, options['max_epoch']))\n\n        train(net, criterion, optimizer, trainloader, epoch=epoch, **options)\n\n        if options['eval_freq'] > 0 and (epoch+1) % options['eval_freq'] == 0 or (epoch+1) == options['max_epoch'] or epoch > 160:\n            print(\"==> Test\")\n            results = test(net, criterion, testloader, outloader, epoch=epoch, **options)\n\n            if best_acc < results['ACC']:\n                best_acc = results['ACC']\n                print(\"Best Acc (%): {:.3f}\\t\".format(best_acc))\n            \n            save_networks(net, options['outf'], file_name, criterion=criterion)\n\n        scheduler.step()\n\nelapsed = round(time.time() - start_time)\nelapsed = str(datetime.timedelta(seconds=elapsed))\nprint(\"Finished. Total elapsed time (h:m:s): {}\".format(elapsed))","metadata":{"id":"jmz0PTNOWowV","execution":{"iopub.status.busy":"2023-04-24T21:33:24.890262Z","iopub.execute_input":"2023-04-24T21:33:24.890726Z","iopub.status.idle":"2023-04-24T22:56:15.666197Z","shell.execute_reply.started":"2023-04-24T21:33:24.890687Z","shell.execute_reply":"2023-04-24T22:56:15.664777Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"==> Epoch 1/30\nBatch 100/3125\t Loss 4.427331 (5.625847)\nBatch 200/3125\t Loss 4.558048 (4.980443)\nBatch 300/3125\t Loss 4.173552 (4.739204)\nBatch 400/3125\t Loss 4.355737 (4.599351)\nBatch 500/3125\t Loss 4.301972 (4.512539)\nBatch 600/3125\t Loss 3.986531 (4.445675)\nBatch 700/3125\t Loss 3.441832 (4.394072)\nBatch 800/3125\t Loss 4.282413 (4.347586)\nBatch 900/3125\t Loss 4.025145 (4.302330)\nBatch 1000/3125\t Loss 3.689342 (4.259738)\nBatch 1100/3125\t Loss 3.747737 (4.228512)\nBatch 1200/3125\t Loss 3.586397 (4.196183)\nBatch 1300/3125\t Loss 3.930526 (4.170571)\nBatch 1400/3125\t Loss 4.533176 (4.147033)\nBatch 1500/3125\t Loss 3.472310 (4.122306)\nBatch 1600/3125\t Loss 4.374207 (4.097488)\nBatch 1700/3125\t Loss 3.907653 (4.079555)\nBatch 1800/3125\t Loss 3.108474 (4.055058)\nBatch 1900/3125\t Loss 3.961716 (4.038743)\nBatch 2000/3125\t Loss 4.145095 (4.021042)\nBatch 2100/3125\t Loss 3.274668 (4.002627)\nBatch 2200/3125\t Loss 3.562547 (3.982782)\nBatch 2300/3125\t Loss 2.777261 (3.963877)\nBatch 2400/3125\t Loss 3.591124 (3.944562)\nBatch 2500/3125\t Loss 3.253794 (3.926672)\nBatch 2600/3125\t Loss 3.932022 (3.906064)\nBatch 2700/3125\t Loss 2.863054 (3.887654)\nBatch 2800/3125\t Loss 3.689794 (3.866574)\nBatch 2900/3125\t Loss 3.222406 (3.842877)\nBatch 3000/3125\t Loss 3.720342 (3.821997)\nBatch 3100/3125\t Loss 3.041316 (3.800310)\n==> Epoch 2/30\nBatch 100/3125\t Loss 3.077044 (3.155477)\nBatch 200/3125\t Loss 2.360734 (3.148299)\nBatch 300/3125\t Loss 3.165900 (3.142251)\nBatch 400/3125\t Loss 3.280619 (3.147985)\nBatch 500/3125\t Loss 3.159737 (3.151590)\nBatch 600/3125\t Loss 3.681694 (3.140595)\nBatch 700/3125\t Loss 2.566683 (3.115897)\nBatch 800/3125\t Loss 1.749849 (3.096571)\nBatch 900/3125\t Loss 2.675952 (3.096888)\nBatch 1000/3125\t Loss 2.808135 (3.076680)\nBatch 1100/3125\t Loss 3.143502 (3.068329)\nBatch 1200/3125\t Loss 2.234493 (3.048075)\nBatch 1300/3125\t Loss 2.997345 (3.032950)\nBatch 1400/3125\t Loss 3.060660 (3.025302)\nBatch 1500/3125\t Loss 2.539257 (3.010739)\nBatch 1600/3125\t Loss 3.245826 (3.001424)\nBatch 1700/3125\t Loss 2.497334 (2.987151)\nBatch 1800/3125\t Loss 3.012859 (2.974325)\nBatch 1900/3125\t Loss 2.572895 (2.961202)\nBatch 2000/3125\t Loss 2.316058 (2.954025)\nBatch 2100/3125\t Loss 3.541224 (2.943640)\nBatch 2200/3125\t Loss 2.363708 (2.935475)\nBatch 2300/3125\t Loss 2.752873 (2.927001)\nBatch 2400/3125\t Loss 1.629457 (2.916978)\nBatch 2500/3125\t Loss 3.145058 (2.909799)\nBatch 2600/3125\t Loss 2.261092 (2.895414)\nBatch 2700/3125\t Loss 2.930251 (2.887174)\nBatch 2800/3125\t Loss 1.967014 (2.874211)\nBatch 2900/3125\t Loss 2.724952 (2.862061)\nBatch 3000/3125\t Loss 2.177974 (2.853341)\nBatch 3100/3125\t Loss 1.911877 (2.841689)\n==> Epoch 3/30\nBatch 100/3125\t Loss 1.867069 (2.604040)\nBatch 200/3125\t Loss 1.643223 (2.567397)\nBatch 300/3125\t Loss 3.565546 (2.586646)\nBatch 400/3125\t Loss 1.803412 (2.593861)\nBatch 500/3125\t Loss 1.540667 (2.580996)\nBatch 600/3125\t Loss 1.684193 (2.566709)\nBatch 700/3125\t Loss 3.484939 (2.563224)\nBatch 800/3125\t Loss 2.448257 (2.556306)\nBatch 900/3125\t Loss 1.628304 (2.558621)\nBatch 1000/3125\t Loss 2.134441 (2.559253)\nBatch 1100/3125\t Loss 2.223139 (2.550209)\nBatch 1200/3125\t Loss 3.200482 (2.540385)\nBatch 1300/3125\t Loss 2.389017 (2.536270)\nBatch 1400/3125\t Loss 2.274923 (2.532358)\nBatch 1500/3125\t Loss 3.277151 (2.528446)\nBatch 1600/3125\t Loss 3.441075 (2.530488)\nBatch 1700/3125\t Loss 2.129811 (2.524913)\nBatch 1800/3125\t Loss 2.918706 (2.523161)\nBatch 1900/3125\t Loss 2.142190 (2.521260)\nBatch 2000/3125\t Loss 2.058250 (2.515636)\nBatch 2100/3125\t Loss 2.859845 (2.514199)\nBatch 2200/3125\t Loss 2.898382 (2.505768)\nBatch 2300/3125\t Loss 1.969130 (2.502256)\nBatch 2400/3125\t Loss 2.985569 (2.498461)\nBatch 2500/3125\t Loss 2.593505 (2.496559)\nBatch 2600/3125\t Loss 2.721024 (2.494969)\nBatch 2700/3125\t Loss 1.581861 (2.494614)\nBatch 2800/3125\t Loss 2.145727 (2.494367)\nBatch 2900/3125\t Loss 2.796953 (2.490797)\nBatch 3000/3125\t Loss 2.304638 (2.492008)\nBatch 3100/3125\t Loss 1.659529 (2.486625)\n==> Epoch 4/30\nBatch 100/3125\t Loss 2.989712 (2.490219)\nBatch 200/3125\t Loss 2.562405 (2.399772)\nBatch 300/3125\t Loss 3.436040 (2.391076)\nBatch 400/3125\t Loss 1.812135 (2.366148)\nBatch 500/3125\t Loss 2.957819 (2.367592)\nBatch 600/3125\t Loss 2.955693 (2.369065)\nBatch 700/3125\t Loss 3.007590 (2.356608)\nBatch 800/3125\t Loss 0.945997 (2.355292)\nBatch 900/3125\t Loss 2.368655 (2.367081)\nBatch 1000/3125\t Loss 1.885887 (2.367189)\nBatch 1100/3125\t Loss 2.439135 (2.363807)\nBatch 1200/3125\t Loss 1.481529 (2.369154)\nBatch 1300/3125\t Loss 4.449090 (2.367006)\nBatch 1400/3125\t Loss 2.945256 (2.365017)\nBatch 1500/3125\t Loss 1.947778 (2.361380)\nBatch 1600/3125\t Loss 1.883121 (2.360375)\nBatch 1700/3125\t Loss 2.531722 (2.360686)\nBatch 1800/3125\t Loss 2.692575 (2.357701)\nBatch 1900/3125\t Loss 1.877200 (2.355768)\nBatch 2000/3125\t Loss 2.591211 (2.351070)\nBatch 2100/3125\t Loss 2.271368 (2.351165)\nBatch 2200/3125\t Loss 2.336694 (2.351641)\nBatch 2300/3125\t Loss 3.282239 (2.352679)\nBatch 2400/3125\t Loss 1.797859 (2.349215)\nBatch 2500/3125\t Loss 3.163213 (2.345928)\nBatch 2600/3125\t Loss 2.530987 (2.347171)\nBatch 2700/3125\t Loss 2.714478 (2.347271)\nBatch 2800/3125\t Loss 1.987710 (2.348726)\nBatch 2900/3125\t Loss 3.538729 (2.346920)\nBatch 3000/3125\t Loss 3.025213 (2.342690)\nBatch 3100/3125\t Loss 1.613783 (2.340919)\n==> Epoch 5/30\nBatch 100/3125\t Loss 2.104766 (2.358258)\nBatch 200/3125\t Loss 2.027551 (2.293672)\nBatch 300/3125\t Loss 2.741400 (2.300304)\nBatch 400/3125\t Loss 3.237382 (2.323922)\nBatch 500/3125\t Loss 2.055388 (2.334263)\nBatch 600/3125\t Loss 3.961501 (2.329806)\nBatch 700/3125\t Loss 2.002162 (2.331874)\nBatch 800/3125\t Loss 1.004538 (2.317905)\nBatch 900/3125\t Loss 1.471163 (2.314410)\nBatch 1000/3125\t Loss 2.901918 (2.306185)\nBatch 1100/3125\t Loss 1.673455 (2.299188)\nBatch 1200/3125\t Loss 1.483941 (2.297884)\nBatch 1300/3125\t Loss 2.528801 (2.303268)\nBatch 1400/3125\t Loss 2.682998 (2.301531)\nBatch 1500/3125\t Loss 3.366052 (2.304161)\nBatch 1600/3125\t Loss 2.151650 (2.306762)\nBatch 1700/3125\t Loss 1.452961 (2.303569)\nBatch 1800/3125\t Loss 2.627550 (2.306151)\nBatch 1900/3125\t Loss 3.324586 (2.309625)\nBatch 2000/3125\t Loss 1.228838 (2.303121)\nBatch 2100/3125\t Loss 2.631382 (2.303727)\nBatch 2200/3125\t Loss 2.596441 (2.300546)\nBatch 2300/3125\t Loss 2.024810 (2.300811)\nBatch 2400/3125\t Loss 2.976339 (2.300290)\nBatch 2500/3125\t Loss 2.764287 (2.300821)\nBatch 2600/3125\t Loss 2.219127 (2.301880)\nBatch 2700/3125\t Loss 2.623963 (2.301823)\nBatch 2800/3125\t Loss 2.122263 (2.302662)\nBatch 2900/3125\t Loss 2.236566 (2.299390)\nBatch 3000/3125\t Loss 2.170323 (2.299776)\nBatch 3100/3125\t Loss 1.019445 (2.298045)\n==> Test\n       TNR    AUROC  DTACC  AUIN   AUOUT \nBas    15.310 72.300 66.830 73.762 68.672\nAcc: 68.67000\nBest Acc (%): 68.670\t\n==> Epoch 6/30\nBatch 100/3125\t Loss 1.442063 (2.215499)\nBatch 200/3125\t Loss 3.150483 (2.273268)\nBatch 300/3125\t Loss 1.959167 (2.299745)\nBatch 400/3125\t Loss 2.261827 (2.270508)\nBatch 500/3125\t Loss 1.282038 (2.273002)\nBatch 600/3125\t Loss 3.009762 (2.266679)\nBatch 700/3125\t Loss 2.031547 (2.254486)\nBatch 800/3125\t Loss 1.909319 (2.262721)\nBatch 900/3125\t Loss 2.317028 (2.255913)\nBatch 1000/3125\t Loss 2.086901 (2.255171)\nBatch 1100/3125\t Loss 1.649494 (2.257924)\nBatch 1200/3125\t Loss 3.210096 (2.258849)\nBatch 1300/3125\t Loss 2.003964 (2.265644)\nBatch 1400/3125\t Loss 2.082067 (2.271475)\nBatch 1500/3125\t Loss 4.517092 (2.268720)\nBatch 1600/3125\t Loss 1.429458 (2.276417)\nBatch 1700/3125\t Loss 2.078177 (2.267836)\nBatch 1800/3125\t Loss 3.142776 (2.271210)\nBatch 1900/3125\t Loss 1.357608 (2.268368)\nBatch 2000/3125\t Loss 2.710967 (2.261874)\nBatch 2100/3125\t Loss 2.561347 (2.260540)\nBatch 2200/3125\t Loss 1.502926 (2.259846)\nBatch 2300/3125\t Loss 2.272159 (2.261193)\nBatch 2400/3125\t Loss 2.311464 (2.265557)\nBatch 2500/3125\t Loss 1.754729 (2.265555)\nBatch 2600/3125\t Loss 3.041741 (2.260148)\nBatch 2700/3125\t Loss 2.265890 (2.260213)\nBatch 2800/3125\t Loss 1.478442 (2.259651)\nBatch 2900/3125\t Loss 2.968085 (2.258122)\nBatch 3000/3125\t Loss 3.136910 (2.259297)\nBatch 3100/3125\t Loss 2.618453 (2.256805)\n==> Epoch 7/30\nBatch 100/3125\t Loss 2.415430 (2.247893)\nBatch 200/3125\t Loss 2.684443 (2.278575)\nBatch 300/3125\t Loss 3.627352 (2.275202)\nBatch 400/3125\t Loss 2.061071 (2.255027)\nBatch 500/3125\t Loss 2.763797 (2.266533)\nBatch 600/3125\t Loss 3.237677 (2.256906)\nBatch 700/3125\t Loss 1.761961 (2.263222)\nBatch 800/3125\t Loss 1.752025 (2.257277)\nBatch 900/3125\t Loss 2.872624 (2.258891)\nBatch 1000/3125\t Loss 2.599583 (2.255597)\nBatch 1100/3125\t Loss 1.786021 (2.253202)\nBatch 1200/3125\t Loss 3.384354 (2.251541)\nBatch 1300/3125\t Loss 2.680310 (2.252660)\nBatch 1400/3125\t Loss 1.076366 (2.251641)\nBatch 1500/3125\t Loss 3.602631 (2.252562)\nBatch 1600/3125\t Loss 2.360345 (2.248518)\nBatch 1700/3125\t Loss 2.520512 (2.245374)\nBatch 1800/3125\t Loss 2.786336 (2.248135)\nBatch 1900/3125\t Loss 2.502563 (2.245659)\nBatch 2000/3125\t Loss 3.216571 (2.246451)\nBatch 2100/3125\t Loss 2.334516 (2.242752)\nBatch 2200/3125\t Loss 2.230492 (2.244346)\nBatch 2300/3125\t Loss 1.437849 (2.245751)\nBatch 2400/3125\t Loss 2.378473 (2.245618)\nBatch 2500/3125\t Loss 3.318385 (2.245377)\nBatch 2600/3125\t Loss 1.843655 (2.243124)\nBatch 2700/3125\t Loss 2.859877 (2.242907)\nBatch 2800/3125\t Loss 2.265165 (2.245427)\nBatch 2900/3125\t Loss 1.818378 (2.246789)\nBatch 3000/3125\t Loss 2.056606 (2.245257)\nBatch 3100/3125\t Loss 2.000331 (2.243553)\n==> Epoch 8/30\nBatch 100/3125\t Loss 2.640341 (2.317966)\nBatch 200/3125\t Loss 2.325401 (2.256783)\nBatch 300/3125\t Loss 1.541338 (2.267410)\nBatch 400/3125\t Loss 2.738460 (2.268598)\nBatch 500/3125\t Loss 1.225297 (2.264016)\nBatch 600/3125\t Loss 1.341342 (2.254183)\nBatch 700/3125\t Loss 2.011700 (2.267286)\nBatch 800/3125\t Loss 1.412685 (2.257941)\nBatch 900/3125\t Loss 3.605690 (2.246537)\nBatch 1000/3125\t Loss 3.382392 (2.268573)\nBatch 1100/3125\t Loss 1.797855 (2.255580)\nBatch 1200/3125\t Loss 2.693903 (2.246910)\nBatch 1300/3125\t Loss 2.406729 (2.249004)\nBatch 1400/3125\t Loss 1.772822 (2.249591)\nBatch 1500/3125\t Loss 2.510197 (2.250470)\nBatch 1600/3125\t Loss 2.189342 (2.247240)\nBatch 1700/3125\t Loss 2.465151 (2.249041)\nBatch 1800/3125\t Loss 2.241115 (2.245686)\nBatch 1900/3125\t Loss 1.766191 (2.246244)\nBatch 2000/3125\t Loss 1.992494 (2.241508)\nBatch 2100/3125\t Loss 2.651063 (2.244152)\nBatch 2200/3125\t Loss 2.451787 (2.244327)\nBatch 2300/3125\t Loss 1.820959 (2.246617)\nBatch 2400/3125\t Loss 2.424455 (2.249625)\nBatch 2500/3125\t Loss 3.053169 (2.249626)\nBatch 2600/3125\t Loss 1.100443 (2.247319)\nBatch 2700/3125\t Loss 1.100450 (2.248444)\nBatch 2800/3125\t Loss 3.004069 (2.242469)\nBatch 2900/3125\t Loss 2.358388 (2.241253)\nBatch 3000/3125\t Loss 1.788584 (2.242920)\nBatch 3100/3125\t Loss 2.433864 (2.241451)\n==> Epoch 9/30\nBatch 100/3125\t Loss 2.045088 (2.166566)\nBatch 200/3125\t Loss 1.541807 (2.185102)\nBatch 300/3125\t Loss 2.367431 (2.197838)\nBatch 400/3125\t Loss 2.310875 (2.210389)\nBatch 500/3125\t Loss 2.266179 (2.207895)\nBatch 600/3125\t Loss 2.459990 (2.217640)\nBatch 700/3125\t Loss 2.473403 (2.220479)\nBatch 800/3125\t Loss 2.082028 (2.226772)\nBatch 900/3125\t Loss 2.167610 (2.224501)\nBatch 1000/3125\t Loss 1.935010 (2.231681)\nBatch 1100/3125\t Loss 2.434668 (2.228179)\nBatch 1200/3125\t Loss 2.122885 (2.228665)\nBatch 1300/3125\t Loss 2.138421 (2.223166)\nBatch 1400/3125\t Loss 2.207016 (2.221382)\nBatch 1500/3125\t Loss 2.541917 (2.222860)\nBatch 1600/3125\t Loss 1.723383 (2.229136)\nBatch 1700/3125\t Loss 3.896297 (2.232587)\nBatch 1800/3125\t Loss 1.821336 (2.227626)\nBatch 1900/3125\t Loss 2.414894 (2.233260)\nBatch 2000/3125\t Loss 2.946335 (2.236013)\nBatch 2100/3125\t Loss 2.779784 (2.235023)\nBatch 2200/3125\t Loss 2.479510 (2.237255)\nBatch 2300/3125\t Loss 1.918576 (2.236590)\nBatch 2400/3125\t Loss 1.430698 (2.238132)\nBatch 2500/3125\t Loss 1.253112 (2.236265)\nBatch 2600/3125\t Loss 2.389762 (2.235832)\nBatch 2700/3125\t Loss 1.944219 (2.236434)\nBatch 2800/3125\t Loss 3.440053 (2.236761)\nBatch 2900/3125\t Loss 2.411473 (2.237486)\nBatch 3000/3125\t Loss 1.948084 (2.237638)\nBatch 3100/3125\t Loss 2.971502 (2.235025)\n==> Epoch 10/30\nBatch 100/3125\t Loss 2.091574 (2.190258)\nBatch 200/3125\t Loss 1.611741 (2.181983)\nBatch 300/3125\t Loss 2.349249 (2.190589)\nBatch 400/3125\t Loss 3.020499 (2.194778)\nBatch 500/3125\t Loss 1.845335 (2.198725)\nBatch 600/3125\t Loss 1.724184 (2.208390)\nBatch 700/3125\t Loss 1.748282 (2.209675)\nBatch 800/3125\t Loss 1.935456 (2.194724)\nBatch 900/3125\t Loss 2.167733 (2.192132)\nBatch 1000/3125\t Loss 2.189136 (2.197226)\nBatch 1100/3125\t Loss 1.613538 (2.199262)\nBatch 1200/3125\t Loss 1.726396 (2.196654)\nBatch 1300/3125\t Loss 3.262878 (2.193768)\nBatch 1400/3125\t Loss 1.286725 (2.197494)\nBatch 1500/3125\t Loss 1.602231 (2.195964)\nBatch 1600/3125\t Loss 2.447664 (2.200351)\nBatch 1700/3125\t Loss 2.718869 (2.203991)\nBatch 1800/3125\t Loss 2.253673 (2.199123)\nBatch 1900/3125\t Loss 1.023202 (2.195595)\nBatch 2000/3125\t Loss 3.440189 (2.193846)\nBatch 2100/3125\t Loss 1.246892 (2.192265)\nBatch 2200/3125\t Loss 2.377440 (2.193170)\nBatch 2300/3125\t Loss 2.208742 (2.196607)\nBatch 2400/3125\t Loss 1.795889 (2.193415)\nBatch 2500/3125\t Loss 2.007947 (2.190737)\nBatch 2600/3125\t Loss 2.185319 (2.190324)\nBatch 2700/3125\t Loss 2.709394 (2.190848)\nBatch 2800/3125\t Loss 2.558516 (2.192749)\nBatch 2900/3125\t Loss 2.336334 (2.196123)\nBatch 3000/3125\t Loss 1.718049 (2.196209)\nBatch 3100/3125\t Loss 2.214939 (2.198365)\n==> Test\n       TNR    AUROC  DTACC  AUIN   AUOUT \nBas    15.590 72.600 66.725 74.466 68.887\nAcc: 70.21000\nBest Acc (%): 70.210\t\n==> Epoch 11/30\nBatch 100/3125\t Loss 2.227107 (2.227088)\nBatch 200/3125\t Loss 1.686904 (2.241528)\nBatch 300/3125\t Loss 2.398407 (2.231393)\nBatch 400/3125\t Loss 2.128174 (2.221139)\nBatch 500/3125\t Loss 2.106175 (2.239102)\nBatch 600/3125\t Loss 2.244339 (2.216236)\nBatch 700/3125\t Loss 1.608308 (2.215807)\nBatch 800/3125\t Loss 1.577020 (2.213137)\nBatch 900/3125\t Loss 2.437232 (2.213628)\nBatch 1000/3125\t Loss 2.512837 (2.210974)\nBatch 1100/3125\t Loss 1.083617 (2.210505)\nBatch 1200/3125\t Loss 1.864989 (2.214035)\nBatch 1300/3125\t Loss 2.610106 (2.216341)\nBatch 1400/3125\t Loss 2.601041 (2.216201)\nBatch 1500/3125\t Loss 2.590628 (2.219151)\nBatch 1600/3125\t Loss 1.985593 (2.216075)\nBatch 1700/3125\t Loss 2.327238 (2.215100)\nBatch 1800/3125\t Loss 2.039001 (2.213465)\nBatch 1900/3125\t Loss 3.319599 (2.210895)\nBatch 2000/3125\t Loss 1.796207 (2.217750)\nBatch 2100/3125\t Loss 2.387736 (2.213293)\nBatch 2200/3125\t Loss 3.546247 (2.217140)\nBatch 2300/3125\t Loss 2.597753 (2.212900)\nBatch 2400/3125\t Loss 3.205941 (2.209845)\nBatch 2500/3125\t Loss 1.542922 (2.209729)\nBatch 2600/3125\t Loss 2.248836 (2.210845)\nBatch 2700/3125\t Loss 1.063609 (2.210472)\nBatch 2800/3125\t Loss 2.734218 (2.211548)\nBatch 2900/3125\t Loss 2.192566 (2.209334)\nBatch 3000/3125\t Loss 2.244889 (2.210604)\nBatch 3100/3125\t Loss 2.450631 (2.210669)\n==> Epoch 12/30\nBatch 100/3125\t Loss 2.060240 (2.199123)\nBatch 200/3125\t Loss 2.573243 (2.203162)\nBatch 300/3125\t Loss 2.530280 (2.182912)\nBatch 400/3125\t Loss 2.833431 (2.167363)\nBatch 500/3125\t Loss 1.864925 (2.181525)\nBatch 600/3125\t Loss 2.466161 (2.205600)\nBatch 700/3125\t Loss 2.733457 (2.208693)\nBatch 800/3125\t Loss 1.746451 (2.205820)\nBatch 900/3125\t Loss 1.528557 (2.207005)\nBatch 1000/3125\t Loss 1.149999 (2.207951)\nBatch 1100/3125\t Loss 2.385844 (2.202304)\nBatch 1200/3125\t Loss 3.265101 (2.196967)\nBatch 1300/3125\t Loss 2.780242 (2.192756)\nBatch 1400/3125\t Loss 2.687153 (2.193384)\nBatch 1500/3125\t Loss 1.740048 (2.196669)\nBatch 1600/3125\t Loss 2.193022 (2.200033)\nBatch 1700/3125\t Loss 0.855525 (2.201083)\nBatch 1800/3125\t Loss 2.240770 (2.210304)\nBatch 1900/3125\t Loss 2.868299 (2.212500)\nBatch 2000/3125\t Loss 2.877226 (2.212386)\nBatch 2100/3125\t Loss 2.041259 (2.210922)\nBatch 2200/3125\t Loss 1.791355 (2.209198)\nBatch 2300/3125\t Loss 2.984085 (2.209169)\nBatch 2400/3125\t Loss 1.941582 (2.207249)\nBatch 2500/3125\t Loss 1.691798 (2.209669)\nBatch 2600/3125\t Loss 1.804170 (2.208968)\nBatch 2700/3125\t Loss 3.381182 (2.203182)\nBatch 2800/3125\t Loss 2.330623 (2.203047)\nBatch 2900/3125\t Loss 2.336226 (2.203636)\nBatch 3000/3125\t Loss 2.139906 (2.206145)\nBatch 3100/3125\t Loss 1.981879 (2.202730)\n==> Epoch 13/30\nBatch 100/3125\t Loss 1.782793 (2.285411)\nBatch 200/3125\t Loss 1.895609 (2.257398)\nBatch 300/3125\t Loss 2.366011 (2.248112)\nBatch 400/3125\t Loss 3.594728 (2.258340)\nBatch 500/3125\t Loss 1.068140 (2.234436)\nBatch 600/3125\t Loss 2.671661 (2.236495)\nBatch 700/3125\t Loss 2.905206 (2.249044)\nBatch 800/3125\t Loss 1.657674 (2.235092)\nBatch 900/3125\t Loss 1.410631 (2.231121)\nBatch 1000/3125\t Loss 1.507512 (2.225913)\nBatch 1100/3125\t Loss 2.234228 (2.222295)\nBatch 1200/3125\t Loss 3.351427 (2.220608)\nBatch 1300/3125\t Loss 3.288087 (2.213681)\nBatch 1400/3125\t Loss 2.567617 (2.219405)\nBatch 1500/3125\t Loss 2.056934 (2.209236)\nBatch 1600/3125\t Loss 1.845406 (2.209097)\nBatch 1700/3125\t Loss 1.894131 (2.207703)\nBatch 1800/3125\t Loss 1.897398 (2.203082)\nBatch 1900/3125\t Loss 1.762949 (2.208202)\nBatch 2000/3125\t Loss 1.427397 (2.203864)\nBatch 2100/3125\t Loss 1.411829 (2.204613)\nBatch 2200/3125\t Loss 1.083708 (2.204684)\nBatch 2300/3125\t Loss 2.372124 (2.204936)\nBatch 2400/3125\t Loss 1.944469 (2.207305)\nBatch 2500/3125\t Loss 2.552312 (2.206306)\nBatch 2600/3125\t Loss 2.963056 (2.204704)\nBatch 2700/3125\t Loss 2.768220 (2.204020)\nBatch 2800/3125\t Loss 2.199097 (2.206106)\nBatch 2900/3125\t Loss 2.669565 (2.205430)\nBatch 3000/3125\t Loss 2.187672 (2.205475)\nBatch 3100/3125\t Loss 1.983569 (2.205912)\n==> Epoch 14/30\nBatch 100/3125\t Loss 3.380295 (2.246777)\nBatch 200/3125\t Loss 2.228847 (2.267269)\nBatch 300/3125\t Loss 1.925598 (2.218591)\nBatch 400/3125\t Loss 2.178298 (2.201264)\nBatch 500/3125\t Loss 2.538093 (2.184036)\nBatch 600/3125\t Loss 1.728257 (2.185599)\nBatch 700/3125\t Loss 2.885093 (2.190162)\nBatch 800/3125\t Loss 3.346192 (2.181642)\nBatch 900/3125\t Loss 3.234045 (2.181382)\nBatch 1000/3125\t Loss 2.406333 (2.176934)\nBatch 1100/3125\t Loss 1.264248 (2.175061)\nBatch 1200/3125\t Loss 2.830079 (2.179222)\nBatch 1300/3125\t Loss 1.221907 (2.186706)\nBatch 1400/3125\t Loss 2.895670 (2.190967)\nBatch 1500/3125\t Loss 2.252972 (2.192922)\nBatch 1600/3125\t Loss 1.911918 (2.192270)\nBatch 1700/3125\t Loss 1.507880 (2.191836)\nBatch 1800/3125\t Loss 1.614987 (2.195167)\nBatch 1900/3125\t Loss 2.448373 (2.191535)\nBatch 2000/3125\t Loss 2.021904 (2.193707)\nBatch 2100/3125\t Loss 1.658385 (2.192540)\nBatch 2200/3125\t Loss 2.277410 (2.189686)\nBatch 2300/3125\t Loss 1.339182 (2.186155)\nBatch 2400/3125\t Loss 2.171359 (2.186415)\nBatch 2500/3125\t Loss 1.492785 (2.190025)\nBatch 2600/3125\t Loss 2.293723 (2.188482)\nBatch 2700/3125\t Loss 3.387330 (2.187907)\nBatch 2800/3125\t Loss 3.198558 (2.189610)\nBatch 2900/3125\t Loss 1.390252 (2.189708)\nBatch 3000/3125\t Loss 2.357872 (2.194204)\nBatch 3100/3125\t Loss 2.036298 (2.195811)\n==> Epoch 15/30\nBatch 100/3125\t Loss 2.308547 (2.247581)\nBatch 200/3125\t Loss 2.004330 (2.212674)\nBatch 300/3125\t Loss 2.062075 (2.188197)\nBatch 400/3125\t Loss 2.088104 (2.207242)\nBatch 500/3125\t Loss 1.552329 (2.208550)\nBatch 600/3125\t Loss 1.355680 (2.204587)\nBatch 700/3125\t Loss 1.810219 (2.208538)\nBatch 800/3125\t Loss 3.484996 (2.205109)\nBatch 900/3125\t Loss 1.390319 (2.214277)\nBatch 1000/3125\t Loss 2.514129 (2.217284)\nBatch 1100/3125\t Loss 2.025227 (2.211609)\nBatch 1200/3125\t Loss 2.069877 (2.204490)\nBatch 1300/3125\t Loss 2.258082 (2.203901)\nBatch 1400/3125\t Loss 1.901838 (2.209242)\nBatch 1500/3125\t Loss 2.314934 (2.206511)\nBatch 1600/3125\t Loss 1.968095 (2.196577)\nBatch 1700/3125\t Loss 1.916585 (2.200114)\nBatch 1800/3125\t Loss 2.219226 (2.203324)\nBatch 1900/3125\t Loss 1.176936 (2.202348)\nBatch 2000/3125\t Loss 1.431139 (2.201110)\nBatch 2100/3125\t Loss 1.712816 (2.201913)\nBatch 2200/3125\t Loss 1.584496 (2.202397)\nBatch 2300/3125\t Loss 2.648369 (2.196792)\nBatch 2400/3125\t Loss 2.158447 (2.197937)\nBatch 2500/3125\t Loss 3.063717 (2.194029)\nBatch 2600/3125\t Loss 2.479070 (2.195819)\nBatch 2700/3125\t Loss 1.722435 (2.196064)\nBatch 2800/3125\t Loss 2.806261 (2.196415)\nBatch 2900/3125\t Loss 2.323862 (2.196100)\nBatch 3000/3125\t Loss 2.877502 (2.194901)\nBatch 3100/3125\t Loss 1.603378 (2.195878)\n==> Test\n       TNR    AUROC  DTACC  AUIN   AUOUT \nBas    16.850 68.241 62.750 68.464 67.382\nAcc: 54.85000\n==> Epoch 16/30\nBatch 100/3125\t Loss 2.209037 (2.147257)\nBatch 200/3125\t Loss 2.441937 (2.207576)\nBatch 300/3125\t Loss 2.731819 (2.204756)\nBatch 400/3125\t Loss 1.488651 (2.196881)\nBatch 500/3125\t Loss 2.386608 (2.193034)\nBatch 600/3125\t Loss 2.225672 (2.206566)\nBatch 700/3125\t Loss 2.401364 (2.199127)\nBatch 800/3125\t Loss 1.258180 (2.191564)\nBatch 900/3125\t Loss 1.534625 (2.191383)\nBatch 1000/3125\t Loss 2.410585 (2.195718)\nBatch 1100/3125\t Loss 2.565632 (2.192531)\nBatch 1200/3125\t Loss 2.923336 (2.192855)\nBatch 1300/3125\t Loss 1.792827 (2.196475)\nBatch 1400/3125\t Loss 2.212351 (2.196933)\nBatch 1500/3125\t Loss 2.146918 (2.196835)\nBatch 1600/3125\t Loss 2.893125 (2.200852)\nBatch 1700/3125\t Loss 2.120663 (2.202206)\nBatch 1800/3125\t Loss 2.842702 (2.202859)\nBatch 1900/3125\t Loss 2.987292 (2.198700)\nBatch 2000/3125\t Loss 1.794680 (2.198821)\nBatch 2100/3125\t Loss 1.882688 (2.199662)\nBatch 2200/3125\t Loss 3.111677 (2.199253)\nBatch 2300/3125\t Loss 2.203983 (2.199166)\nBatch 2400/3125\t Loss 2.186970 (2.196519)\nBatch 2500/3125\t Loss 3.021321 (2.196585)\nBatch 2600/3125\t Loss 1.088719 (2.196218)\nBatch 2700/3125\t Loss 2.710561 (2.198498)\nBatch 2800/3125\t Loss 2.599154 (2.194715)\nBatch 2900/3125\t Loss 2.059316 (2.193031)\nBatch 3000/3125\t Loss 1.997275 (2.193118)\nBatch 3100/3125\t Loss 1.883349 (2.194213)\n==> Epoch 17/30\nBatch 100/3125\t Loss 2.214100 (2.197078)\nBatch 200/3125\t Loss 2.598427 (2.177593)\nBatch 300/3125\t Loss 2.421071 (2.205114)\nBatch 400/3125\t Loss 2.106512 (2.197428)\nBatch 500/3125\t Loss 2.895425 (2.186325)\nBatch 600/3125\t Loss 1.434259 (2.189195)\nBatch 700/3125\t Loss 1.870897 (2.194239)\nBatch 800/3125\t Loss 2.552462 (2.191961)\nBatch 900/3125\t Loss 3.252077 (2.194887)\nBatch 1000/3125\t Loss 3.726758 (2.199006)\nBatch 1100/3125\t Loss 2.307947 (2.194298)\nBatch 1200/3125\t Loss 3.486497 (2.196384)\nBatch 1300/3125\t Loss 2.246762 (2.189716)\nBatch 1400/3125\t Loss 2.390954 (2.188599)\nBatch 1500/3125\t Loss 2.091780 (2.188633)\nBatch 1600/3125\t Loss 3.031652 (2.191830)\nBatch 1700/3125\t Loss 2.200751 (2.192985)\nBatch 1800/3125\t Loss 3.685779 (2.196705)\nBatch 1900/3125\t Loss 2.291629 (2.187785)\nBatch 2000/3125\t Loss 2.599041 (2.185970)\nBatch 2100/3125\t Loss 2.622998 (2.180856)\nBatch 2200/3125\t Loss 3.155315 (2.184535)\nBatch 2300/3125\t Loss 3.260800 (2.183136)\nBatch 2400/3125\t Loss 1.776783 (2.184243)\nBatch 2500/3125\t Loss 2.008383 (2.182942)\nBatch 2600/3125\t Loss 1.615116 (2.182648)\nBatch 2700/3125\t Loss 1.816284 (2.182717)\nBatch 2800/3125\t Loss 1.572793 (2.183822)\nBatch 2900/3125\t Loss 3.226545 (2.182506)\nBatch 3000/3125\t Loss 2.264385 (2.179755)\nBatch 3100/3125\t Loss 2.047794 (2.180075)\n==> Epoch 18/30\nBatch 100/3125\t Loss 2.799589 (2.218010)\nBatch 200/3125\t Loss 1.905367 (2.225364)\nBatch 300/3125\t Loss 1.907148 (2.214667)\nBatch 400/3125\t Loss 2.190703 (2.240182)\nBatch 500/3125\t Loss 1.420863 (2.239103)\nBatch 600/3125\t Loss 0.861155 (2.232467)\nBatch 700/3125\t Loss 2.161075 (2.223272)\nBatch 800/3125\t Loss 1.662723 (2.212824)\nBatch 900/3125\t Loss 1.782923 (2.203335)\nBatch 1000/3125\t Loss 2.197710 (2.208601)\nBatch 1100/3125\t Loss 1.959651 (2.222550)\nBatch 1200/3125\t Loss 1.934467 (2.218601)\nBatch 1300/3125\t Loss 2.444812 (2.209218)\nBatch 1400/3125\t Loss 2.568847 (2.211500)\nBatch 1500/3125\t Loss 1.776900 (2.211371)\nBatch 1600/3125\t Loss 2.114910 (2.209407)\nBatch 1700/3125\t Loss 2.145181 (2.207611)\nBatch 1800/3125\t Loss 2.227189 (2.213090)\nBatch 1900/3125\t Loss 2.393119 (2.210641)\nBatch 2000/3125\t Loss 2.494667 (2.212736)\nBatch 2100/3125\t Loss 3.098593 (2.209218)\nBatch 2200/3125\t Loss 2.444553 (2.208452)\nBatch 2300/3125\t Loss 2.088074 (2.203992)\nBatch 2400/3125\t Loss 3.063087 (2.206689)\nBatch 2500/3125\t Loss 1.513622 (2.209716)\nBatch 2600/3125\t Loss 2.971198 (2.208709)\nBatch 2700/3125\t Loss 1.956226 (2.207446)\nBatch 2800/3125\t Loss 1.763067 (2.208051)\nBatch 2900/3125\t Loss 1.755642 (2.208550)\nBatch 3000/3125\t Loss 2.265325 (2.208728)\nBatch 3100/3125\t Loss 2.796961 (2.210901)\n==> Epoch 19/30\nBatch 100/3125\t Loss 1.972849 (2.133125)\nBatch 200/3125\t Loss 2.429357 (2.186273)\nBatch 300/3125\t Loss 2.405323 (2.187435)\nBatch 400/3125\t Loss 1.520887 (2.187315)\nBatch 500/3125\t Loss 1.924974 (2.183615)\nBatch 600/3125\t Loss 2.814456 (2.196206)\nBatch 700/3125\t Loss 1.830268 (2.187112)\nBatch 800/3125\t Loss 1.932793 (2.180826)\nBatch 900/3125\t Loss 3.107325 (2.176601)\nBatch 1000/3125\t Loss 1.987778 (2.172607)\nBatch 1100/3125\t Loss 1.609165 (2.171608)\nBatch 1200/3125\t Loss 2.375609 (2.176093)\nBatch 1300/3125\t Loss 1.566074 (2.183037)\nBatch 1400/3125\t Loss 2.011908 (2.183271)\nBatch 1500/3125\t Loss 3.021908 (2.190664)\nBatch 1600/3125\t Loss 2.116663 (2.183894)\nBatch 1700/3125\t Loss 1.348680 (2.184491)\nBatch 1800/3125\t Loss 2.148271 (2.183762)\nBatch 1900/3125\t Loss 2.481008 (2.182708)\nBatch 2000/3125\t Loss 1.928223 (2.177799)\nBatch 2100/3125\t Loss 1.865321 (2.176064)\nBatch 2200/3125\t Loss 1.748370 (2.182733)\nBatch 2300/3125\t Loss 1.623116 (2.181417)\nBatch 2400/3125\t Loss 2.024200 (2.187219)\nBatch 2500/3125\t Loss 1.170750 (2.189092)\nBatch 2600/3125\t Loss 1.592221 (2.190456)\nBatch 2700/3125\t Loss 3.073093 (2.191980)\nBatch 2800/3125\t Loss 1.958895 (2.191864)\nBatch 2900/3125\t Loss 1.327256 (2.190308)\nBatch 3000/3125\t Loss 2.521451 (2.190195)\nBatch 3100/3125\t Loss 1.736347 (2.187148)\n==> Epoch 20/30\nBatch 100/3125\t Loss 2.171168 (2.186130)\nBatch 200/3125\t Loss 2.279927 (2.149873)\nBatch 300/3125\t Loss 2.135662 (2.155638)\nBatch 400/3125\t Loss 1.980321 (2.159920)\nBatch 500/3125\t Loss 1.474422 (2.168781)\nBatch 600/3125\t Loss 3.316165 (2.177458)\nBatch 700/3125\t Loss 1.306723 (2.185908)\nBatch 800/3125\t Loss 2.065695 (2.180709)\nBatch 900/3125\t Loss 2.699358 (2.188091)\nBatch 1000/3125\t Loss 1.930724 (2.181392)\nBatch 1100/3125\t Loss 3.110399 (2.176898)\nBatch 1200/3125\t Loss 2.123355 (2.185136)\nBatch 1300/3125\t Loss 2.885164 (2.188352)\nBatch 1400/3125\t Loss 2.116692 (2.182240)\nBatch 1500/3125\t Loss 2.426251 (2.173283)\nBatch 1600/3125\t Loss 2.055578 (2.176058)\nBatch 1700/3125\t Loss 2.709775 (2.176534)\nBatch 1800/3125\t Loss 1.437731 (2.176581)\nBatch 1900/3125\t Loss 2.002629 (2.179801)\nBatch 2000/3125\t Loss 3.313956 (2.176527)\nBatch 2100/3125\t Loss 2.598462 (2.178968)\nBatch 2200/3125\t Loss 2.896947 (2.176585)\nBatch 2300/3125\t Loss 1.325966 (2.178334)\nBatch 2400/3125\t Loss 1.999622 (2.173146)\nBatch 2500/3125\t Loss 2.197549 (2.171427)\nBatch 2600/3125\t Loss 1.854641 (2.172928)\nBatch 2700/3125\t Loss 3.221709 (2.171256)\nBatch 2800/3125\t Loss 1.368501 (2.175249)\nBatch 2900/3125\t Loss 2.304015 (2.171994)\nBatch 3000/3125\t Loss 2.128562 (2.171640)\nBatch 3100/3125\t Loss 1.483636 (2.171062)\n==> Test\n       TNR    AUROC  DTACC  AUIN   AUOUT \nBas    17.830 73.918 67.980 75.636 70.216\nAcc: 68.35000\n==> Epoch 21/30\nBatch 100/3125\t Loss 3.589404 (2.246239)\nBatch 200/3125\t Loss 1.661165 (2.153056)\nBatch 300/3125\t Loss 1.594148 (2.157787)\nBatch 400/3125\t Loss 1.417549 (2.146939)\nBatch 500/3125\t Loss 2.457537 (2.144476)\nBatch 600/3125\t Loss 1.403012 (2.144243)\nBatch 700/3125\t Loss 2.661867 (2.149684)\nBatch 800/3125\t Loss 2.816549 (2.158440)\nBatch 900/3125\t Loss 2.487161 (2.152407)\nBatch 1000/3125\t Loss 2.325364 (2.160947)\nBatch 1100/3125\t Loss 1.499052 (2.163943)\nBatch 1200/3125\t Loss 1.532652 (2.169813)\nBatch 1300/3125\t Loss 2.004911 (2.175977)\nBatch 1400/3125\t Loss 1.729121 (2.175896)\nBatch 1500/3125\t Loss 2.113014 (2.178480)\nBatch 1600/3125\t Loss 2.192234 (2.179850)\nBatch 1700/3125\t Loss 2.385437 (2.172903)\nBatch 1800/3125\t Loss 2.812398 (2.167757)\nBatch 1900/3125\t Loss 2.469872 (2.166973)\nBatch 2000/3125\t Loss 2.549680 (2.173501)\nBatch 2100/3125\t Loss 2.967430 (2.173873)\nBatch 2200/3125\t Loss 2.822539 (2.174414)\nBatch 2300/3125\t Loss 3.104953 (2.175921)\nBatch 2400/3125\t Loss 1.380173 (2.181871)\nBatch 2500/3125\t Loss 1.886034 (2.177723)\nBatch 2600/3125\t Loss 2.060866 (2.177553)\nBatch 2700/3125\t Loss 2.011587 (2.179002)\nBatch 2800/3125\t Loss 1.526282 (2.180238)\nBatch 2900/3125\t Loss 1.803171 (2.180275)\nBatch 3000/3125\t Loss 2.792622 (2.180687)\nBatch 3100/3125\t Loss 2.318661 (2.180040)\n==> Epoch 22/30\nBatch 100/3125\t Loss 1.666254 (2.134593)\nBatch 200/3125\t Loss 1.546678 (2.196966)\nBatch 300/3125\t Loss 3.115453 (2.217750)\nBatch 400/3125\t Loss 3.173749 (2.194156)\nBatch 500/3125\t Loss 2.407346 (2.176212)\nBatch 600/3125\t Loss 3.021367 (2.169695)\nBatch 700/3125\t Loss 1.485024 (2.154974)\nBatch 800/3125\t Loss 2.178515 (2.145792)\nBatch 900/3125\t Loss 3.001968 (2.152783)\nBatch 1000/3125\t Loss 1.283159 (2.151873)\nBatch 1100/3125\t Loss 1.791633 (2.144057)\nBatch 1200/3125\t Loss 2.121827 (2.143175)\nBatch 1300/3125\t Loss 2.526929 (2.150961)\nBatch 1400/3125\t Loss 2.036604 (2.157125)\nBatch 1500/3125\t Loss 2.710810 (2.156633)\nBatch 1600/3125\t Loss 2.908080 (2.167508)\nBatch 1700/3125\t Loss 2.640475 (2.169774)\nBatch 1800/3125\t Loss 1.884609 (2.173089)\nBatch 1900/3125\t Loss 2.006300 (2.169859)\nBatch 2000/3125\t Loss 0.832283 (2.170274)\nBatch 2100/3125\t Loss 1.717551 (2.168696)\nBatch 2200/3125\t Loss 1.209862 (2.170836)\nBatch 2300/3125\t Loss 2.418335 (2.175078)\nBatch 2400/3125\t Loss 2.371519 (2.173855)\nBatch 2500/3125\t Loss 1.765275 (2.175381)\nBatch 2600/3125\t Loss 1.498463 (2.175686)\nBatch 2700/3125\t Loss 2.669311 (2.178045)\nBatch 2800/3125\t Loss 2.975102 (2.181195)\nBatch 2900/3125\t Loss 2.213121 (2.180714)\nBatch 3000/3125\t Loss 2.264547 (2.184409)\nBatch 3100/3125\t Loss 1.843104 (2.181961)\n==> Epoch 23/30\nBatch 100/3125\t Loss 3.315951 (2.256982)\nBatch 200/3125\t Loss 2.662695 (2.190992)\nBatch 300/3125\t Loss 1.826874 (2.207311)\nBatch 400/3125\t Loss 2.074894 (2.194198)\nBatch 500/3125\t Loss 1.983688 (2.206961)\nBatch 600/3125\t Loss 2.671254 (2.195470)\nBatch 700/3125\t Loss 2.872208 (2.183617)\nBatch 800/3125\t Loss 2.715994 (2.186758)\nBatch 900/3125\t Loss 1.846246 (2.184900)\nBatch 1000/3125\t Loss 1.789663 (2.187595)\nBatch 1100/3125\t Loss 1.671616 (2.180123)\nBatch 1200/3125\t Loss 2.902810 (2.180975)\nBatch 1300/3125\t Loss 3.815161 (2.183177)\nBatch 1400/3125\t Loss 2.022188 (2.185252)\nBatch 1500/3125\t Loss 1.890036 (2.182410)\nBatch 1600/3125\t Loss 1.782091 (2.181359)\nBatch 1700/3125\t Loss 1.681563 (2.177422)\nBatch 1800/3125\t Loss 2.722919 (2.179488)\nBatch 1900/3125\t Loss 3.088224 (2.184692)\nBatch 2000/3125\t Loss 2.840557 (2.190448)\nBatch 2100/3125\t Loss 2.398942 (2.186625)\nBatch 2200/3125\t Loss 2.738014 (2.188553)\nBatch 2300/3125\t Loss 1.698158 (2.185774)\nBatch 2400/3125\t Loss 2.583879 (2.186803)\nBatch 2500/3125\t Loss 1.301664 (2.181907)\nBatch 2600/3125\t Loss 3.185297 (2.179643)\nBatch 2700/3125\t Loss 3.148823 (2.180820)\nBatch 2800/3125\t Loss 1.854192 (2.179676)\nBatch 2900/3125\t Loss 2.221792 (2.179168)\nBatch 3000/3125\t Loss 1.772226 (2.176950)\nBatch 3100/3125\t Loss 2.080555 (2.173259)\n==> Epoch 24/30\nBatch 100/3125\t Loss 2.804601 (2.259697)\nBatch 200/3125\t Loss 1.369330 (2.207139)\nBatch 300/3125\t Loss 0.908048 (2.194839)\nBatch 400/3125\t Loss 2.672169 (2.196405)\nBatch 500/3125\t Loss 1.698794 (2.170183)\nBatch 600/3125\t Loss 2.500468 (2.169457)\nBatch 700/3125\t Loss 1.841999 (2.170482)\nBatch 800/3125\t Loss 2.537330 (2.162406)\nBatch 900/3125\t Loss 2.400940 (2.166602)\nBatch 1000/3125\t Loss 2.780356 (2.171207)\nBatch 1100/3125\t Loss 1.347997 (2.166030)\nBatch 1200/3125\t Loss 2.416243 (2.160649)\nBatch 1300/3125\t Loss 2.713543 (2.168096)\nBatch 1400/3125\t Loss 2.052543 (2.168667)\nBatch 1500/3125\t Loss 2.024034 (2.164720)\nBatch 1600/3125\t Loss 1.983647 (2.171826)\nBatch 1700/3125\t Loss 2.756613 (2.174512)\nBatch 1800/3125\t Loss 2.523265 (2.175293)\nBatch 1900/3125\t Loss 1.756169 (2.178544)\nBatch 2000/3125\t Loss 2.747820 (2.181169)\nBatch 2100/3125\t Loss 2.401561 (2.181756)\nBatch 2200/3125\t Loss 2.634163 (2.184311)\nBatch 2300/3125\t Loss 1.820528 (2.183811)\nBatch 2400/3125\t Loss 1.935592 (2.182695)\nBatch 2500/3125\t Loss 2.912493 (2.183263)\nBatch 2600/3125\t Loss 2.003971 (2.178606)\nBatch 2700/3125\t Loss 2.004860 (2.180981)\nBatch 2800/3125\t Loss 1.954018 (2.178989)\nBatch 2900/3125\t Loss 1.201995 (2.180940)\nBatch 3000/3125\t Loss 2.022338 (2.181002)\nBatch 3100/3125\t Loss 1.505652 (2.179706)\n==> Epoch 25/30\nBatch 100/3125\t Loss 2.874821 (2.155760)\nBatch 200/3125\t Loss 2.306026 (2.188493)\nBatch 300/3125\t Loss 1.474141 (2.188645)\nBatch 400/3125\t Loss 2.077080 (2.160434)\nBatch 500/3125\t Loss 1.781894 (2.182895)\nBatch 600/3125\t Loss 2.385743 (2.183012)\nBatch 700/3125\t Loss 3.228097 (2.174340)\nBatch 800/3125\t Loss 1.647842 (2.178417)\nBatch 900/3125\t Loss 1.172434 (2.163105)\nBatch 1000/3125\t Loss 1.284842 (2.167965)\nBatch 1100/3125\t Loss 2.465221 (2.166025)\nBatch 1200/3125\t Loss 1.147215 (2.170402)\nBatch 1300/3125\t Loss 1.598888 (2.165157)\nBatch 1400/3125\t Loss 2.133150 (2.167092)\nBatch 1500/3125\t Loss 1.999595 (2.162120)\nBatch 1600/3125\t Loss 2.672245 (2.162487)\nBatch 1700/3125\t Loss 2.048453 (2.169784)\nBatch 1800/3125\t Loss 2.490013 (2.174971)\nBatch 1900/3125\t Loss 2.553432 (2.172081)\nBatch 2000/3125\t Loss 2.688016 (2.175946)\nBatch 2100/3125\t Loss 2.589019 (2.178069)\nBatch 2200/3125\t Loss 1.849838 (2.174259)\nBatch 2300/3125\t Loss 0.931284 (2.178726)\nBatch 2400/3125\t Loss 2.074243 (2.176109)\nBatch 2500/3125\t Loss 2.717985 (2.179607)\nBatch 2600/3125\t Loss 3.169996 (2.175639)\nBatch 2700/3125\t Loss 1.246232 (2.177814)\nBatch 2800/3125\t Loss 1.930377 (2.181138)\nBatch 2900/3125\t Loss 2.140090 (2.179936)\nBatch 3000/3125\t Loss 2.677157 (2.180381)\nBatch 3100/3125\t Loss 1.665818 (2.178982)\n==> Test\n       TNR    AUROC  DTACC  AUIN   AUOUT \nBas    22.030 73.719 67.375 73.930 72.493\nAcc: 65.67000\n==> Epoch 26/30\nBatch 100/3125\t Loss 2.161454 (2.085404)\nBatch 200/3125\t Loss 3.557147 (2.181617)\nBatch 300/3125\t Loss 2.831235 (2.180700)\nBatch 400/3125\t Loss 1.739617 (2.175004)\nBatch 500/3125\t Loss 1.236363 (2.174459)\nBatch 600/3125\t Loss 2.284713 (2.168958)\nBatch 700/3125\t Loss 2.797838 (2.179533)\nBatch 800/3125\t Loss 3.024704 (2.187794)\nBatch 900/3125\t Loss 0.701060 (2.185700)\nBatch 1000/3125\t Loss 2.166706 (2.199056)\nBatch 1100/3125\t Loss 1.280257 (2.191926)\nBatch 1200/3125\t Loss 2.196428 (2.193640)\nBatch 1300/3125\t Loss 2.192298 (2.191836)\nBatch 1400/3125\t Loss 1.372009 (2.195276)\nBatch 1500/3125\t Loss 2.616803 (2.203013)\nBatch 1600/3125\t Loss 1.442879 (2.201094)\nBatch 1700/3125\t Loss 1.791336 (2.201669)\nBatch 1800/3125\t Loss 0.946163 (2.199533)\nBatch 1900/3125\t Loss 1.352265 (2.202510)\nBatch 2000/3125\t Loss 2.260068 (2.199567)\nBatch 2100/3125\t Loss 2.360525 (2.189947)\nBatch 2200/3125\t Loss 1.930200 (2.195401)\nBatch 2300/3125\t Loss 1.870775 (2.189750)\nBatch 2400/3125\t Loss 2.571961 (2.190232)\nBatch 2500/3125\t Loss 2.084005 (2.187764)\nBatch 2600/3125\t Loss 1.722532 (2.188531)\nBatch 2700/3125\t Loss 2.868815 (2.192403)\nBatch 2800/3125\t Loss 2.483899 (2.192166)\nBatch 2900/3125\t Loss 1.331594 (2.188696)\nBatch 3000/3125\t Loss 1.072432 (2.185179)\nBatch 3100/3125\t Loss 2.509873 (2.184016)\n==> Epoch 27/30\nBatch 100/3125\t Loss 1.592867 (2.215227)\nBatch 200/3125\t Loss 2.378298 (2.237670)\nBatch 300/3125\t Loss 2.999695 (2.237809)\nBatch 400/3125\t Loss 2.518559 (2.204582)\nBatch 500/3125\t Loss 1.896531 (2.208528)\nBatch 600/3125\t Loss 2.884118 (2.199574)\nBatch 700/3125\t Loss 1.969716 (2.189405)\nBatch 800/3125\t Loss 2.042123 (2.189980)\nBatch 900/3125\t Loss 2.242585 (2.185556)\nBatch 1000/3125\t Loss 2.657310 (2.189635)\nBatch 1100/3125\t Loss 1.792134 (2.187282)\nBatch 1200/3125\t Loss 2.683136 (2.186615)\nBatch 1300/3125\t Loss 2.761158 (2.182245)\nBatch 1400/3125\t Loss 1.294125 (2.189771)\nBatch 1500/3125\t Loss 1.851465 (2.190561)\nBatch 1600/3125\t Loss 3.237024 (2.188079)\nBatch 1700/3125\t Loss 2.440178 (2.193354)\nBatch 1800/3125\t Loss 2.150538 (2.189309)\nBatch 1900/3125\t Loss 1.679492 (2.188354)\nBatch 2000/3125\t Loss 1.786220 (2.191476)\nBatch 2100/3125\t Loss 1.757426 (2.194318)\nBatch 2200/3125\t Loss 2.631727 (2.197327)\nBatch 2300/3125\t Loss 3.017001 (2.194723)\nBatch 2400/3125\t Loss 3.164482 (2.198274)\nBatch 2500/3125\t Loss 1.147577 (2.198889)\nBatch 2600/3125\t Loss 1.978165 (2.196506)\nBatch 2700/3125\t Loss 2.973462 (2.193864)\nBatch 2800/3125\t Loss 2.892285 (2.193891)\nBatch 2900/3125\t Loss 2.719027 (2.198163)\nBatch 3000/3125\t Loss 2.126954 (2.199802)\nBatch 3100/3125\t Loss 2.737504 (2.197717)\n==> Epoch 28/30\nBatch 100/3125\t Loss 1.699722 (2.190897)\nBatch 200/3125\t Loss 1.337171 (2.184284)\nBatch 300/3125\t Loss 1.661514 (2.190478)\nBatch 400/3125\t Loss 2.232102 (2.187174)\nBatch 500/3125\t Loss 1.840596 (2.193400)\nBatch 600/3125\t Loss 3.370832 (2.195815)\nBatch 700/3125\t Loss 3.092122 (2.198758)\nBatch 800/3125\t Loss 1.795479 (2.190393)\nBatch 900/3125\t Loss 3.028821 (2.191457)\nBatch 1000/3125\t Loss 1.882115 (2.187773)\nBatch 1100/3125\t Loss 1.765839 (2.194248)\nBatch 1200/3125\t Loss 1.231134 (2.190173)\nBatch 1300/3125\t Loss 3.638995 (2.194121)\nBatch 1400/3125\t Loss 2.087647 (2.192426)\nBatch 1500/3125\t Loss 2.536194 (2.190049)\nBatch 1600/3125\t Loss 2.924621 (2.191413)\nBatch 1700/3125\t Loss 0.859713 (2.192949)\nBatch 1800/3125\t Loss 2.114870 (2.197238)\nBatch 1900/3125\t Loss 2.534195 (2.195275)\nBatch 2000/3125\t Loss 3.409476 (2.193409)\nBatch 2100/3125\t Loss 2.795535 (2.194708)\nBatch 2200/3125\t Loss 3.109641 (2.191807)\nBatch 2300/3125\t Loss 0.682031 (2.189439)\nBatch 2400/3125\t Loss 2.251678 (2.187102)\nBatch 2500/3125\t Loss 2.498537 (2.186907)\nBatch 2600/3125\t Loss 1.834018 (2.188944)\nBatch 2700/3125\t Loss 1.504877 (2.189851)\nBatch 2800/3125\t Loss 2.029617 (2.188484)\nBatch 2900/3125\t Loss 1.825750 (2.189431)\nBatch 3000/3125\t Loss 2.905607 (2.191630)\nBatch 3100/3125\t Loss 2.910483 (2.192014)\n==> Epoch 29/30\nBatch 100/3125\t Loss 2.162744 (2.209410)\nBatch 200/3125\t Loss 2.569782 (2.171540)\nBatch 300/3125\t Loss 2.810617 (2.181740)\nBatch 400/3125\t Loss 2.013769 (2.185708)\nBatch 500/3125\t Loss 2.411270 (2.167382)\nBatch 600/3125\t Loss 2.768740 (2.155383)\nBatch 700/3125\t Loss 0.972945 (2.164805)\nBatch 800/3125\t Loss 3.154517 (2.170378)\nBatch 900/3125\t Loss 1.937367 (2.170796)\nBatch 1000/3125\t Loss 2.093111 (2.180539)\nBatch 1100/3125\t Loss 1.566583 (2.173164)\nBatch 1200/3125\t Loss 1.631679 (2.175859)\nBatch 1300/3125\t Loss 2.958483 (2.179542)\nBatch 1400/3125\t Loss 1.873440 (2.177429)\nBatch 1500/3125\t Loss 1.798283 (2.180750)\nBatch 1600/3125\t Loss 2.563536 (2.185456)\nBatch 1700/3125\t Loss 2.178333 (2.187279)\nBatch 1800/3125\t Loss 2.605437 (2.186035)\nBatch 1900/3125\t Loss 3.205240 (2.184429)\nBatch 2000/3125\t Loss 0.737194 (2.188647)\nBatch 2100/3125\t Loss 2.800343 (2.191948)\nBatch 2200/3125\t Loss 3.709895 (2.186766)\nBatch 2300/3125\t Loss 2.550594 (2.186900)\nBatch 2400/3125\t Loss 1.797518 (2.186743)\nBatch 2500/3125\t Loss 2.133229 (2.188346)\nBatch 2600/3125\t Loss 1.545835 (2.185566)\nBatch 2700/3125\t Loss 3.053600 (2.184918)\nBatch 2800/3125\t Loss 2.741580 (2.185412)\nBatch 2900/3125\t Loss 2.309574 (2.186263)\nBatch 3000/3125\t Loss 2.633923 (2.187242)\nBatch 3100/3125\t Loss 1.242121 (2.185966)\n==> Epoch 30/30\nBatch 100/3125\t Loss 2.461663 (2.244253)\nBatch 200/3125\t Loss 1.838570 (2.221010)\nBatch 300/3125\t Loss 1.115153 (2.183881)\nBatch 400/3125\t Loss 1.304657 (2.182698)\nBatch 500/3125\t Loss 3.169073 (2.168833)\nBatch 600/3125\t Loss 1.267913 (2.189592)\nBatch 700/3125\t Loss 1.413210 (2.207413)\nBatch 800/3125\t Loss 2.928119 (2.198938)\nBatch 900/3125\t Loss 1.979209 (2.198677)\nBatch 1000/3125\t Loss 2.809733 (2.199793)\nBatch 1100/3125\t Loss 2.253922 (2.195410)\nBatch 1200/3125\t Loss 3.479167 (2.203932)\nBatch 1300/3125\t Loss 0.993851 (2.213220)\nBatch 1400/3125\t Loss 2.390073 (2.206188)\nBatch 1500/3125\t Loss 3.162647 (2.206668)\nBatch 1600/3125\t Loss 2.006005 (2.215561)\nBatch 1700/3125\t Loss 2.334194 (2.216097)\nBatch 1800/3125\t Loss 2.228072 (2.209766)\nBatch 1900/3125\t Loss 1.010809 (2.200953)\nBatch 2000/3125\t Loss 1.657904 (2.202930)\nBatch 2100/3125\t Loss 3.026364 (2.202238)\nBatch 2200/3125\t Loss 2.135756 (2.199125)\nBatch 2300/3125\t Loss 1.168773 (2.198896)\nBatch 2400/3125\t Loss 3.232786 (2.200117)\nBatch 2500/3125\t Loss 2.088064 (2.200541)\nBatch 2600/3125\t Loss 1.704382 (2.195547)\nBatch 2700/3125\t Loss 2.494392 (2.193264)\nBatch 2800/3125\t Loss 2.587738 (2.191826)\nBatch 2900/3125\t Loss 2.348076 (2.193637)\nBatch 3000/3125\t Loss 1.608483 (2.193072)\nBatch 3100/3125\t Loss 3.288085 (2.192593)\n==> Test\n       TNR    AUROC  DTACC  AUIN   AUOUT \nBas    14.770 69.372 63.610 71.430 66.417\nAcc: 63.01000\nFinished. Total elapsed time (h:m:s): 1:22:51\n","output_type":"stream"}]},{"cell_type":"code","source":"if options['eval']:\n        net, criterion = load_networks(net, options['outf'], file_name, criterion=criterion)\n        outloaders = Data.out_loaders\n        results = test(net, criterion, testloader, outloader, epoch=0, **options)\n        acc = results['ACC']\n        res = dict()\n        res['ACC'] = dict()\n        acc_res = []\n        for key in Data.out_keys:\n            results = test_robustness(net, criterion, outloaders[key], epoch=0, label=key, **options)\n            print('{} (%): {:.3f}\\t'.format(key, results['ACC']))\n            res['ACC'][key] = results['ACC']\n            acc_res.append(results['ACC'])\n        print('Mean ACC:', np.mean(acc_res))\n        print('Mean Error:', 100-np.mean(acc_res))","metadata":{"id":"7tTiict2s8mr","execution":{"iopub.status.busy":"2023-04-24T22:56:15.667977Z","iopub.execute_input":"2023-04-24T22:56:15.668316Z","iopub.status.idle":"2023-04-24T23:05:22.668186Z","shell.execute_reply.started":"2023-04-24T22:56:15.668283Z","shell.execute_reply":"2023-04-24T23:05:22.666820Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"       TNR    AUROC  DTACC  AUIN   AUOUT \nBas    14.770 69.372 63.610 71.430 66.417\nAcc: 63.01000\ngaussian_noise (%): 49.378\t\nshot_noise (%): 50.724\t\nimpulse_noise (%): 43.628\t\ndefocus_blur (%): 51.258\t\nglass_blur (%): 35.248\t\nmotion_blur (%): 44.942\t\nzoom_blur (%): 46.490\t\nsnow (%): 55.030\t\nfrost (%): 52.494\t\nfog (%): 53.662\t\nbrightness (%): 61.426\t\ncontrast (%): 45.208\t\nelastic_transform (%): 48.764\t\npixelate (%): 47.034\t\njpeg_compression (%): 50.622\t\nMean ACC: 49.060533333333325\nMean Error: 50.939466666666675\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"IRxS5jfTAQlV"},"execution_count":null,"outputs":[]}]}