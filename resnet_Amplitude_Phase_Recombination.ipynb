{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "v-E1ObShkKrG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import datetime\n",
        "import time\n",
        "import csv\n",
        "import os.path as osp\n",
        "import numpy as np\n",
        "import warnings\n",
        "import errno\n",
        "import importlib\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "_yp8KiUpkgzv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "rBN4J-w8kmqG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Base augmentations operators.\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps, ImageEnhance\n",
        "\n",
        "# ImageNet code should change this value\n",
        "IMAGE_SIZE = 32\n",
        "\n",
        "\n",
        "def int_parameter(level, maxval):\n",
        "  \"\"\"Helper function to scale `val` between 0 and maxval .\n",
        "  Args:\n",
        "    level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n",
        "    maxval: Maximum value that the operation can have. This will be scaled to\n",
        "      level/PARAMETER_MAX.\n",
        "  Returns:\n",
        "    An int that results from scaling `maxval` according to `level`.\n",
        "  \"\"\"\n",
        "  return int(level * maxval / 10)\n",
        "\n",
        "\n",
        "def float_parameter(level, maxval):\n",
        "  \"\"\"Helper function to scale `val` between 0 and maxval.\n",
        "  Args:\n",
        "    level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n",
        "    maxval: Maximum value that the operation can have. This will be scaled to\n",
        "      level/PARAMETER_MAX.\n",
        "  Returns:\n",
        "    A float that results from scaling `maxval` according to `level`.\n",
        "  \"\"\"\n",
        "  return float(level) * maxval / 10.\n",
        "\n",
        "\n",
        "def sample_level(n):\n",
        "  return np.random.uniform(low=0.1, high=n)\n",
        "\n",
        "\n",
        "def autocontrast(pil_img, _):\n",
        "  return ImageOps.autocontrast(pil_img)\n",
        "\n",
        "\n",
        "def equalize(pil_img, _):\n",
        "  return ImageOps.equalize(pil_img)\n",
        "\n",
        "\n",
        "def posterize(pil_img, level):\n",
        "  level = int_parameter(sample_level(level), 4)\n",
        "  return ImageOps.posterize(pil_img, 4 - level)\n",
        "\n",
        "\n",
        "def rotate(pil_img, level):\n",
        "  degrees = int_parameter(sample_level(level), 30)\n",
        "  if np.random.uniform() > 0.5:\n",
        "    degrees = -degrees\n",
        "  return pil_img.rotate(degrees, resample=Image.BILINEAR)\n",
        "\n",
        "\n",
        "def solarize(pil_img, level):\n",
        "  level = int_parameter(sample_level(level), 256)\n",
        "  return ImageOps.solarize(pil_img, 256 - level)\n",
        "\n",
        "\n",
        "def shear_x(pil_img, level):\n",
        "  level = float_parameter(sample_level(level), 0.3)\n",
        "  if np.random.uniform() > 0.5:\n",
        "    level = -level\n",
        "  return pil_img.transform((IMAGE_SIZE, IMAGE_SIZE),\n",
        "                           Image.AFFINE, (1, level, 0, 0, 1, 0),\n",
        "                           resample=Image.BILINEAR)\n",
        "\n",
        "\n",
        "def shear_y(pil_img, level):\n",
        "  level = float_parameter(sample_level(level), 0.3)\n",
        "  if np.random.uniform() > 0.5:\n",
        "    level = -level\n",
        "  return pil_img.transform((IMAGE_SIZE, IMAGE_SIZE),\n",
        "                           Image.AFFINE, (1, 0, 0, level, 1, 0),\n",
        "                           resample=Image.BILINEAR)\n",
        "\n",
        "\n",
        "def translate_x(pil_img, level):\n",
        "  level = int_parameter(sample_level(level), IMAGE_SIZE / 3)\n",
        "  if np.random.random() > 0.5:\n",
        "    level = -level\n",
        "  return pil_img.transform((IMAGE_SIZE, IMAGE_SIZE),\n",
        "                           Image.AFFINE, (1, 0, level, 0, 1, 0),\n",
        "                           resample=Image.BILINEAR)\n",
        "\n",
        "\n",
        "def translate_y(pil_img, level):\n",
        "  level = int_parameter(sample_level(level), IMAGE_SIZE / 3)\n",
        "  if np.random.random() > 0.5:\n",
        "    level = -level\n",
        "  return pil_img.transform((IMAGE_SIZE, IMAGE_SIZE),\n",
        "                           Image.AFFINE, (1, 0, 0, 0, 1, level),\n",
        "                           resample=Image.BILINEAR)\n",
        "\n",
        "\n",
        "# operation that overlaps with ImageNet-C's test set\n",
        "def color(pil_img, level):\n",
        "    level = float_parameter(sample_level(level), 1.8) + 0.1\n",
        "    return ImageEnhance.Color(pil_img).enhance(level)\n",
        "\n",
        "\n",
        "# operation that overlaps with ImageNet-C's test set\n",
        "def contrast(pil_img, level):\n",
        "    level = float_parameter(sample_level(level), 1.8) + 0.1\n",
        "    return ImageEnhance.Contrast(pil_img).enhance(level)\n",
        "\n",
        "\n",
        "# operation that overlaps with ImageNet-C's test set\n",
        "def brightness(pil_img, level):\n",
        "    level = float_parameter(sample_level(level), 1.8) + 0.1\n",
        "    return ImageEnhance.Brightness(pil_img).enhance(level)\n",
        "\n",
        "\n",
        "# operation that overlaps with ImageNet-C's test set\n",
        "def sharpness(pil_img, level):\n",
        "    level = float_parameter(sample_level(level), 1.8) + 0.1\n",
        "    return ImageEnhance.Sharpness(pil_img).enhance(level)\n",
        "\n",
        "\n",
        "augmentations = [\n",
        "    autocontrast, equalize, posterize, rotate, solarize, shear_x, shear_y,\n",
        "    translate_x, translate_y\n",
        "]\n",
        "\n",
        "augmentations_all = [\n",
        "    autocontrast, equalize, posterize, rotate, solarize, shear_x, shear_y,\n",
        "    translate_x, translate_y, color, contrast, brightness, sharpness\n",
        "]"
      ],
      "metadata": {
        "id": "sNZr99VEtShx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import random\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class APRecombination(object):\n",
        "    def __init__(self, img_size=32, aug=None):\n",
        "        if aug is None:\n",
        "            augmentations.IMAGE_SIZE = img_size\n",
        "            self.aug_list = augmentations.augmentations\n",
        "        else:\n",
        "            self.aug_list = aug.augmentations\n",
        "\n",
        "    def __call__(self, x):\n",
        "        '''\n",
        "        :param img: (PIL Image): Image\n",
        "        :return: code img (PIL Image): Image\n",
        "        '''\n",
        "\n",
        "        op = np.random.choice(self.aug_list)\n",
        "        x = op(x, 3)\n",
        "\n",
        "        p = random.uniform(0, 1)\n",
        "        if p > 0.5:\n",
        "            return x\n",
        "\n",
        "        x_aug = x.copy()\n",
        "        op = np.random.choice(self.aug_list)\n",
        "        x_aug = op(x_aug, 3)\n",
        "\n",
        "        x = np.array(x).astype(np.uint8) \n",
        "        x_aug = np.array(x_aug).astype(np.uint8)\n",
        "        \n",
        "        fft_1 = np.fft.fftshift(np.fft.fftn(x))\n",
        "        fft_2 = np.fft.fftshift(np.fft.fftn(x_aug))\n",
        "        \n",
        "        abs_1, angle_1 = np.abs(fft_1), np.angle(fft_1)\n",
        "        abs_2, angle_2 = np.abs(fft_2), np.angle(fft_2)\n",
        "\n",
        "        fft_1 = abs_1*np.exp((1j) * angle_2)\n",
        "        fft_2 = abs_2*np.exp((1j) * angle_1)\n",
        "\n",
        "        p = random.uniform(0, 1)\n",
        "\n",
        "        if p > 0.5:\n",
        "            x = np.fft.ifftn(np.fft.ifftshift(fft_1))\n",
        "        else:\n",
        "            x = np.fft.ifftn(np.fft.ifftshift(fft_2))\n",
        "\n",
        "        x = x.astype(np.uint8)\n",
        "        x = Image.fromarray(x)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "iY7qbQxLrzc1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "normalize = transforms.Compose([\n",
        "        transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]),\n",
        "    ])\n",
        "\n",
        "def train_transforms(_transforms):\n",
        "    transforms_list = []\n",
        "    if 'aprs' in _transforms:\n",
        "        print('APRecombination', _transforms)\n",
        "        transforms_list.extend([\n",
        "            transforms.RandomApply([APRecombination()], p=1.0),\n",
        "            transforms.RandomCrop(32, padding=4, fill=128),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "    else:\n",
        "        transforms_list.extend([\n",
        "            transforms.RandomCrop(32, padding=4, fill=128),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    return transforms_list\n",
        "\n",
        "\n",
        "def test_transforms():\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    return test_transform"
      ],
      "metadata": {
        "id": "MjjHKN8ktMKX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import pickle\n",
        "import numpy as np\n",
        "from scipy import signal\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10, CIFAR100, ImageFolder\n",
        "\n",
        "\n",
        "class CIFARC(CIFAR10):\n",
        "    def __init__(\n",
        "            self,\n",
        "            root,\n",
        "            key = 'zoom_blur',\n",
        "            transform = None,\n",
        "            target_transform = None,\n",
        "    ):\n",
        "\n",
        "        super(CIFAR10, self).__init__(root, transform=transform,\n",
        "                                      target_transform=target_transform)\n",
        "\n",
        "        data_path = os.path.join(root, key+'.npy')\n",
        "        labels_path = os.path.join(root, 'labels.npy')\n",
        "\n",
        "        self.data = np.load(data_path)\n",
        "        self.targets = np.load(labels_path)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "class CIFAR10D(object):\n",
        "    def __init__(self, dataroot='', use_gpu=True, num_workers=4, batch_size=128, _transforms='', _eval=False):\n",
        "\n",
        "        transforms_list = train_transforms(_transforms)\n",
        "\n",
        "        train_transform = transforms.Compose(transforms_list)\n",
        "        test_transform = test_transforms()\n",
        "        self.train_transform = train_transform\n",
        "\n",
        "        pin_memory = True if use_gpu else False\n",
        "\n",
        "        data_root = os.path.join(dataroot, 'cifar10')\n",
        "\n",
        "        trainset = CIFAR10(root=data_root, train=True, download=True, transform=train_transform)\n",
        "        \n",
        "        self.train_loader = torch.utils.data.DataLoader(\n",
        "            trainset, batch_size=batch_size, shuffle=True,\n",
        "            num_workers=num_workers, pin_memory=pin_memory,\n",
        "        )\n",
        "        \n",
        "        testset = CIFAR10(root=data_root, train=False, download=True, transform=test_transform)\n",
        "        \n",
        "        self.test_loader = torch.utils.data.DataLoader(\n",
        "            testset, batch_size=batch_size, shuffle=False,\n",
        "            num_workers=num_workers, pin_memory=pin_memory,\n",
        "        )\n",
        "\n",
        "        if _eval:\n",
        "            self.out_loaders = dict()\n",
        "            self.out_keys = ['gaussian_noise', 'shot_noise', 'impulse_noise', 'defocus_blur',\n",
        "                            'glass_blur', 'motion_blur', 'zoom_blur', 'snow', 'frost', 'fog',\n",
        "                            'brightness', 'contrast', 'elastic_transform', 'pixelate',\n",
        "                            'jpeg_compression']\n",
        "\n",
        "            data_root = os.path.join(dataroot, 'CIFAR-10-C')\n",
        "            for key in self.out_keys:\n",
        "                outset = CIFARC(root=data_root, key=key, transform=test_transform)\n",
        "                out_loader = torch.utils.data.DataLoader(\n",
        "                    outset, batch_size=batch_size, shuffle=False,\n",
        "                    num_workers=num_workers, pin_memory=pin_memory,\n",
        "                )\n",
        "                self.out_loaders[key] = out_loader\n",
        "        \n",
        "        self.num_classes = 10\n",
        "\n",
        "class CIFAR100D(object):\n",
        "    def __init__(self, dataroot='', use_gpu=True, num_workers=4, batch_size=128, _transforms='', _eval=False):\n",
        "\n",
        "        transforms_list = train_transforms(_transforms)\n",
        "\n",
        "        train_transform = transforms.Compose(transforms_list)\n",
        "        test_transform = test_transforms()\n",
        "        self.train_transform = train_transform\n",
        "\n",
        "        pin_memory = True if use_gpu else False\n",
        "\n",
        "        data_root = os.path.join(dataroot, 'cifar100')\n",
        "\n",
        "        trainset = CIFAR100(root=data_root, train=True, download=True, transform=train_transform)\n",
        "        \n",
        "        self.train_loader = torch.utils.data.DataLoader(\n",
        "            trainset, batch_size=batch_size, shuffle=True,\n",
        "            num_workers=num_workers, pin_memory=pin_memory,\n",
        "        )\n",
        "        \n",
        "        testset = CIFAR100(root=data_root, train=False, download=True, transform=test_transform)\n",
        "        \n",
        "        self.test_loader = torch.utils.data.DataLoader(\n",
        "            testset, batch_size=batch_size, shuffle=False,\n",
        "            num_workers=num_workers, pin_memory=pin_memory,\n",
        "        )\n",
        "\n",
        "        if _eval:\n",
        "            self.out_loaders = dict()\n",
        "            self.out_keys = ['gaussian_noise', 'shot_noise', 'impulse_noise', 'defocus_blur',\n",
        "                            'glass_blur', 'motion_blur', 'zoom_blur', 'snow', 'frost', 'fog',\n",
        "                            'brightness', 'contrast', 'elastic_transform', 'pixelate',\n",
        "                            'jpeg_compression']\n",
        "\n",
        "            data_root = os.path.join(dataroot, 'CIFAR-100-C')\n",
        "            for key in self.out_keys:\n",
        "                outset = CIFARC(root=data_root, key=key, transform=test_transform)\n",
        "                out_loader = torch.utils.data.DataLoader(\n",
        "                    outset, batch_size=batch_size, shuffle=False,\n",
        "                    num_workers=num_workers, pin_memory=pin_memory,\n",
        "                )\n",
        "                self.out_loaders[key] = out_loader\n",
        "\n",
        "        self.num_classes = 100"
      ],
      "metadata": {
        "id": "0KTRpO0HtZxi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''ResNet in PyTorch.\n",
        "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, rf=False, _eval=False):\n",
        "        if _eval:\n",
        "            self.eval()\n",
        "        else:\n",
        "            self.train()\n",
        "\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        y = self.linear(out)\n",
        "        if rf:\n",
        "            return out, y\n",
        "        return y\n",
        "\n",
        "\n",
        "def ResNet18(num_classes):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])"
      ],
      "metadata": {
        "id": "TnzLuRymtioH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2019 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"AllConv implementation (https://arxiv.org/abs/1412.6806).\"\"\"\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.sigmoid(1.702 * x) * x\n",
        "\n",
        "\n",
        "def make_layers(cfg):\n",
        "  \"\"\"Create a single layer.\"\"\"\n",
        "  layers = []\n",
        "  in_channels = 3\n",
        "  for v in cfg:\n",
        "    if v == 'Md':\n",
        "      layers += [nn.MaxPool2d(kernel_size=2, stride=2), nn.Dropout(p=0.5)]\n",
        "    elif v == 'A':\n",
        "      layers += [nn.AvgPool2d(kernel_size=8)]\n",
        "    elif v == 'NIN':\n",
        "      conv2d = nn.Conv2d(in_channels, in_channels, kernel_size=1, padding=1)\n",
        "      layers += [conv2d, nn.BatchNorm2d(in_channels), GELU()]\n",
        "    elif v == 'nopad':\n",
        "      conv2d = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=0)\n",
        "      layers += [conv2d, nn.BatchNorm2d(in_channels), GELU()]\n",
        "    else:\n",
        "      conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "      layers += [conv2d, nn.BatchNorm2d(v), GELU()]\n",
        "      in_channels = v\n",
        "  return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class AllConvNet(nn.Module):\n",
        "  \"\"\"AllConvNet main class.\"\"\"\n",
        "\n",
        "  def __init__(self, num_classes):\n",
        "    super(AllConvNet, self).__init__()\n",
        "\n",
        "    self.num_classes = num_classes\n",
        "    self.width1, w1 = 96, 96\n",
        "    self.width2, w2 = 192, 192\n",
        "\n",
        "    self.features = make_layers(\n",
        "        [w1, w1, w1, 'Md', w2, w2, w2, 'Md', 'nopad', 'NIN', 'NIN', 'A'])\n",
        "    self.classifier = nn.Linear(self.width2, num_classes)\n",
        "\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        m.weight.data.normal_(0, math.sqrt(2. / n))  # He initialization\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        m.bias.data.zero_()\n",
        "\n",
        "  def forward(self, x, rf=False, _eval=False):\n",
        "    if _eval:\n",
        "        # switch to eval mode\n",
        "        self.eval()\n",
        "    else:\n",
        "        self.train()\n",
        "\n",
        "    x = self.features(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    y = self.classifier(x)\n",
        "    if rf:\n",
        "        return x, y\n",
        "    return y"
      ],
      "metadata": {
        "id": "pDXheT0_soxd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"DenseNet implementation (https://arxiv.org/abs/1608.06993).\"\"\"\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "  \"\"\"Bottleneck block for DenseNet.\"\"\"\n",
        "\n",
        "  def __init__(self, n_channels, growth_rate):\n",
        "    super(Bottleneck, self).__init__()\n",
        "    inter_channels = 4 * growth_rate\n",
        "    self.bn1 = nn.BatchNorm2d(n_channels)\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        n_channels, inter_channels, kernel_size=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(inter_channels)\n",
        "    self.conv2 = nn.Conv2d(\n",
        "        inter_channels, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(F.relu(self.bn1(x)))\n",
        "    out = self.conv2(F.relu(self.bn2(out)))\n",
        "    out = torch.cat((x, out), 1)\n",
        "    return out\n",
        "\n",
        "\n",
        "class SingleLayer(nn.Module):\n",
        "  \"\"\"Layer container for blocks.\"\"\"\n",
        "\n",
        "  def __init__(self, n_channels, growth_rate):\n",
        "    super(SingleLayer, self).__init__()\n",
        "    self.bn1 = nn.BatchNorm2d(n_channels)\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        n_channels, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(F.relu(self.bn1(x)))\n",
        "    out = torch.cat((x, out), 1)\n",
        "    return out\n",
        "\n",
        "\n",
        "class Transition(nn.Module):\n",
        "  \"\"\"Transition block.\"\"\"\n",
        "\n",
        "  def __init__(self, n_channels, n_out_channels):\n",
        "    super(Transition, self).__init__()\n",
        "    self.bn1 = nn.BatchNorm2d(n_channels)\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        n_channels, n_out_channels, kernel_size=1, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(F.relu(self.bn1(x)))\n",
        "    out = F.avg_pool2d(out, 2)\n",
        "    return out\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "  \"\"\"DenseNet main class.\"\"\"\n",
        "\n",
        "  def __init__(self, growth_rate, depth, reduction, n_classes, bottleneck):\n",
        "    super(DenseNet, self).__init__()\n",
        "\n",
        "    if bottleneck:\n",
        "      n_dense_blocks = int((depth - 4) / 6)\n",
        "    else:\n",
        "      n_dense_blocks = int((depth - 4) / 3)\n",
        "\n",
        "    n_channels = 2 * growth_rate\n",
        "    self.conv1 = nn.Conv2d(3, n_channels, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "    self.dense1 = self._make_dense(n_channels, growth_rate, n_dense_blocks,\n",
        "                                   bottleneck)\n",
        "    n_channels += n_dense_blocks * growth_rate\n",
        "    n_out_channels = int(math.floor(n_channels * reduction))\n",
        "    self.trans1 = Transition(n_channels, n_out_channels)\n",
        "\n",
        "    n_channels = n_out_channels\n",
        "    self.dense2 = self._make_dense(n_channels, growth_rate, n_dense_blocks,\n",
        "                                   bottleneck)\n",
        "    n_channels += n_dense_blocks * growth_rate\n",
        "    n_out_channels = int(math.floor(n_channels * reduction))\n",
        "    self.trans2 = Transition(n_channels, n_out_channels)\n",
        "\n",
        "    n_channels = n_out_channels\n",
        "    self.dense3 = self._make_dense(n_channels, growth_rate, n_dense_blocks,\n",
        "                                   bottleneck)\n",
        "    n_channels += n_dense_blocks * growth_rate\n",
        "\n",
        "    self.bn1 = nn.BatchNorm2d(n_channels)\n",
        "    self.fc = nn.Linear(n_channels, n_classes)\n",
        "\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        m.bias.data.zero_()\n",
        "\n",
        "  def _make_dense(self, n_channels, growth_rate, n_dense_blocks, bottleneck):\n",
        "    layers = []\n",
        "    for _ in range(int(n_dense_blocks)):\n",
        "      if bottleneck:\n",
        "        layers.append(Bottleneck(n_channels, growth_rate))\n",
        "      else:\n",
        "        layers.append(SingleLayer(n_channels, growth_rate))\n",
        "      n_channels += growth_rate\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x, rf=False, _eval=False):\n",
        "    if _eval:\n",
        "        # switch to eval mode\n",
        "        self.eval()\n",
        "    else:\n",
        "        self.train()\n",
        "    out = self.conv1(x)\n",
        "    out = self.trans1(self.dense1(out))\n",
        "    out = self.trans2(self.dense2(out))\n",
        "    out = self.dense3(out)\n",
        "    out = torch.squeeze(F.avg_pool2d(F.relu(self.bn1(out)), 8))\n",
        "    y = self.fc(out)\n",
        "    if rf:\n",
        "        return out, y\n",
        "    return y\n",
        "\n",
        "def densenet(growth_rate=12, depth=40, num_classes=10):\n",
        "  model = DenseNet(growth_rate, depth, 1., num_classes, False)\n",
        "  return model"
      ],
      "metadata": {
        "id": "HA6nd4hassc-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"ResNeXt implementation (https://arxiv.org/abs/1611.05431).\"\"\"\n",
        "import math\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class ResNeXtBottleneck(nn.Module):\n",
        "  \"\"\"ResNeXt Bottleneck Block type C (https://github.com/facebookresearch/ResNeXt/blob/master/models/resnext.lua).\"\"\"\n",
        "  expansion = 4\n",
        "\n",
        "  def __init__(self,\n",
        "               inplanes,\n",
        "               planes,\n",
        "               cardinality,\n",
        "               base_width,\n",
        "               stride=1,\n",
        "               downsample=None):\n",
        "    super(ResNeXtBottleneck, self).__init__()\n",
        "\n",
        "    dim = int(math.floor(planes * (base_width / 64.0)))\n",
        "\n",
        "    self.conv_reduce = nn.Conv2d(\n",
        "        inplanes,\n",
        "        dim * cardinality,\n",
        "        kernel_size=1,\n",
        "        stride=1,\n",
        "        padding=0,\n",
        "        bias=False)\n",
        "    self.bn_reduce = nn.BatchNorm2d(dim * cardinality)\n",
        "\n",
        "    self.conv_conv = nn.Conv2d(\n",
        "        dim * cardinality,\n",
        "        dim * cardinality,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=1,\n",
        "        groups=cardinality,\n",
        "        bias=False)\n",
        "    self.bn = nn.BatchNorm2d(dim * cardinality)\n",
        "\n",
        "    self.conv_expand = nn.Conv2d(\n",
        "        dim * cardinality,\n",
        "        planes * 4,\n",
        "        kernel_size=1,\n",
        "        stride=1,\n",
        "        padding=0,\n",
        "        bias=False)\n",
        "    self.bn_expand = nn.BatchNorm2d(planes * 4)\n",
        "\n",
        "    self.downsample = downsample\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "\n",
        "    bottleneck = self.conv_reduce(x)\n",
        "    bottleneck = F.relu(self.bn_reduce(bottleneck), inplace=True)\n",
        "\n",
        "    bottleneck = self.conv_conv(bottleneck)\n",
        "    bottleneck = F.relu(self.bn(bottleneck), inplace=True)\n",
        "\n",
        "    bottleneck = self.conv_expand(bottleneck)\n",
        "    bottleneck = self.bn_expand(bottleneck)\n",
        "\n",
        "    if self.downsample is not None:\n",
        "      residual = self.downsample(x)\n",
        "\n",
        "    return F.relu(residual + bottleneck, inplace=True)\n",
        "\n",
        "\n",
        "class CifarResNeXt(nn.Module):\n",
        "  \"\"\"ResNext optimized for the Cifar dataset, as specified in https://arxiv.org/pdf/1611.05431.pdf.\"\"\"\n",
        "\n",
        "  def __init__(self, block, depth, cardinality, base_width, num_classes):\n",
        "    super(CifarResNeXt, self).__init__()\n",
        "\n",
        "    # Model type specifies number of layers for CIFAR-10 and CIFAR-100 model\n",
        "    assert (depth - 2) % 9 == 0, 'depth should be one of 29, 38, 47, 56, 101'\n",
        "    layer_blocks = (depth - 2) // 9\n",
        "\n",
        "    self.cardinality = cardinality\n",
        "    self.base_width = base_width\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "    self.conv_1_3x3 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
        "    self.bn_1 = nn.BatchNorm2d(64)\n",
        "\n",
        "    self.inplanes = 64\n",
        "    self.stage_1 = self._make_layer(block, 64, layer_blocks, 1)\n",
        "    self.stage_2 = self._make_layer(block, 128, layer_blocks, 2)\n",
        "    self.stage_3 = self._make_layer(block, 256, layer_blocks, 2)\n",
        "    self.avgpool = nn.AvgPool2d(8)\n",
        "    self.classifier = nn.Linear(256 * block.expansion, num_classes)\n",
        "\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        init.kaiming_normal(m.weight)\n",
        "        m.bias.data.zero_()\n",
        "\n",
        "  def _make_layer(self, block, planes, blocks, stride=1):\n",
        "    downsample = None\n",
        "    if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "      downsample = nn.Sequential(\n",
        "          nn.Conv2d(\n",
        "              self.inplanes,\n",
        "              planes * block.expansion,\n",
        "              kernel_size=1,\n",
        "              stride=stride,\n",
        "              bias=False),\n",
        "          nn.BatchNorm2d(planes * block.expansion),\n",
        "      )\n",
        "\n",
        "    layers = []\n",
        "    layers.append(\n",
        "        block(self.inplanes, planes, self.cardinality, self.base_width, stride,\n",
        "              downsample))\n",
        "    self.inplanes = planes * block.expansion\n",
        "    for _ in range(1, blocks):\n",
        "      layers.append(\n",
        "          block(self.inplanes, planes, self.cardinality, self.base_width))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x, rf=False, _eval=False):\n",
        "    if _eval:\n",
        "        # switch to eval mode\n",
        "        self.eval()\n",
        "    else:\n",
        "        self.train()\n",
        "    x = self.conv_1_3x3(x)\n",
        "    x = F.relu(self.bn_1(x), inplace=True)\n",
        "    x = self.stage_1(x)\n",
        "    x = self.stage_2(x)\n",
        "    x = self.stage_3(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    y = self.classifier(x)\n",
        "\n",
        "    if rf:\n",
        "        return x, y\n",
        "    return y\n",
        "\n",
        "def resnext29(num_classes=10, cardinality=4, base_width=32):\n",
        "  model = CifarResNeXt(ResNeXtBottleneck, 29, cardinality, base_width,\n",
        "                       num_classes)\n",
        "  return model"
      ],
      "metadata": {
        "id": "FLqt2DoJsvPZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"WideResNet implementation (https://arxiv.org/abs/1605.07146).\"\"\"\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "  \"\"\"Basic ResNet block.\"\"\"\n",
        "\n",
        "  def __init__(self, in_planes, out_planes, stride, drop_rate=0.0):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "    self.relu1 = nn.ReLU(inplace=True)\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=1,\n",
        "        bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(out_planes)\n",
        "    self.relu2 = nn.ReLU(inplace=True)\n",
        "    self.conv2 = nn.Conv2d(\n",
        "        out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.drop_rate = drop_rate\n",
        "    self.is_in_equal_out = (in_planes == out_planes)\n",
        "    self.conv_shortcut = (not self.is_in_equal_out) and nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=1,\n",
        "        stride=stride,\n",
        "        padding=0,\n",
        "        bias=False) or None\n",
        "\n",
        "  def forward(self, x):\n",
        "    if not self.is_in_equal_out:\n",
        "      x = self.relu1(self.bn1(x))\n",
        "    else:\n",
        "      out = self.relu1(self.bn1(x))\n",
        "    if self.is_in_equal_out:\n",
        "      out = self.relu2(self.bn2(self.conv1(out)))\n",
        "    else:\n",
        "      out = self.relu2(self.bn2(self.conv1(x)))\n",
        "    if self.drop_rate > 0:\n",
        "      out = F.dropout(out, p=self.drop_rate, training=self.training)\n",
        "    out = self.conv2(out)\n",
        "    if not self.is_in_equal_out:\n",
        "      return torch.add(self.conv_shortcut(x), out)\n",
        "    else:\n",
        "      return torch.add(x, out)\n",
        "\n",
        "\n",
        "class NetworkBlock(nn.Module):\n",
        "  \"\"\"Layer container for blocks.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               nb_layers,\n",
        "               in_planes,\n",
        "               out_planes,\n",
        "               block,\n",
        "               stride,\n",
        "               drop_rate=0.0):\n",
        "    super(NetworkBlock, self).__init__()\n",
        "    self.layer = self._make_layer(block, in_planes, out_planes, nb_layers,\n",
        "                                  stride, drop_rate)\n",
        "\n",
        "  def _make_layer(self, block, in_planes, out_planes, nb_layers, stride,\n",
        "                  drop_rate):\n",
        "    layers = []\n",
        "    for i in range(nb_layers):\n",
        "      layers.append(\n",
        "          block(i == 0 and in_planes or out_planes, out_planes,\n",
        "                i == 0 and stride or 1, drop_rate))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer(x)\n",
        "\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "  \"\"\"WideResNet class.\"\"\"\n",
        "\n",
        "  def __init__(self, depth, num_classes, widen_factor=1, drop_rate=0.0):\n",
        "    super(WideResNet, self).__init__()\n",
        "    n_channels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n",
        "    assert (depth - 4) % 6 == 0\n",
        "    n = (depth - 4) // 6\n",
        "    block = BasicBlock\n",
        "    # 1st conv before any network block\n",
        "    self.conv1 = nn.Conv2d(\n",
        "        3, n_channels[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    # 1st block\n",
        "    self.block1 = NetworkBlock(n, n_channels[0], n_channels[1], block, 1,\n",
        "                               drop_rate)\n",
        "    # 2nd block\n",
        "    self.block2 = NetworkBlock(n, n_channels[1], n_channels[2], block, 2,\n",
        "                               drop_rate)\n",
        "    # 3rd block\n",
        "    self.block3 = NetworkBlock(n, n_channels[2], n_channels[3], block, 2,\n",
        "                               drop_rate)\n",
        "    # global average pooling and classifier\n",
        "    self.bn1 = nn.BatchNorm2d(n_channels[3])\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.fc = nn.Linear(n_channels[3], num_classes)\n",
        "    self.n_channels = n_channels[3]\n",
        "\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        m.bias.data.zero_()\n",
        "\n",
        "  def forward(self, x, rf=False, _eval=False):\n",
        "    if _eval:\n",
        "        # switch to eval mode\n",
        "        self.eval()\n",
        "    else:\n",
        "        self.train()\n",
        "\n",
        "    out = self.conv1(x)\n",
        "    out = self.block1(out)\n",
        "    out = self.block2(out)\n",
        "    out = self.block3(out)\n",
        "    out = self.relu(self.bn1(out))\n",
        "    out = F.avg_pool2d(out, 8)\n",
        "    out = out.view(-1, self.n_channels)\n",
        "    y =  self.fc(out)\n",
        "\n",
        "    if rf:\n",
        "        return out, y\n",
        "    return y"
      ],
      "metadata": {
        "id": "4gfUmcamsxhr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import os.path as osp\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch.fft\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def test(net, criterion, testloader, outloader, attack=None, epoch=None, **options):\n",
        "    net.eval()\n",
        "    correct, total, adv_correct = 0, 0, 0\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    _pred_k, _pred_u, _labels = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, labels in testloader:\n",
        "            if options['use_gpu']:\n",
        "                data, labels = data.cuda(), labels.cuda()             \n",
        "                data = normalize(data)\n",
        "                logits = net(data, _eval=True)\n",
        "                predictions = logits.data.max(1)[1]\n",
        "                total += labels.size(0)\n",
        "                correct += (predictions == labels.data).sum()\n",
        "            \n",
        "                _pred_k.append(logits.data.cpu().numpy())\n",
        "                _labels.append(labels.data.cpu().numpy())\n",
        "\n",
        "        for batch_idx, (data, labels) in enumerate(outloader):\n",
        "            if options['use_gpu']:\n",
        "                data, labels = data.cuda(), labels.cuda()\n",
        "                data = normalize(data)\n",
        "            with torch.set_grad_enabled(False):\n",
        "                logits = net(data, _eval=True)\n",
        "                _pred_u.append(logits.data.cpu().numpy())\n",
        "\n",
        "    _pred_k = np.concatenate(_pred_k, 0)\n",
        "    _pred_u = np.concatenate(_pred_u, 0)\n",
        "    _labels = np.concatenate(_labels, 0)\n",
        "    \n",
        "    # # Out-of-Distribution detction evaluation\n",
        "    x1, x2 = np.max(_pred_k, axis=1), np.max(_pred_u, axis=1)\n",
        "    results = metric_ood(x1, x2)['Bas']\n",
        "\n",
        "    # Accuracy\n",
        "    acc = float(correct) * 100. / float(total)\n",
        "    results['ACC'] = acc\n",
        "\n",
        "    print('Acc: {:.5f}'.format(acc))\n",
        "\n",
        "    return results\n",
        "\n",
        "def test_robustness(net, criterion, testloader, epoch=None, label='', **options):\n",
        "    net.eval()\n",
        "    results = dict()\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, labels in testloader:\n",
        "            if options['use_gpu']:\n",
        "                data, labels = data.cuda(), labels.cuda()\n",
        "                data = normalize(data)\n",
        "            with torch.set_grad_enabled(False):\n",
        "                logits = net(data, _eval=True)\n",
        "                predictions = logits.data.max(1)[1]\n",
        "                total += labels.size(0)\n",
        "                correct += (predictions == labels.data).sum()\n",
        "\n",
        "    # Accuracy\n",
        "    acc = float(correct) * 100. / float(total)\n",
        "    results['ACC'] = acc\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "tbsgoVV10myW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "def get_curve_online(known, novel, stypes = ['Bas']):\n",
        "    tp, fp = dict(), dict()\n",
        "    tnr_at_tpr95 = dict()\n",
        "    for stype in stypes:\n",
        "        known.sort()\n",
        "        novel.sort()\n",
        "        end = np.max([np.max(known), np.max(novel)])\n",
        "        start = np.min([np.min(known),np.min(novel)])\n",
        "        num_k = known.shape[0]\n",
        "        num_n = novel.shape[0]\n",
        "        tp[stype] = -np.ones([num_k+num_n+1], dtype=int)\n",
        "        fp[stype] = -np.ones([num_k+num_n+1], dtype=int)\n",
        "        tp[stype][0], fp[stype][0] = num_k, num_n\n",
        "        k, n = 0, 0\n",
        "        for l in range(num_k+num_n):\n",
        "            if k == num_k:\n",
        "                tp[stype][l+1:] = tp[stype][l]\n",
        "                fp[stype][l+1:] = np.arange(fp[stype][l]-1, -1, -1)\n",
        "                break\n",
        "            elif n == num_n:\n",
        "                tp[stype][l+1:] = np.arange(tp[stype][l]-1, -1, -1)\n",
        "                fp[stype][l+1:] = fp[stype][l]\n",
        "                break\n",
        "            else:\n",
        "                if novel[n] < known[k]:\n",
        "                    n += 1\n",
        "                    tp[stype][l+1] = tp[stype][l]\n",
        "                    fp[stype][l+1] = fp[stype][l] - 1\n",
        "                else:\n",
        "                    k += 1\n",
        "                    tp[stype][l+1] = tp[stype][l] - 1\n",
        "                    fp[stype][l+1] = fp[stype][l]\n",
        "        tpr95_pos = np.abs(tp[stype] / num_k - .95).argmin()\n",
        "        tnr_at_tpr95[stype] = 1. - fp[stype][tpr95_pos] / num_n\n",
        "    return tp, fp, tnr_at_tpr95\n",
        "\n",
        "def metric_ood(x1, x2, stypes = ['Bas'], verbose=True):\n",
        "    tp, fp, tnr_at_tpr95 = get_curve_online(x1, x2, stypes)\n",
        "    results = dict()\n",
        "    mtypes = ['TNR', 'AUROC', 'DTACC', 'AUIN', 'AUOUT']\n",
        "    if verbose:\n",
        "        print('      ', end='')\n",
        "        for mtype in mtypes:\n",
        "            print(' {mtype:6s}'.format(mtype=mtype), end='')\n",
        "        print('')\n",
        "        \n",
        "    for stype in stypes:\n",
        "        if verbose:\n",
        "            print('{stype:5s} '.format(stype=stype), end='')\n",
        "        results[stype] = dict()\n",
        "        \n",
        "        # TNR\n",
        "        mtype = 'TNR'\n",
        "        results[stype][mtype] = 100.*tnr_at_tpr95[stype]\n",
        "        if verbose:\n",
        "            print(' {val:6.3f}'.format(val=results[stype][mtype]), end='')\n",
        "        \n",
        "        # AUROC\n",
        "        mtype = 'AUROC'\n",
        "        tpr = np.concatenate([[1.], tp[stype]/tp[stype][0], [0.]])\n",
        "        fpr = np.concatenate([[1.], fp[stype]/fp[stype][0], [0.]])\n",
        "        results[stype][mtype] = 100.*(-np.trapz(1.-fpr, tpr))\n",
        "        if verbose:\n",
        "            print(' {val:6.3f}'.format(val=results[stype][mtype]), end='')\n",
        "        \n",
        "        # DTACC\n",
        "        mtype = 'DTACC'\n",
        "        results[stype][mtype] = 100.*(.5 * (tp[stype]/tp[stype][0] + 1.-fp[stype]/fp[stype][0]).max())\n",
        "        if verbose:\n",
        "            print(' {val:6.3f}'.format(val=results[stype][mtype]), end='')\n",
        "        \n",
        "        # AUIN\n",
        "        mtype = 'AUIN'\n",
        "        denom = tp[stype]+fp[stype]\n",
        "        denom[denom == 0.] = -1.\n",
        "        pin_ind = np.concatenate([[True], denom > 0., [True]])\n",
        "        pin = np.concatenate([[.5], tp[stype]/denom, [0.]])\n",
        "        results[stype][mtype] = 100.*(-np.trapz(pin[pin_ind], tpr[pin_ind]))\n",
        "        if verbose:\n",
        "            print(' {val:6.3f}'.format(val=results[stype][mtype]), end='')\n",
        "        \n",
        "        # AUOUT\n",
        "        mtype = 'AUOUT'\n",
        "        denom = tp[stype][0]-tp[stype]+fp[stype][0]-fp[stype]\n",
        "        denom[denom == 0.] = -1.\n",
        "        pout_ind = np.concatenate([[True], denom > 0., [True]])\n",
        "        pout = np.concatenate([[0.], (fp[stype][0]-fp[stype])/denom, [.5]])\n",
        "        results[stype][mtype] = 100.*(np.trapz(pout[pout_ind], 1.-fpr[pout_ind]))\n",
        "        if verbose:\n",
        "            print(' {val:6.3f}'.format(val=results[stype][mtype]), end='')\n",
        "            print('')\n",
        "    \n",
        "    return results\n",
        "\n",
        "def compute_oscr(pred_k, pred_u, labels):\n",
        "    x1, x2 = np.max(pred_k, axis=1), np.max(pred_u, axis=1)\n",
        "    pred = np.argmax(pred_k, axis=1)\n",
        "    correct = (pred == labels)\n",
        "    m_x1 = np.zeros(len(x1))\n",
        "    m_x1[pred == labels] = 1\n",
        "    k_target = np.concatenate((m_x1, np.zeros(len(x2))), axis=0)\n",
        "    u_target = np.concatenate((np.zeros(len(x1)), np.ones(len(x2))), axis=0)\n",
        "    predict = np.concatenate((x1, x2), axis=0)\n",
        "    n = len(predict)\n",
        "\n",
        "    # Cutoffs are of prediction values\n",
        "    \n",
        "    CCR = [0 for x in range(n+2)]\n",
        "    FPR = [0 for x in range(n+2)] \n",
        "\n",
        "    idx = predict.argsort()\n",
        "\n",
        "    s_k_target = k_target[idx]\n",
        "    s_u_target = u_target[idx]\n",
        "\n",
        "    for k in range(n-1):\n",
        "        CC = s_k_target[k+1:].sum()\n",
        "        FP = s_u_target[k:].sum()\n",
        "\n",
        "        # True\tPositive Rate\n",
        "        CCR[k] = float(CC) / float(len(x1))\n",
        "        # False Positive Rate\n",
        "        FPR[k] = float(FP) / float(len(x2))\n",
        "\n",
        "    CCR[n] = 0.0\n",
        "    FPR[n] = 0.0\n",
        "    CCR[n+1] = 1.0\n",
        "    FPR[n+1] = 1.0\n",
        "\n",
        "    # Positions of ROC curve (FPR, TPR)\n",
        "    ROC = sorted(zip(FPR, CCR), reverse=True)\n",
        "\n",
        "    OSCR = 0\n",
        "\n",
        "    # Compute AUROC Using Trapezoidal Rule\n",
        "    for j in range(n+1):\n",
        "        h =   ROC[j][0] - ROC[j+1][0]\n",
        "        w =  (ROC[j][1] + ROC[j+1][1]) / 2.0\n",
        "\n",
        "        OSCR = OSCR + h*w\n",
        "\n",
        "    return OSCR\n"
      ],
      "metadata": {
        "id": "NC3UcZuv0wOh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mkdir_if_missing(directory):\n",
        "    if not osp.exists(directory):\n",
        "        try:\n",
        "            os.makedirs(directory)\n",
        "        except OSError as e:\n",
        "            if e.errno != errno.EEXIST:\n",
        "                raise\n",
        "def save_networks(networks, result_dir, name='', loss='', criterion=None):\n",
        "    mkdir_if_missing(osp.join(result_dir, 'checkpoints'))\n",
        "    weights = networks.state_dict()\n",
        "    filename = '{}/checkpoints/{}_{}.pth'.format(result_dir, name, loss)\n",
        "    torch.save(weights, filename)\n",
        "    if criterion:\n",
        "        weights = criterion.state_dict()\n",
        "        filename = '{}/checkpoints/{}_{}_criterion.pth'.format(result_dir, name, loss)\n",
        "        torch.save(weights, filename)\n",
        "\n",
        "def load_networks(networks, result_dir, name='', loss='', criterion=None):\n",
        "    weights = networks.state_dict()\n",
        "    filename = '{}/checkpoints/{}_{}.pth'.format(result_dir, name, loss)\n",
        "    networks.load_state_dict(torch.load(filename))\n",
        "    if criterion:\n",
        "        weights = criterion.state_dict()\n",
        "        filename = '{}/checkpoints/{}_{}_criterion.pth'.format(result_dir, name, loss)\n",
        "        criterion.load_state_dict(torch.load(filename))\n",
        "\n",
        "    return networks, criterion"
      ],
      "metadata": {
        "id": "PJYT7Nd10ALr"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def mix_data(x, use_cuda=True, prob=0.6):\n",
        "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
        "\n",
        "    p = random.uniform(0, 1)\n",
        "\n",
        "    if p > prob:\n",
        "        return x\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    if use_cuda:\n",
        "        index = torch.randperm(batch_size).cuda()\n",
        "    else:\n",
        "        index = torch.randperm(batch_size)\n",
        "\n",
        "    fft_1 = torch.fft.fftn(x, dim=(1,2,3))\n",
        "    abs_1, angle_1 = torch.abs(fft_1), torch.angle(fft_1)\n",
        "\n",
        "    fft_2 = torch.fft.fftn(x[index, :], dim=(1,2,3))\n",
        "    abs_2, angle_2 = torch.abs(fft_2), torch.angle(fft_2)\n",
        "\n",
        "    fft_1 = abs_2*torch.exp((1j) * angle_1)\n",
        "\n",
        "    mixed_x = torch.fft.ifftn(fft_1, dim=(1,2,3)).float()\n",
        "\n",
        "    return mixed_x\n",
        "\n",
        "\n",
        "def train(net, criterion, optimizer, trainloader, epoch=None, **options):\n",
        "    net.train()\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    loss_all = 0\n",
        "    for batch_idx, (data, labels) in enumerate(trainloader):\n",
        "        if options['use_gpu']:\n",
        "            inputs, targets = data.cuda(), labels.cuda()\n",
        "\n",
        "        inputs_mix = mix_data(inputs)\n",
        "        inputs_mix = Variable(inputs_mix)\n",
        "        batch_size = inputs.size(0)\n",
        "        inputs, inputs_mix = normalize(inputs), normalize(inputs_mix)\n",
        "\n",
        "        inputs = torch.cat([inputs, inputs_mix], 0)\n",
        "\n",
        "        with torch.set_grad_enabled(True):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            _, y = net(inputs, True)\n",
        "            loss = criterion(y[:batch_size], targets) + criterion(y[batch_size:], targets)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        losses.update(loss.item(), targets.size(0))\n",
        "\n",
        "        if (batch_idx+1) % options['print_freq'] == 0:\n",
        "            print(\"Batch {}/{}\\t Loss {:.6f} ({:.6f})\" \\\n",
        "                  .format(batch_idx+1, len(trainloader), losses.val, losses.avg))\n",
        "        \n",
        "        loss_all += losses.avg\n",
        "\n",
        "    return loss_all"
      ],
      "metadata": {
        "id": "vUp5pEUm2Xk1"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value.\n",
        "       \n",
        "       Code imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "class Logger(object):\n",
        "    \"\"\"\n",
        "    Write console output to external text file.\n",
        "    \n",
        "    Code imported from https://github.com/Cysu/open-reid/blob/master/reid/utils/logging.py.\n",
        "    \"\"\"\n",
        "    def __init__(self, fpath=None):\n",
        "        self.console = sys.stdout\n",
        "        self.file = None\n",
        "        if fpath is not None:\n",
        "            mkdir_if_missing(os.path.dirname(fpath))\n",
        "            self.file = open(fpath, 'w')\n",
        "\n",
        "    def __del__(self):\n",
        "        self.close()\n",
        "\n",
        "    def __enter__(self):\n",
        "        pass\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        self.close()\n",
        "\n",
        "    def write(self, msg):\n",
        "        self.console.write(msg)\n",
        "        if self.file is not None:\n",
        "            self.file.write(msg)\n",
        "\n",
        "    def flush(self):\n",
        "        self.console.flush()\n",
        "        if self.file is not None:\n",
        "            self.file.flush()\n",
        "            os.fsync(self.file.fileno())\n",
        "\n",
        "    def close(self):\n",
        "        self.console.close()\n",
        "        if self.file is not None:\n",
        "            self.file.close()"
      ],
      "metadata": {
        "id": "8laTrZuZ2eJc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./outf\n",
        "!mkdir ./data\n",
        "\n",
        "!wget https://zenodo.org/record/2535967/files/CIFAR-10-C.tar"
      ],
      "metadata": {
        "id": "_TJg-jZAhwmG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77934a05-33c7-4d0c-e7a8-7c93b064768a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ./outf: File exists\n",
            "mkdir: cannot create directory ./data: File exists\n",
            "--2023-04-24 11:22:32--  https://zenodo.org/record/2535967/files/CIFAR-10-C.tar\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.124.72\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.124.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2918471680 (2.7G) [application/octet-stream]\n",
            "Saving to: CIFAR-10-C.tar.1\n",
            "\n",
            "CIFAR-10-C.tar.1    100%[===================>]   2.72G  2.12MB/s    in 26m 25s \n",
            "\n",
            "2023-04-24 11:48:58 (1.76 MB/s) - CIFAR-10-C.tar.1 saved [2918471680/2918471680]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvf CIFAR-10-C.tar -C ./data"
      ],
      "metadata": {
        "id": "w5gRM-dwntwX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a52ce32-20a7-45f8-d02e-ea5b784b3086"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIFAR-10-C/\n",
            "CIFAR-10-C/fog.npy\n",
            "CIFAR-10-C/jpeg_compression.npy\n",
            "CIFAR-10-C/zoom_blur.npy\n",
            "CIFAR-10-C/speckle_noise.npy\n",
            "CIFAR-10-C/glass_blur.npy\n",
            "CIFAR-10-C/spatter.npy\n",
            "CIFAR-10-C/shot_noise.npy\n",
            "CIFAR-10-C/defocus_blur.npy\n",
            "CIFAR-10-C/elastic_transform.npy\n",
            "CIFAR-10-C/gaussian_blur.npy\n",
            "CIFAR-10-C/frost.npy\n",
            "CIFAR-10-C/saturate.npy\n",
            "CIFAR-10-C/brightness.npy\n",
            "CIFAR-10-C/snow.npy\n",
            "CIFAR-10-C/gaussian_noise.npy\n",
            "CIFAR-10-C/motion_blur.npy\n",
            "CIFAR-10-C/contrast.npy\n",
            "CIFAR-10-C/impulse_noise.npy\n",
            "CIFAR-10-C/labels.npy\n",
            "CIFAR-10-C/pixelate.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ./data/CIFAR-10-C/"
      ],
      "metadata": {
        "id": "3J0OI71FmZwy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cf51cae-0f75-41bd-8ef0-66f96c600163"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "brightness.npy\t       gaussian_noise.npy    saturate.npy\n",
            "contrast.npy\t       glass_blur.npy\t     shot_noise.npy\n",
            "defocus_blur.npy       impulse_noise.npy     snow.npy\n",
            "elastic_transform.npy  jpeg_compression.npy  spatter.npy\n",
            "fog.npy\t\t       labels.npy\t     speckle_noise.npy\n",
            "frost.npy\t       motion_blur.npy\t     zoom_blur.npy\n",
            "gaussian_blur.npy      pixelate.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "options = {\n",
        "    'data': './data',\n",
        "    'outf': './results',\n",
        "    'dataset': 'cifar10',\n",
        "    'workers': 8,\n",
        "    'batch_size': 16,\n",
        "    'lr': 0.1,\n",
        "    'max_epoch': 30,\n",
        "    'stepsize': 10,\n",
        "    'aug': 'none',\n",
        "    'model': 'resnet18',\n",
        "    'eval_freq': 5,\n",
        "    'print_freq': 100,\n",
        "    'gpu': '0',\n",
        "    'seed': 0,\n",
        "    'use_cpu': False,\n",
        "    'eval': True,\n",
        "    'epsilon': 0.0157,\n",
        "    'alpha': 0.00784,\n",
        "    'k': 10,\n",
        "    'perturbation_type': 'linf'\n",
        "}\n"
      ],
      "metadata": {
        "id": "kpqP-MrH0Ald"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'cifar10' == options['dataset']:\n",
        "        Data = CIFAR10D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['aug'], _eval=options['eval'])\n",
        "        OODData = CIFAR100D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['aug'])\n",
        "else:\n",
        "        Data = CIFAR100D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['aug'], _eval=options['eval'])\n",
        "        OODData = CIFAR10D(dataroot=options['data'], batch_size=options['batch_size'], _transforms=options['aug'])"
      ],
      "metadata": {
        "id": "PsH_MKlSjd-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fd773c6-790c-4daf-e2ef-4081dc278ba2"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader, testloader, outloader = Data.train_loader, Data.test_loader, OODData.test_loader\n",
        "num_classes = Data.num_classes\n",
        "\n",
        "\n",
        "'''ResNet in PyTorch.\n",
        "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, rf=False, _eval=False):\n",
        "        if _eval:\n",
        "            self.eval()\n",
        "        else:\n",
        "            self.train()\n",
        "\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        y = self.linear(out)\n",
        "        if rf:\n",
        "            return out, y\n",
        "        return y\n",
        "\n",
        "\n",
        "def ResNet18(num_classes):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])"
      ],
      "metadata": {
        "id": "cGGLtRXoobo1"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating model: {}\".format(options['model']))\n",
        "if 'wide_resnet' in options['model']:\n",
        "        print('wide_resnet')\n",
        "        net = WideResNet(40, num_classes, 2, 0.0)\n",
        "elif 'allconv' in options['model']:\n",
        "        print('allconv')\n",
        "        net = AllConvNet(num_classes)\n",
        "elif 'densenet' in options['model']:\n",
        "        print('densenet')\n",
        "        net = densenet(num_classes=num_classes)\n",
        "elif 'resnext' in options['model']:\n",
        "        print('resnext29')\n",
        "        net = resnext29(num_classes)\n",
        "else:\n",
        "        print('resnet18')\n",
        "        net = ResNet18(num_classes=num_classes)\n"
      ],
      "metadata": {
        "id": "4NmhUnj5opN6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8e95ee2-43c5-4aa8-cb5b-c9dc99ce608e"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model: resnet18\n",
            "resnet18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(options['seed'])\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = options['gpu']\n",
        "use_gpu = torch.cuda.is_available()\n",
        "if options['use_cpu']: use_gpu = False\n",
        "\n",
        "options.update({'use_gpu': use_gpu})\n",
        "options['use_gpu']"
      ],
      "metadata": {
        "id": "c_XXanTQdxDh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e55d84bd-8ff4-4d23-8b4f-d2d28b448469"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "if use_gpu:\n",
        "        net = nn.DataParallel(net, device_ids=[i for i in range(len(options['gpu'].split(',')))]).cuda()\n",
        "        criterion = criterion.cuda()"
      ],
      "metadata": {
        "id": "UJ8nlpIcpzci"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = '{}_{}_{}'.format(options['model'], options['dataset'], options['aug'])\n"
      ],
      "metadata": {
        "id": "-3eUpU7ztjaM"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_list = [{'params': net.parameters()},\n",
        "                {'params': criterion.parameters()}]"
      ],
      "metadata": {
        "id": "OJlKwuxb04v-"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(params_list, lr=options['lr'], momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
        "scheduler = lr_scheduler.MultiStepLR(optimizer, gamma=0.2, milestones=[60, 120, 160, 190])\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "best_acc = 0.0"
      ],
      "metadata": {
        "id": "y9Ho8WZQ08LG"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if options['eval']:\n",
        "        net, criterion = load_networks(net, options['outf'], file_name, criterion=criterion)\n",
        "        outloaders = Data.out_loaders\n",
        "        results = test(net, criterion, testloader, outloader, epoch=0, **options)\n",
        "        acc = results['ACC']\n",
        "        res = dict()\n",
        "        res['ACC'] = dict()\n",
        "        acc_res = []\n",
        "        for key in Data.out_keys:\n",
        "            results = test_robustness(net, criterion, outloaders[key], epoch=0, label=key, **options)\n",
        "            print('{} (%): {:.3f}\\t'.format(key, results['ACC']))\n",
        "            res['ACC'][key] = results['ACC']\n",
        "            acc_res.append(results['ACC'])\n",
        "        print('Mean ACC:', np.mean(acc_res))\n",
        "        print('Mean Error:', 100-np.mean(acc_res))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xM2jfHK6Zgeb",
        "outputId": "905501fb-ee93-4ba9-a4e0-70ecde90b791"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       TNR    AUROC  DTACC  AUIN   AUOUT \n",
            "Bas    14.670 71.406 65.965 73.478 67.887\n",
            "Acc: 68.43000\n",
            "gaussian_noise (%): 45.520\t\n",
            "shot_noise (%): 50.442\t\n",
            "impulse_noise (%): 35.790\t\n",
            "defocus_blur (%): 58.310\t\n",
            "glass_blur (%): 33.034\t\n",
            "motion_blur (%): 52.198\t\n",
            "zoom_blur (%): 49.986\t\n",
            "snow (%): 57.924\t\n",
            "frost (%): 58.284\t\n",
            "fog (%): 58.874\t\n",
            "brightness (%): 67.894\t\n",
            "contrast (%): 43.844\t\n",
            "elastic_transform (%): 58.122\t\n",
            "pixelate (%): 51.158\t\n",
            "jpeg_compression (%): 56.090\t\n",
            "Mean ACC: 51.83133333333334\n",
            "Mean Error: 48.16866666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(options['max_epoch']):\n",
        "        print(\"==> Epoch {}/{}\".format(epoch+1, options['max_epoch']))\n",
        "\n",
        "        train(net, criterion, optimizer, trainloader, epoch=epoch, **options)\n",
        "\n",
        "        if options['eval_freq'] > 0 and (epoch+1) % options['eval_freq'] == 0 or (epoch+1) == options['max_epoch'] or epoch > 160:\n",
        "            print(\"==> Test\")\n",
        "            results = test(net, criterion, testloader, outloader, epoch=epoch, **options)\n",
        "\n",
        "            if best_acc < results['ACC']:\n",
        "                best_acc = results['ACC']\n",
        "                print(\"Best Acc (%): {:.3f}\\t\".format(best_acc))\n",
        "            \n",
        "            save_networks(net, options['outf'], file_name, criterion=criterion)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "elapsed = round(time.time() - start_time)\n",
        "elapsed = str(datetime.timedelta(seconds=elapsed))\n",
        "print(\"Finished. Total elapsed time (h:m:s): {}\".format(elapsed))"
      ],
      "metadata": {
        "id": "IEFaTaqm1r_6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c0d337a-0c49-45ee-a5de-2011be80593d"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Epoch 1/30\n",
            "Batch 100/3125\t Loss 2.496162 (2.129762)\n",
            "Batch 200/3125\t Loss 1.493688 (2.116169)\n",
            "Batch 300/3125\t Loss 2.118077 (2.112234)\n",
            "Batch 400/3125\t Loss 2.804455 (2.136117)\n",
            "Batch 500/3125\t Loss 1.887927 (2.146888)\n",
            "Batch 600/3125\t Loss 3.462397 (2.164925)\n",
            "Batch 700/3125\t Loss 4.371687 (2.149053)\n",
            "Batch 800/3125\t Loss 2.991106 (2.149567)\n",
            "Batch 900/3125\t Loss 1.010136 (2.145279)\n",
            "Batch 1000/3125\t Loss 2.488120 (2.148970)\n",
            "Batch 1100/3125\t Loss 1.706316 (2.155561)\n",
            "Batch 1200/3125\t Loss 2.280683 (2.162405)\n",
            "Batch 1300/3125\t Loss 1.987024 (2.164641)\n",
            "Batch 1400/3125\t Loss 2.892583 (2.174140)\n",
            "Batch 1500/3125\t Loss 1.858281 (2.172696)\n",
            "Batch 1600/3125\t Loss 2.961724 (2.172900)\n",
            "Batch 1700/3125\t Loss 3.072594 (2.170911)\n",
            "Batch 1800/3125\t Loss 1.679527 (2.173720)\n",
            "Batch 1900/3125\t Loss 2.273516 (2.170565)\n",
            "Batch 2000/3125\t Loss 1.993241 (2.169729)\n",
            "Batch 2100/3125\t Loss 1.312725 (2.167946)\n",
            "Batch 2200/3125\t Loss 1.986657 (2.169276)\n",
            "Batch 2300/3125\t Loss 2.806121 (2.174119)\n",
            "Batch 2400/3125\t Loss 2.505670 (2.174956)\n",
            "Batch 2500/3125\t Loss 2.080786 (2.176962)\n",
            "Batch 2600/3125\t Loss 1.166665 (2.175872)\n",
            "Batch 2700/3125\t Loss 2.612030 (2.179498)\n",
            "Batch 2800/3125\t Loss 2.435372 (2.178429)\n",
            "Batch 2900/3125\t Loss 1.646208 (2.179997)\n",
            "Batch 3000/3125\t Loss 1.487251 (2.178855)\n",
            "Batch 3100/3125\t Loss 2.213935 (2.178500)\n",
            "==> Epoch 2/30\n",
            "Batch 100/3125\t Loss 0.893412 (2.049987)\n",
            "Batch 200/3125\t Loss 2.062023 (2.066632)\n",
            "Batch 300/3125\t Loss 2.616684 (2.110949)\n",
            "Batch 400/3125\t Loss 2.136756 (2.132957)\n",
            "Batch 500/3125\t Loss 2.173974 (2.128143)\n",
            "Batch 600/3125\t Loss 1.061122 (2.139907)\n",
            "Batch 700/3125\t Loss 1.328839 (2.147295)\n",
            "Batch 800/3125\t Loss 1.880712 (2.154717)\n",
            "Batch 900/3125\t Loss 3.292228 (2.157243)\n",
            "Batch 1000/3125\t Loss 1.828669 (2.157617)\n",
            "Batch 1100/3125\t Loss 2.773446 (2.161803)\n",
            "Batch 1200/3125\t Loss 1.774260 (2.171071)\n",
            "Batch 1300/3125\t Loss 1.258131 (2.165104)\n",
            "Batch 1400/3125\t Loss 2.215724 (2.170886)\n",
            "Batch 1500/3125\t Loss 1.805191 (2.174083)\n",
            "Batch 1600/3125\t Loss 2.519730 (2.174413)\n",
            "Batch 1700/3125\t Loss 2.324843 (2.174207)\n",
            "Batch 1800/3125\t Loss 1.897280 (2.171868)\n",
            "Batch 1900/3125\t Loss 1.821917 (2.173375)\n",
            "Batch 2000/3125\t Loss 1.846031 (2.178658)\n",
            "Batch 2100/3125\t Loss 0.942656 (2.180595)\n",
            "Batch 2200/3125\t Loss 3.514352 (2.181679)\n",
            "Batch 2300/3125\t Loss 3.006962 (2.180116)\n",
            "Batch 2400/3125\t Loss 2.231553 (2.183767)\n",
            "Batch 2500/3125\t Loss 2.374807 (2.183571)\n",
            "Batch 2600/3125\t Loss 2.138398 (2.179188)\n",
            "Batch 2700/3125\t Loss 1.574143 (2.181406)\n",
            "Batch 2800/3125\t Loss 2.750051 (2.179480)\n",
            "Batch 2900/3125\t Loss 1.168145 (2.179444)\n",
            "Batch 3000/3125\t Loss 2.201878 (2.181989)\n",
            "Batch 3100/3125\t Loss 2.828267 (2.180147)\n",
            "==> Epoch 3/30\n",
            "Batch 100/3125\t Loss 2.590794 (2.219933)\n",
            "Batch 200/3125\t Loss 2.304758 (2.188734)\n",
            "Batch 300/3125\t Loss 2.416936 (2.174894)\n",
            "Batch 400/3125\t Loss 2.380250 (2.162241)\n",
            "Batch 500/3125\t Loss 2.457733 (2.165390)\n",
            "Batch 600/3125\t Loss 2.779469 (2.171691)\n",
            "Batch 700/3125\t Loss 1.939536 (2.169776)\n",
            "Batch 800/3125\t Loss 3.005687 (2.162109)\n",
            "Batch 900/3125\t Loss 1.500354 (2.167231)\n",
            "Batch 1000/3125\t Loss 2.439322 (2.167720)\n",
            "Batch 1100/3125\t Loss 1.406110 (2.172132)\n",
            "Batch 1200/3125\t Loss 2.850223 (2.170264)\n",
            "Batch 1300/3125\t Loss 3.416395 (2.170513)\n",
            "Batch 1400/3125\t Loss 2.493876 (2.170341)\n",
            "Batch 1500/3125\t Loss 1.675248 (2.170551)\n",
            "Batch 1600/3125\t Loss 1.463114 (2.171466)\n",
            "Batch 1700/3125\t Loss 3.429330 (2.170005)\n",
            "Batch 1800/3125\t Loss 1.275078 (2.172564)\n",
            "Batch 1900/3125\t Loss 2.094128 (2.172141)\n",
            "Batch 2000/3125\t Loss 2.273751 (2.174156)\n",
            "Batch 2100/3125\t Loss 1.960547 (2.177768)\n",
            "Batch 2200/3125\t Loss 2.151189 (2.175943)\n",
            "Batch 2300/3125\t Loss 1.789583 (2.173430)\n",
            "Batch 2400/3125\t Loss 2.548878 (2.173941)\n",
            "Batch 2500/3125\t Loss 1.684871 (2.175948)\n",
            "Batch 2600/3125\t Loss 2.115355 (2.175988)\n",
            "Batch 2700/3125\t Loss 1.941009 (2.175932)\n",
            "Batch 2800/3125\t Loss 3.250251 (2.172349)\n",
            "Batch 2900/3125\t Loss 3.143172 (2.169632)\n",
            "Batch 3000/3125\t Loss 1.479547 (2.170217)\n",
            "Batch 3100/3125\t Loss 1.462956 (2.168935)\n",
            "==> Epoch 4/30\n",
            "Batch 100/3125\t Loss 2.405369 (2.091934)\n",
            "Batch 200/3125\t Loss 1.083758 (2.148172)\n",
            "Batch 300/3125\t Loss 2.989779 (2.170840)\n",
            "Batch 400/3125\t Loss 1.444013 (2.135199)\n",
            "Batch 500/3125\t Loss 2.794552 (2.143686)\n",
            "Batch 600/3125\t Loss 2.295380 (2.166594)\n",
            "Batch 700/3125\t Loss 1.873144 (2.145694)\n",
            "Batch 800/3125\t Loss 2.565267 (2.148922)\n",
            "Batch 900/3125\t Loss 2.773561 (2.138531)\n",
            "Batch 1000/3125\t Loss 3.086261 (2.132498)\n",
            "Batch 1100/3125\t Loss 3.655208 (2.141493)\n",
            "Batch 1200/3125\t Loss 2.147150 (2.131528)\n",
            "Batch 1300/3125\t Loss 1.918273 (2.140177)\n",
            "Batch 1400/3125\t Loss 1.612259 (2.138035)\n",
            "Batch 1500/3125\t Loss 2.545733 (2.147398)\n",
            "Batch 1600/3125\t Loss 2.541315 (2.145088)\n",
            "Batch 1700/3125\t Loss 1.949285 (2.145009)\n",
            "Batch 1800/3125\t Loss 1.953794 (2.147368)\n",
            "Batch 1900/3125\t Loss 3.042008 (2.147976)\n",
            "Batch 2000/3125\t Loss 1.869262 (2.153324)\n",
            "Batch 2100/3125\t Loss 2.177725 (2.154494)\n",
            "Batch 2200/3125\t Loss 1.226892 (2.156632)\n",
            "Batch 2300/3125\t Loss 3.167763 (2.159687)\n",
            "Batch 2400/3125\t Loss 2.607580 (2.162301)\n",
            "Batch 2500/3125\t Loss 1.831149 (2.162875)\n",
            "Batch 2600/3125\t Loss 2.572311 (2.167597)\n",
            "Batch 2700/3125\t Loss 2.593473 (2.167038)\n",
            "Batch 2800/3125\t Loss 2.687776 (2.167131)\n",
            "Batch 2900/3125\t Loss 2.169121 (2.169945)\n",
            "Batch 3000/3125\t Loss 2.709642 (2.172318)\n",
            "Batch 3100/3125\t Loss 1.921890 (2.175148)\n",
            "==> Epoch 5/30\n",
            "Batch 100/3125\t Loss 2.244081 (2.224735)\n",
            "Batch 200/3125\t Loss 2.033172 (2.238938)\n",
            "Batch 300/3125\t Loss 1.182680 (2.221917)\n",
            "Batch 400/3125\t Loss 1.663734 (2.202834)\n",
            "Batch 500/3125\t Loss 2.339540 (2.205889)\n",
            "Batch 600/3125\t Loss 1.832997 (2.194453)\n",
            "Batch 700/3125\t Loss 2.415045 (2.185052)\n",
            "Batch 800/3125\t Loss 2.689456 (2.195245)\n",
            "Batch 900/3125\t Loss 1.804843 (2.186900)\n",
            "Batch 1000/3125\t Loss 1.134293 (2.182808)\n",
            "Batch 1100/3125\t Loss 2.826892 (2.189909)\n",
            "Batch 1200/3125\t Loss 1.461487 (2.184398)\n",
            "Batch 1300/3125\t Loss 1.619776 (2.170871)\n",
            "Batch 1400/3125\t Loss 1.409884 (2.178518)\n",
            "Batch 1500/3125\t Loss 2.408095 (2.179786)\n",
            "Batch 1600/3125\t Loss 2.950938 (2.179859)\n",
            "Batch 1700/3125\t Loss 2.463944 (2.177068)\n",
            "Batch 1800/3125\t Loss 2.887230 (2.178410)\n",
            "Batch 1900/3125\t Loss 1.993705 (2.177245)\n",
            "Batch 2000/3125\t Loss 3.023828 (2.180448)\n",
            "Batch 2100/3125\t Loss 1.212747 (2.180308)\n",
            "Batch 2200/3125\t Loss 2.690324 (2.181060)\n",
            "Batch 2300/3125\t Loss 1.795824 (2.178209)\n",
            "Batch 2400/3125\t Loss 3.148684 (2.175796)\n",
            "Batch 2500/3125\t Loss 1.597604 (2.177596)\n",
            "Batch 2600/3125\t Loss 0.812456 (2.175321)\n",
            "Batch 2700/3125\t Loss 1.890822 (2.174895)\n",
            "Batch 2800/3125\t Loss 1.617260 (2.176320)\n",
            "Batch 2900/3125\t Loss 1.796145 (2.173550)\n",
            "Batch 3000/3125\t Loss 1.868560 (2.173971)\n",
            "Batch 3100/3125\t Loss 2.633198 (2.177621)\n",
            "==> Test\n",
            "       TNR    AUROC  DTACC  AUIN   AUOUT \n",
            "Bas    15.250 70.127 64.700 71.174 67.502\n",
            "Acc: 60.72000\n",
            "Best Acc (%): 60.720\t\n",
            "==> Epoch 6/30\n",
            "Batch 100/3125\t Loss 2.715619 (2.195944)\n",
            "Batch 200/3125\t Loss 2.473975 (2.222302)\n",
            "Batch 300/3125\t Loss 2.492759 (2.188899)\n",
            "Batch 400/3125\t Loss 2.834776 (2.212653)\n",
            "Batch 500/3125\t Loss 2.329848 (2.208540)\n",
            "Batch 600/3125\t Loss 2.177514 (2.199636)\n",
            "Batch 700/3125\t Loss 2.297413 (2.194635)\n",
            "Batch 800/3125\t Loss 1.888636 (2.198902)\n",
            "Batch 900/3125\t Loss 2.984143 (2.210236)\n",
            "Batch 1000/3125\t Loss 2.795460 (2.218981)\n",
            "Batch 1100/3125\t Loss 1.609379 (2.207089)\n",
            "Batch 1200/3125\t Loss 2.890779 (2.202225)\n",
            "Batch 1300/3125\t Loss 1.809842 (2.193690)\n",
            "Batch 1400/3125\t Loss 2.032958 (2.191221)\n",
            "Batch 1500/3125\t Loss 2.077146 (2.185487)\n",
            "Batch 1600/3125\t Loss 2.500742 (2.188732)\n",
            "Batch 1700/3125\t Loss 2.918000 (2.188412)\n",
            "Batch 1800/3125\t Loss 1.521154 (2.186197)\n",
            "Batch 1900/3125\t Loss 2.050738 (2.187383)\n",
            "Batch 2000/3125\t Loss 2.136203 (2.185985)\n",
            "Batch 2100/3125\t Loss 2.069129 (2.181614)\n",
            "Batch 2200/3125\t Loss 1.122522 (2.178789)\n",
            "Batch 2300/3125\t Loss 1.274760 (2.180751)\n",
            "Batch 2400/3125\t Loss 3.211682 (2.182043)\n",
            "Batch 2500/3125\t Loss 1.962910 (2.179362)\n",
            "Batch 2600/3125\t Loss 2.931909 (2.179876)\n",
            "Batch 2700/3125\t Loss 2.654466 (2.181595)\n",
            "Batch 2800/3125\t Loss 0.951457 (2.181210)\n",
            "Batch 2900/3125\t Loss 2.345303 (2.180809)\n",
            "Batch 3000/3125\t Loss 1.859581 (2.176914)\n",
            "Batch 3100/3125\t Loss 2.037024 (2.178631)\n",
            "==> Epoch 7/30\n",
            "Batch 100/3125\t Loss 1.725792 (2.165065)\n",
            "Batch 200/3125\t Loss 2.045639 (2.213898)\n",
            "Batch 300/3125\t Loss 3.302140 (2.170234)\n",
            "Batch 400/3125\t Loss 2.187913 (2.164831)\n",
            "Batch 500/3125\t Loss 2.215699 (2.166299)\n",
            "Batch 600/3125\t Loss 1.758968 (2.163780)\n",
            "Batch 700/3125\t Loss 2.550395 (2.182255)\n",
            "Batch 800/3125\t Loss 2.072423 (2.181426)\n",
            "Batch 900/3125\t Loss 2.794679 (2.172944)\n",
            "Batch 1000/3125\t Loss 2.789273 (2.170769)\n",
            "Batch 1100/3125\t Loss 2.531249 (2.174926)\n",
            "Batch 1200/3125\t Loss 1.654677 (2.170079)\n",
            "Batch 1300/3125\t Loss 1.446911 (2.164339)\n",
            "Batch 1400/3125\t Loss 2.379678 (2.163073)\n",
            "Batch 1500/3125\t Loss 1.574905 (2.164198)\n",
            "Batch 1600/3125\t Loss 1.061602 (2.164265)\n",
            "Batch 1700/3125\t Loss 1.344731 (2.165950)\n",
            "Batch 1800/3125\t Loss 2.075067 (2.169045)\n",
            "Batch 1900/3125\t Loss 2.251966 (2.169152)\n",
            "Batch 2000/3125\t Loss 3.792575 (2.169031)\n",
            "Batch 2100/3125\t Loss 1.643485 (2.169920)\n",
            "Batch 2200/3125\t Loss 1.976905 (2.175381)\n",
            "Batch 2300/3125\t Loss 2.142138 (2.175031)\n",
            "Batch 2400/3125\t Loss 1.931700 (2.179741)\n",
            "Batch 2500/3125\t Loss 3.391068 (2.177610)\n",
            "Batch 2600/3125\t Loss 1.478491 (2.177127)\n",
            "Batch 2700/3125\t Loss 3.432023 (2.175665)\n",
            "Batch 2800/3125\t Loss 2.280220 (2.172465)\n",
            "Batch 2900/3125\t Loss 1.173702 (2.175922)\n",
            "Batch 3000/3125\t Loss 3.011547 (2.180109)\n",
            "Batch 3100/3125\t Loss 2.058441 (2.178724)\n",
            "==> Epoch 8/30\n",
            "Batch 100/3125\t Loss 1.810057 (2.104238)\n",
            "Batch 200/3125\t Loss 2.563263 (2.168118)\n",
            "Batch 300/3125\t Loss 0.807089 (2.157047)\n",
            "Batch 400/3125\t Loss 1.907776 (2.181376)\n",
            "Batch 500/3125\t Loss 2.333093 (2.180304)\n",
            "Batch 600/3125\t Loss 2.626666 (2.181490)\n",
            "Batch 700/3125\t Loss 3.782310 (2.184412)\n",
            "Batch 800/3125\t Loss 1.645393 (2.190325)\n",
            "Batch 900/3125\t Loss 1.664103 (2.184228)\n",
            "Batch 1000/3125\t Loss 0.747184 (2.181352)\n",
            "Batch 1100/3125\t Loss 2.901616 (2.179764)\n",
            "Batch 1200/3125\t Loss 2.187423 (2.179995)\n",
            "Batch 1300/3125\t Loss 2.636958 (2.177667)\n",
            "Batch 1400/3125\t Loss 2.083750 (2.180259)\n",
            "Batch 1500/3125\t Loss 2.336118 (2.176651)\n",
            "Batch 1600/3125\t Loss 1.659325 (2.178357)\n",
            "Batch 1700/3125\t Loss 1.910056 (2.174736)\n",
            "Batch 1800/3125\t Loss 2.467419 (2.174721)\n",
            "Batch 1900/3125\t Loss 2.582412 (2.175637)\n",
            "Batch 2000/3125\t Loss 2.415349 (2.172177)\n",
            "Batch 2100/3125\t Loss 1.559102 (2.170736)\n",
            "Batch 2200/3125\t Loss 1.772766 (2.173136)\n",
            "Batch 2300/3125\t Loss 1.624266 (2.171726)\n",
            "Batch 2400/3125\t Loss 2.578949 (2.171221)\n",
            "Batch 2500/3125\t Loss 1.329284 (2.172278)\n",
            "Batch 2600/3125\t Loss 1.929971 (2.171685)\n",
            "Batch 2700/3125\t Loss 2.544285 (2.166722)\n",
            "Batch 2800/3125\t Loss 1.727156 (2.166081)\n",
            "Batch 2900/3125\t Loss 2.510013 (2.163908)\n",
            "Batch 3000/3125\t Loss 2.788816 (2.162908)\n",
            "Batch 3100/3125\t Loss 1.258441 (2.164918)\n",
            "==> Epoch 9/30\n",
            "Batch 100/3125\t Loss 2.255866 (2.137150)\n",
            "Batch 200/3125\t Loss 2.212548 (2.202278)\n",
            "Batch 300/3125\t Loss 2.549641 (2.179000)\n",
            "Batch 400/3125\t Loss 1.608291 (2.150742)\n",
            "Batch 500/3125\t Loss 2.987065 (2.173686)\n",
            "Batch 600/3125\t Loss 1.859937 (2.178079)\n",
            "Batch 700/3125\t Loss 2.105165 (2.188260)\n",
            "Batch 800/3125\t Loss 2.436111 (2.180451)\n",
            "Batch 900/3125\t Loss 2.645330 (2.183429)\n",
            "Batch 1000/3125\t Loss 3.623259 (2.185843)\n",
            "Batch 1100/3125\t Loss 1.549714 (2.187087)\n",
            "Batch 1200/3125\t Loss 2.222567 (2.189949)\n",
            "Batch 1300/3125\t Loss 1.509552 (2.187800)\n",
            "Batch 1400/3125\t Loss 2.361691 (2.188974)\n",
            "Batch 1500/3125\t Loss 1.035865 (2.192799)\n",
            "Batch 1600/3125\t Loss 1.824931 (2.188746)\n",
            "Batch 1700/3125\t Loss 1.999049 (2.183347)\n",
            "Batch 1800/3125\t Loss 2.967423 (2.179621)\n",
            "Batch 1900/3125\t Loss 2.473241 (2.182604)\n",
            "Batch 2000/3125\t Loss 2.820176 (2.178697)\n",
            "Batch 2100/3125\t Loss 1.452281 (2.175675)\n",
            "Batch 2200/3125\t Loss 1.123446 (2.171839)\n",
            "Batch 2300/3125\t Loss 2.324083 (2.170848)\n",
            "Batch 2400/3125\t Loss 1.610956 (2.168837)\n",
            "Batch 2500/3125\t Loss 2.266874 (2.171708)\n",
            "Batch 2600/3125\t Loss 2.000562 (2.173744)\n",
            "Batch 2700/3125\t Loss 2.519427 (2.173877)\n",
            "Batch 2800/3125\t Loss 2.427028 (2.175117)\n",
            "Batch 2900/3125\t Loss 1.436390 (2.172482)\n",
            "Batch 3000/3125\t Loss 1.101878 (2.172459)\n",
            "Batch 3100/3125\t Loss 2.284461 (2.169372)\n",
            "==> Epoch 10/30\n",
            "Batch 100/3125\t Loss 3.182262 (2.203752)\n",
            "Batch 200/3125\t Loss 2.014115 (2.183506)\n",
            "Batch 300/3125\t Loss 1.278317 (2.185648)\n",
            "Batch 400/3125\t Loss 1.172818 (2.184999)\n",
            "Batch 500/3125\t Loss 2.653436 (2.196379)\n",
            "Batch 600/3125\t Loss 1.671416 (2.200327)\n",
            "Batch 700/3125\t Loss 1.706855 (2.201112)\n",
            "Batch 800/3125\t Loss 2.018835 (2.203558)\n",
            "Batch 900/3125\t Loss 2.883418 (2.199918)\n",
            "Batch 1000/3125\t Loss 2.390838 (2.202748)\n",
            "Batch 1100/3125\t Loss 2.097849 (2.195776)\n",
            "Batch 1200/3125\t Loss 2.549339 (2.193958)\n",
            "Batch 1300/3125\t Loss 3.265271 (2.187282)\n",
            "Batch 1400/3125\t Loss 2.882823 (2.183745)\n",
            "Batch 1500/3125\t Loss 3.024392 (2.185654)\n",
            "Batch 1600/3125\t Loss 1.080171 (2.187001)\n",
            "Batch 1700/3125\t Loss 2.583218 (2.188159)\n",
            "Batch 1800/3125\t Loss 1.627249 (2.189096)\n",
            "Batch 1900/3125\t Loss 2.120644 (2.186374)\n",
            "Batch 2000/3125\t Loss 1.681227 (2.182305)\n",
            "Batch 2100/3125\t Loss 2.919714 (2.181668)\n",
            "Batch 2200/3125\t Loss 2.628789 (2.181998)\n",
            "Batch 2300/3125\t Loss 2.099183 (2.182838)\n",
            "Batch 2400/3125\t Loss 2.294574 (2.182426)\n",
            "Batch 2500/3125\t Loss 1.452431 (2.181121)\n",
            "Batch 2600/3125\t Loss 3.701032 (2.182684)\n",
            "Batch 2700/3125\t Loss 2.466566 (2.184871)\n",
            "Batch 2800/3125\t Loss 2.328458 (2.182179)\n",
            "Batch 2900/3125\t Loss 2.326530 (2.182068)\n",
            "Batch 3000/3125\t Loss 2.357426 (2.179964)\n",
            "Batch 3100/3125\t Loss 2.059040 (2.182770)\n",
            "==> Test\n",
            "       TNR    AUROC  DTACC  AUIN   AUOUT \n",
            "Bas    14.740 73.702 67.900 75.815 69.588\n",
            "Acc: 68.41000\n",
            "Best Acc (%): 68.410\t\n",
            "==> Epoch 11/30\n",
            "Batch 100/3125\t Loss 3.112034 (2.209273)\n",
            "Batch 200/3125\t Loss 1.677347 (2.125665)\n",
            "Batch 300/3125\t Loss 2.785434 (2.145772)\n",
            "Batch 400/3125\t Loss 2.755740 (2.146343)\n",
            "Batch 500/3125\t Loss 0.594635 (2.169633)\n",
            "Batch 600/3125\t Loss 1.234285 (2.172824)\n",
            "Batch 700/3125\t Loss 1.930067 (2.174655)\n",
            "Batch 800/3125\t Loss 1.632511 (2.178647)\n",
            "Batch 900/3125\t Loss 1.856237 (2.172783)\n",
            "Batch 1000/3125\t Loss 1.668120 (2.161635)\n",
            "Batch 1100/3125\t Loss 2.386274 (2.160905)\n",
            "Batch 1200/3125\t Loss 3.252506 (2.155272)\n",
            "Batch 1300/3125\t Loss 2.744392 (2.162209)\n",
            "Batch 1400/3125\t Loss 1.741142 (2.155672)\n",
            "Batch 1500/3125\t Loss 1.639804 (2.161575)\n",
            "Batch 1600/3125\t Loss 1.742133 (2.158991)\n",
            "Batch 1700/3125\t Loss 1.646925 (2.163778)\n",
            "Batch 1800/3125\t Loss 1.230966 (2.162516)\n",
            "Batch 1900/3125\t Loss 2.477968 (2.164142)\n",
            "Batch 2000/3125\t Loss 1.663316 (2.165738)\n",
            "Batch 2100/3125\t Loss 2.611180 (2.167910)\n",
            "Batch 2200/3125\t Loss 2.208228 (2.171458)\n",
            "Batch 2300/3125\t Loss 3.228067 (2.173455)\n",
            "Batch 2400/3125\t Loss 2.530333 (2.176325)\n",
            "Batch 2500/3125\t Loss 1.819072 (2.177097)\n",
            "Batch 2600/3125\t Loss 3.615070 (2.176976)\n",
            "Batch 2700/3125\t Loss 1.778856 (2.174055)\n",
            "Batch 2800/3125\t Loss 2.031321 (2.173527)\n",
            "Batch 2900/3125\t Loss 2.710021 (2.171093)\n",
            "Batch 3000/3125\t Loss 1.934695 (2.171174)\n",
            "Batch 3100/3125\t Loss 1.886553 (2.173188)\n",
            "==> Epoch 12/30\n",
            "Batch 100/3125\t Loss 2.280628 (2.174427)\n",
            "Batch 200/3125\t Loss 1.562147 (2.182447)\n",
            "Batch 300/3125\t Loss 2.724629 (2.222028)\n",
            "Batch 400/3125\t Loss 2.880667 (2.182402)\n",
            "Batch 500/3125\t Loss 1.386836 (2.179990)\n",
            "Batch 600/3125\t Loss 1.326036 (2.177151)\n",
            "Batch 700/3125\t Loss 1.961188 (2.169684)\n",
            "Batch 800/3125\t Loss 2.118454 (2.170682)\n",
            "Batch 900/3125\t Loss 2.432384 (2.173165)\n",
            "Batch 1000/3125\t Loss 1.618189 (2.171370)\n",
            "Batch 1100/3125\t Loss 2.670676 (2.166176)\n",
            "Batch 1200/3125\t Loss 3.275838 (2.173583)\n",
            "Batch 1300/3125\t Loss 2.161511 (2.170630)\n",
            "Batch 1400/3125\t Loss 2.531412 (2.167597)\n",
            "Batch 1500/3125\t Loss 2.078074 (2.164161)\n",
            "Batch 1600/3125\t Loss 2.543941 (2.164239)\n",
            "Batch 1700/3125\t Loss 2.654895 (2.164631)\n",
            "Batch 1800/3125\t Loss 2.737253 (2.161521)\n",
            "Batch 1900/3125\t Loss 1.393264 (2.166629)\n",
            "Batch 2000/3125\t Loss 2.141923 (2.169465)\n",
            "Batch 2100/3125\t Loss 2.214423 (2.167645)\n",
            "Batch 2200/3125\t Loss 3.004492 (2.171620)\n",
            "Batch 2300/3125\t Loss 2.659138 (2.176226)\n",
            "Batch 2400/3125\t Loss 1.803020 (2.173583)\n",
            "Batch 2500/3125\t Loss 2.113357 (2.174149)\n",
            "Batch 2600/3125\t Loss 1.860235 (2.174978)\n",
            "Batch 2700/3125\t Loss 2.313015 (2.172822)\n",
            "Batch 2800/3125\t Loss 2.088199 (2.174147)\n",
            "Batch 2900/3125\t Loss 2.509268 (2.175256)\n",
            "Batch 3000/3125\t Loss 2.321535 (2.172625)\n",
            "Batch 3100/3125\t Loss 1.064844 (2.170040)\n",
            "==> Epoch 13/30\n",
            "Batch 100/3125\t Loss 3.035483 (2.244054)\n",
            "Batch 200/3125\t Loss 2.141030 (2.163818)\n",
            "Batch 300/3125\t Loss 2.619762 (2.183434)\n",
            "Batch 400/3125\t Loss 2.241024 (2.187335)\n",
            "Batch 500/3125\t Loss 1.858050 (2.174218)\n",
            "Batch 600/3125\t Loss 1.776400 (2.175947)\n",
            "Batch 700/3125\t Loss 1.963858 (2.176403)\n",
            "Batch 800/3125\t Loss 1.166307 (2.164017)\n",
            "Batch 900/3125\t Loss 2.713983 (2.157642)\n",
            "Batch 1000/3125\t Loss 2.507080 (2.157640)\n",
            "Batch 1100/3125\t Loss 2.325377 (2.152394)\n",
            "Batch 1200/3125\t Loss 1.744172 (2.157131)\n",
            "Batch 1300/3125\t Loss 2.180732 (2.151565)\n",
            "Batch 1400/3125\t Loss 2.629643 (2.160680)\n",
            "Batch 1500/3125\t Loss 2.347853 (2.160488)\n",
            "Batch 1600/3125\t Loss 2.099694 (2.166742)\n",
            "Batch 1700/3125\t Loss 3.282529 (2.163914)\n",
            "Batch 1800/3125\t Loss 1.966090 (2.167248)\n",
            "Batch 1900/3125\t Loss 2.212075 (2.165417)\n",
            "Batch 2000/3125\t Loss 1.834374 (2.166388)\n",
            "Batch 2100/3125\t Loss 1.946435 (2.167644)\n",
            "Batch 2200/3125\t Loss 1.064875 (2.167052)\n",
            "Batch 2300/3125\t Loss 2.539876 (2.166486)\n",
            "Batch 2400/3125\t Loss 2.630273 (2.167378)\n",
            "Batch 2500/3125\t Loss 1.464973 (2.170103)\n",
            "Batch 2600/3125\t Loss 1.854693 (2.173479)\n",
            "Batch 2700/3125\t Loss 2.551445 (2.172912)\n",
            "Batch 2800/3125\t Loss 1.734420 (2.174689)\n",
            "Batch 2900/3125\t Loss 1.589540 (2.172521)\n",
            "Batch 3000/3125\t Loss 1.777073 (2.171520)\n",
            "Batch 3100/3125\t Loss 2.659117 (2.168240)\n",
            "==> Epoch 14/30\n",
            "Batch 100/3125\t Loss 2.896170 (2.188358)\n",
            "Batch 200/3125\t Loss 3.150857 (2.191947)\n",
            "Batch 300/3125\t Loss 3.798317 (2.167208)\n",
            "Batch 400/3125\t Loss 3.045109 (2.167494)\n",
            "Batch 500/3125\t Loss 2.287804 (2.185711)\n",
            "Batch 600/3125\t Loss 2.065602 (2.173500)\n",
            "Batch 700/3125\t Loss 1.908507 (2.178906)\n",
            "Batch 800/3125\t Loss 1.891505 (2.174356)\n",
            "Batch 900/3125\t Loss 2.225982 (2.181051)\n",
            "Batch 1000/3125\t Loss 1.701552 (2.171293)\n",
            "Batch 1100/3125\t Loss 1.582168 (2.167558)\n",
            "Batch 1200/3125\t Loss 2.139156 (2.170023)\n",
            "Batch 1300/3125\t Loss 2.874955 (2.172299)\n",
            "Batch 1400/3125\t Loss 1.651161 (2.168141)\n",
            "Batch 1500/3125\t Loss 2.426537 (2.169921)\n",
            "Batch 1600/3125\t Loss 0.782389 (2.167636)\n",
            "Batch 1700/3125\t Loss 2.127451 (2.165607)\n",
            "Batch 1800/3125\t Loss 3.110770 (2.171741)\n",
            "Batch 1900/3125\t Loss 2.059370 (2.169464)\n",
            "Batch 2000/3125\t Loss 2.133531 (2.166684)\n",
            "Batch 2100/3125\t Loss 2.679565 (2.166567)\n",
            "Batch 2200/3125\t Loss 1.922867 (2.165848)\n",
            "Batch 2300/3125\t Loss 2.664128 (2.165853)\n",
            "Batch 2400/3125\t Loss 2.731010 (2.165799)\n",
            "Batch 2500/3125\t Loss 1.128557 (2.164099)\n",
            "Batch 2600/3125\t Loss 1.351908 (2.163437)\n",
            "Batch 2700/3125\t Loss 1.824980 (2.164674)\n",
            "Batch 2800/3125\t Loss 2.405848 (2.162976)\n",
            "Batch 2900/3125\t Loss 2.185620 (2.161714)\n",
            "Batch 3000/3125\t Loss 2.471913 (2.161036)\n",
            "Batch 3100/3125\t Loss 2.053040 (2.161855)\n",
            "==> Epoch 15/30\n",
            "Batch 100/3125\t Loss 2.236547 (2.066393)\n",
            "Batch 200/3125\t Loss 3.561641 (2.108958)\n",
            "Batch 300/3125\t Loss 2.565479 (2.130845)\n",
            "Batch 400/3125\t Loss 1.592412 (2.142356)\n",
            "Batch 500/3125\t Loss 2.325123 (2.149102)\n",
            "Batch 600/3125\t Loss 2.695138 (2.165625)\n",
            "Batch 700/3125\t Loss 2.247836 (2.147226)\n",
            "Batch 800/3125\t Loss 2.085295 (2.145213)\n",
            "Batch 900/3125\t Loss 1.889331 (2.149000)\n",
            "Batch 1000/3125\t Loss 2.321806 (2.147269)\n",
            "Batch 1100/3125\t Loss 3.358099 (2.151317)\n",
            "Batch 1200/3125\t Loss 1.322932 (2.151289)\n",
            "Batch 1300/3125\t Loss 3.147457 (2.152366)\n",
            "Batch 1400/3125\t Loss 2.027559 (2.148163)\n",
            "Batch 1500/3125\t Loss 1.708331 (2.147523)\n",
            "Batch 1600/3125\t Loss 2.536553 (2.153620)\n",
            "Batch 1700/3125\t Loss 3.123838 (2.157301)\n",
            "Batch 1800/3125\t Loss 2.543759 (2.162717)\n",
            "Batch 1900/3125\t Loss 1.562698 (2.160102)\n",
            "Batch 2000/3125\t Loss 2.630594 (2.160243)\n",
            "Batch 2100/3125\t Loss 2.066114 (2.163987)\n",
            "Batch 2200/3125\t Loss 3.197973 (2.161327)\n",
            "Batch 2300/3125\t Loss 2.308149 (2.159888)\n",
            "Batch 2400/3125\t Loss 2.168073 (2.157653)\n",
            "Batch 2500/3125\t Loss 2.580329 (2.157522)\n",
            "Batch 2600/3125\t Loss 1.666926 (2.158158)\n",
            "Batch 2700/3125\t Loss 1.596761 (2.158121)\n",
            "Batch 2800/3125\t Loss 2.535268 (2.159351)\n",
            "Batch 2900/3125\t Loss 2.605402 (2.160866)\n",
            "Batch 3000/3125\t Loss 2.244463 (2.164432)\n",
            "Batch 3100/3125\t Loss 1.809933 (2.162111)\n",
            "==> Test\n",
            "       TNR    AUROC  DTACC  AUIN   AUOUT \n",
            "Bas    17.610 74.010 68.145 74.941 70.745\n",
            "Acc: 69.04000\n",
            "Best Acc (%): 69.040\t\n",
            "==> Epoch 16/30\n",
            "Batch 100/3125\t Loss 2.669567 (2.176266)\n",
            "Batch 200/3125\t Loss 3.193273 (2.170220)\n",
            "Batch 300/3125\t Loss 3.450742 (2.179069)\n",
            "Batch 400/3125\t Loss 1.529396 (2.169894)\n",
            "Batch 500/3125\t Loss 2.945606 (2.180436)\n",
            "Batch 600/3125\t Loss 2.433942 (2.175923)\n",
            "Batch 700/3125\t Loss 1.546191 (2.182465)\n",
            "Batch 800/3125\t Loss 2.673590 (2.189407)\n",
            "Batch 900/3125\t Loss 1.924265 (2.191231)\n",
            "Batch 1000/3125\t Loss 3.023832 (2.191669)\n",
            "Batch 1100/3125\t Loss 2.377786 (2.186713)\n",
            "Batch 1200/3125\t Loss 2.342135 (2.185418)\n",
            "Batch 1300/3125\t Loss 2.128133 (2.186156)\n",
            "Batch 1400/3125\t Loss 1.582081 (2.183359)\n",
            "Batch 1500/3125\t Loss 1.683690 (2.182903)\n",
            "Batch 1600/3125\t Loss 2.082335 (2.184201)\n",
            "Batch 1700/3125\t Loss 2.875449 (2.181061)\n",
            "Batch 1800/3125\t Loss 1.911239 (2.181775)\n",
            "Batch 1900/3125\t Loss 1.801473 (2.180783)\n",
            "Batch 2000/3125\t Loss 2.075524 (2.183845)\n",
            "Batch 2100/3125\t Loss 2.859396 (2.183280)\n",
            "Batch 2200/3125\t Loss 2.473089 (2.182228)\n",
            "Batch 2300/3125\t Loss 1.254774 (2.181655)\n",
            "Batch 2400/3125\t Loss 1.197883 (2.182325)\n",
            "Batch 2500/3125\t Loss 2.254061 (2.181714)\n",
            "Batch 2600/3125\t Loss 2.184051 (2.180297)\n",
            "Batch 2700/3125\t Loss 1.833593 (2.180821)\n",
            "Batch 2800/3125\t Loss 2.435803 (2.177940)\n",
            "Batch 2900/3125\t Loss 1.930711 (2.177884)\n",
            "Batch 3000/3125\t Loss 1.863784 (2.178635)\n",
            "Batch 3100/3125\t Loss 1.187329 (2.177384)\n",
            "==> Epoch 17/30\n",
            "Batch 100/3125\t Loss 1.477424 (2.140062)\n",
            "Batch 200/3125\t Loss 1.730336 (2.190498)\n",
            "Batch 300/3125\t Loss 2.863618 (2.183968)\n",
            "Batch 400/3125\t Loss 1.404916 (2.184889)\n",
            "Batch 500/3125\t Loss 1.496639 (2.182285)\n",
            "Batch 600/3125\t Loss 2.235521 (2.179144)\n",
            "Batch 700/3125\t Loss 2.788660 (2.171065)\n",
            "Batch 800/3125\t Loss 2.640050 (2.172917)\n",
            "Batch 900/3125\t Loss 2.066267 (2.167878)\n",
            "Batch 1000/3125\t Loss 2.557823 (2.172145)\n",
            "Batch 1100/3125\t Loss 2.524695 (2.167717)\n",
            "Batch 1200/3125\t Loss 1.666132 (2.163062)\n",
            "Batch 1300/3125\t Loss 1.993344 (2.163193)\n",
            "Batch 1400/3125\t Loss 1.893265 (2.162181)\n",
            "Batch 1500/3125\t Loss 2.365466 (2.164439)\n",
            "Batch 1600/3125\t Loss 2.425436 (2.155205)\n",
            "Batch 1700/3125\t Loss 2.936904 (2.163681)\n",
            "Batch 1800/3125\t Loss 2.178056 (2.159619)\n",
            "Batch 1900/3125\t Loss 2.461893 (2.161750)\n",
            "Batch 2000/3125\t Loss 1.689242 (2.164205)\n",
            "Batch 2100/3125\t Loss 1.299187 (2.161023)\n",
            "Batch 2200/3125\t Loss 2.099717 (2.156183)\n",
            "Batch 2300/3125\t Loss 1.103185 (2.156430)\n",
            "Batch 2400/3125\t Loss 2.113082 (2.156251)\n",
            "Batch 2500/3125\t Loss 2.834289 (2.158608)\n",
            "Batch 2600/3125\t Loss 1.460024 (2.163985)\n",
            "Batch 2700/3125\t Loss 2.764623 (2.165118)\n",
            "Batch 2800/3125\t Loss 1.890624 (2.165152)\n",
            "Batch 2900/3125\t Loss 2.557223 (2.167255)\n",
            "Batch 3000/3125\t Loss 2.342845 (2.167044)\n",
            "Batch 3100/3125\t Loss 2.407001 (2.168352)\n",
            "==> Epoch 18/30\n",
            "Batch 100/3125\t Loss 3.805521 (2.144936)\n",
            "Batch 200/3125\t Loss 2.724349 (2.181838)\n",
            "Batch 300/3125\t Loss 0.667699 (2.175983)\n",
            "Batch 400/3125\t Loss 1.787150 (2.165077)\n",
            "Batch 500/3125\t Loss 2.836486 (2.164597)\n",
            "Batch 600/3125\t Loss 1.345400 (2.182923)\n",
            "Batch 700/3125\t Loss 2.027200 (2.185206)\n",
            "Batch 800/3125\t Loss 2.262824 (2.191384)\n",
            "Batch 900/3125\t Loss 2.353981 (2.189314)\n",
            "Batch 1000/3125\t Loss 1.013773 (2.170916)\n",
            "Batch 1100/3125\t Loss 2.096162 (2.172667)\n",
            "Batch 1200/3125\t Loss 1.567452 (2.172235)\n",
            "Batch 1300/3125\t Loss 3.377743 (2.178070)\n",
            "Batch 1400/3125\t Loss 2.081568 (2.181302)\n",
            "Batch 1500/3125\t Loss 2.126808 (2.186531)\n",
            "Batch 1600/3125\t Loss 1.817338 (2.181217)\n",
            "Batch 1700/3125\t Loss 2.376768 (2.187732)\n",
            "Batch 1800/3125\t Loss 1.607576 (2.185831)\n",
            "Batch 1900/3125\t Loss 1.522808 (2.183995)\n",
            "Batch 2000/3125\t Loss 2.171143 (2.182271)\n",
            "Batch 2100/3125\t Loss 2.469411 (2.180576)\n",
            "Batch 2200/3125\t Loss 2.493318 (2.175899)\n",
            "Batch 2300/3125\t Loss 2.133518 (2.177527)\n",
            "Batch 2400/3125\t Loss 1.860657 (2.177148)\n",
            "Batch 2500/3125\t Loss 2.503111 (2.175460)\n",
            "Batch 2600/3125\t Loss 2.028046 (2.173931)\n",
            "Batch 2700/3125\t Loss 1.278318 (2.167746)\n",
            "Batch 2800/3125\t Loss 2.003624 (2.168182)\n",
            "Batch 2900/3125\t Loss 2.127688 (2.170789)\n",
            "Batch 3000/3125\t Loss 2.609701 (2.169411)\n",
            "Batch 3100/3125\t Loss 2.864915 (2.167095)\n",
            "==> Epoch 19/30\n",
            "Batch 100/3125\t Loss 2.055238 (2.062041)\n",
            "Batch 200/3125\t Loss 1.657912 (2.059741)\n",
            "Batch 300/3125\t Loss 2.604746 (2.130851)\n",
            "Batch 400/3125\t Loss 2.040445 (2.137013)\n",
            "Batch 500/3125\t Loss 2.244042 (2.155267)\n",
            "Batch 600/3125\t Loss 2.239548 (2.158667)\n",
            "Batch 700/3125\t Loss 2.617064 (2.161267)\n",
            "Batch 800/3125\t Loss 1.392005 (2.155391)\n",
            "Batch 900/3125\t Loss 1.444493 (2.162437)\n",
            "Batch 1000/3125\t Loss 0.929988 (2.156302)\n",
            "Batch 1100/3125\t Loss 2.614882 (2.154195)\n",
            "Batch 1200/3125\t Loss 2.470308 (2.163647)\n",
            "Batch 1300/3125\t Loss 2.735192 (2.165253)\n",
            "Batch 1400/3125\t Loss 2.504484 (2.166060)\n",
            "Batch 1500/3125\t Loss 2.127991 (2.166608)\n",
            "Batch 1600/3125\t Loss 3.523635 (2.174040)\n",
            "Batch 1700/3125\t Loss 1.930299 (2.167047)\n",
            "Batch 1800/3125\t Loss 2.116344 (2.166257)\n",
            "Batch 1900/3125\t Loss 2.445158 (2.165896)\n",
            "Batch 2000/3125\t Loss 3.254752 (2.165745)\n",
            "Batch 2100/3125\t Loss 2.204086 (2.165979)\n",
            "Batch 2200/3125\t Loss 2.963049 (2.162847)\n",
            "Batch 2300/3125\t Loss 2.409819 (2.161018)\n",
            "Batch 2400/3125\t Loss 3.032801 (2.162201)\n",
            "Batch 2500/3125\t Loss 1.673276 (2.163350)\n",
            "Batch 2600/3125\t Loss 1.675385 (2.160829)\n",
            "Batch 2700/3125\t Loss 2.436551 (2.156119)\n",
            "Batch 2800/3125\t Loss 2.702363 (2.155094)\n",
            "Batch 2900/3125\t Loss 2.332437 (2.157679)\n",
            "Batch 3000/3125\t Loss 1.998702 (2.158930)\n",
            "Batch 3100/3125\t Loss 1.587769 (2.157420)\n",
            "==> Epoch 20/30\n",
            "Batch 100/3125\t Loss 2.649075 (2.089749)\n",
            "Batch 200/3125\t Loss 0.789513 (2.160930)\n",
            "Batch 300/3125\t Loss 2.701398 (2.174105)\n",
            "Batch 400/3125\t Loss 2.108109 (2.156008)\n",
            "Batch 500/3125\t Loss 2.481081 (2.173999)\n",
            "Batch 600/3125\t Loss 2.328323 (2.153546)\n",
            "Batch 700/3125\t Loss 4.067539 (2.148580)\n",
            "Batch 800/3125\t Loss 1.477482 (2.149842)\n",
            "Batch 900/3125\t Loss 2.638102 (2.158823)\n",
            "Batch 1000/3125\t Loss 2.621101 (2.158243)\n",
            "Batch 1100/3125\t Loss 1.665926 (2.156380)\n",
            "Batch 1200/3125\t Loss 0.800932 (2.153102)\n",
            "Batch 1300/3125\t Loss 1.244386 (2.150660)\n",
            "Batch 1400/3125\t Loss 1.735922 (2.150300)\n",
            "Batch 1500/3125\t Loss 2.189812 (2.152820)\n",
            "Batch 1600/3125\t Loss 2.281117 (2.155407)\n",
            "Batch 1700/3125\t Loss 2.351184 (2.159890)\n",
            "Batch 1800/3125\t Loss 1.633901 (2.161755)\n",
            "Batch 1900/3125\t Loss 1.473957 (2.161419)\n",
            "Batch 2000/3125\t Loss 1.602721 (2.161559)\n",
            "Batch 2100/3125\t Loss 2.559529 (2.163926)\n",
            "Batch 2200/3125\t Loss 2.720899 (2.163717)\n",
            "Batch 2300/3125\t Loss 2.838253 (2.168015)\n",
            "Batch 2400/3125\t Loss 2.116982 (2.164406)\n",
            "Batch 2500/3125\t Loss 1.929435 (2.166141)\n",
            "Batch 2600/3125\t Loss 1.833289 (2.170368)\n",
            "Batch 2700/3125\t Loss 2.400059 (2.171153)\n",
            "Batch 2800/3125\t Loss 1.640739 (2.169556)\n",
            "Batch 2900/3125\t Loss 2.758991 (2.168449)\n",
            "Batch 3000/3125\t Loss 2.904843 (2.165837)\n",
            "Batch 3100/3125\t Loss 2.097026 (2.165703)\n",
            "==> Test\n",
            "       TNR    AUROC  DTACC  AUIN   AUOUT \n",
            "Bas    15.000 72.864 67.195 75.063 69.017\n",
            "Acc: 68.60000\n",
            "==> Epoch 21/30\n",
            "Batch 100/3125\t Loss 2.700887 (2.188493)\n",
            "Batch 200/3125\t Loss 1.427979 (2.173310)\n",
            "Batch 300/3125\t Loss 1.613684 (2.163121)\n",
            "Batch 400/3125\t Loss 1.585198 (2.169342)\n",
            "Batch 500/3125\t Loss 1.753467 (2.167969)\n",
            "Batch 600/3125\t Loss 2.029510 (2.173113)\n",
            "Batch 700/3125\t Loss 1.323910 (2.177589)\n",
            "Batch 800/3125\t Loss 2.429873 (2.178666)\n",
            "Batch 900/3125\t Loss 2.903429 (2.173911)\n",
            "Batch 1000/3125\t Loss 3.356830 (2.168861)\n",
            "Batch 1100/3125\t Loss 1.577077 (2.169922)\n",
            "Batch 1200/3125\t Loss 1.324536 (2.172281)\n",
            "Batch 1300/3125\t Loss 2.777054 (2.158598)\n",
            "Batch 1400/3125\t Loss 3.096257 (2.158347)\n",
            "Batch 1500/3125\t Loss 1.344151 (2.154097)\n",
            "Batch 1600/3125\t Loss 2.927403 (2.151810)\n",
            "Batch 1700/3125\t Loss 2.609520 (2.153505)\n",
            "Batch 1800/3125\t Loss 1.761340 (2.155709)\n",
            "Batch 1900/3125\t Loss 2.357934 (2.156184)\n",
            "Batch 2000/3125\t Loss 1.399824 (2.156901)\n",
            "Batch 2100/3125\t Loss 2.003803 (2.155377)\n",
            "Batch 2200/3125\t Loss 1.642190 (2.157241)\n",
            "Batch 2300/3125\t Loss 1.266525 (2.159064)\n",
            "Batch 2400/3125\t Loss 2.874506 (2.162019)\n",
            "Batch 2500/3125\t Loss 2.903848 (2.160390)\n",
            "Batch 2600/3125\t Loss 1.559765 (2.162290)\n",
            "Batch 2700/3125\t Loss 0.971237 (2.166044)\n",
            "Batch 2800/3125\t Loss 1.840499 (2.165406)\n",
            "Batch 2900/3125\t Loss 1.359318 (2.165587)\n",
            "Batch 3000/3125\t Loss 1.493299 (2.167237)\n",
            "Batch 3100/3125\t Loss 2.339937 (2.166082)\n",
            "==> Epoch 22/30\n",
            "Batch 100/3125\t Loss 3.509927 (2.182179)\n",
            "Batch 200/3125\t Loss 2.233078 (2.122257)\n",
            "Batch 300/3125\t Loss 2.233151 (2.124090)\n",
            "Batch 400/3125\t Loss 2.216821 (2.131362)\n",
            "Batch 500/3125\t Loss 1.753636 (2.153645)\n",
            "Batch 600/3125\t Loss 2.292115 (2.162859)\n",
            "Batch 700/3125\t Loss 2.746811 (2.167208)\n",
            "Batch 800/3125\t Loss 1.404706 (2.162233)\n",
            "Batch 900/3125\t Loss 3.053987 (2.169824)\n",
            "Batch 1000/3125\t Loss 1.650659 (2.161917)\n",
            "Batch 1100/3125\t Loss 2.664371 (2.160743)\n",
            "Batch 1200/3125\t Loss 1.784380 (2.171543)\n",
            "Batch 1300/3125\t Loss 2.987211 (2.168477)\n",
            "Batch 1400/3125\t Loss 1.829772 (2.176075)\n",
            "Batch 1500/3125\t Loss 1.126565 (2.174703)\n",
            "Batch 1600/3125\t Loss 2.775403 (2.175672)\n",
            "Batch 1700/3125\t Loss 1.729311 (2.180057)\n",
            "Batch 1800/3125\t Loss 3.217009 (2.179695)\n",
            "Batch 1900/3125\t Loss 2.083039 (2.180026)\n",
            "Batch 2000/3125\t Loss 2.885038 (2.179685)\n",
            "Batch 2100/3125\t Loss 2.809999 (2.175791)\n",
            "Batch 2200/3125\t Loss 2.288480 (2.177819)\n",
            "Batch 2300/3125\t Loss 2.653158 (2.176070)\n",
            "Batch 2400/3125\t Loss 2.522925 (2.181999)\n",
            "Batch 2500/3125\t Loss 2.613796 (2.179710)\n",
            "Batch 2600/3125\t Loss 3.126362 (2.179967)\n",
            "Batch 2700/3125\t Loss 2.774421 (2.179324)\n",
            "Batch 2800/3125\t Loss 2.912766 (2.181059)\n",
            "Batch 2900/3125\t Loss 2.100479 (2.183275)\n",
            "Batch 3000/3125\t Loss 2.143233 (2.181222)\n",
            "Batch 3100/3125\t Loss 2.457735 (2.180256)\n",
            "==> Epoch 23/30\n",
            "Batch 100/3125\t Loss 2.420426 (2.230424)\n",
            "Batch 200/3125\t Loss 2.473765 (2.145041)\n",
            "Batch 300/3125\t Loss 0.880504 (2.168880)\n",
            "Batch 400/3125\t Loss 3.191449 (2.203919)\n",
            "Batch 500/3125\t Loss 1.670616 (2.172779)\n",
            "Batch 600/3125\t Loss 2.171247 (2.175921)\n",
            "Batch 700/3125\t Loss 1.583640 (2.179252)\n",
            "Batch 800/3125\t Loss 2.421557 (2.169709)\n",
            "Batch 900/3125\t Loss 1.792637 (2.170594)\n",
            "Batch 1000/3125\t Loss 1.264498 (2.171998)\n",
            "Batch 1100/3125\t Loss 2.358029 (2.173357)\n",
            "Batch 1200/3125\t Loss 3.283054 (2.170700)\n",
            "Batch 1300/3125\t Loss 2.782746 (2.164565)\n",
            "Batch 1400/3125\t Loss 2.123972 (2.165098)\n",
            "Batch 1500/3125\t Loss 2.706470 (2.164124)\n",
            "Batch 1600/3125\t Loss 2.869955 (2.168391)\n",
            "Batch 1700/3125\t Loss 1.353392 (2.169175)\n",
            "Batch 1800/3125\t Loss 2.404138 (2.170873)\n",
            "Batch 1900/3125\t Loss 2.627773 (2.170078)\n",
            "Batch 2000/3125\t Loss 2.924238 (2.169620)\n",
            "Batch 2100/3125\t Loss 1.906876 (2.171026)\n",
            "Batch 2200/3125\t Loss 0.460234 (2.165134)\n",
            "Batch 2300/3125\t Loss 2.113744 (2.164641)\n",
            "Batch 2400/3125\t Loss 1.594250 (2.164672)\n",
            "Batch 2500/3125\t Loss 1.786949 (2.160353)\n",
            "Batch 2600/3125\t Loss 2.857703 (2.163051)\n",
            "Batch 2700/3125\t Loss 1.413292 (2.163448)\n",
            "Batch 2800/3125\t Loss 2.807243 (2.160295)\n",
            "Batch 2900/3125\t Loss 1.668007 (2.162320)\n",
            "Batch 3000/3125\t Loss 3.351502 (2.162830)\n",
            "Batch 3100/3125\t Loss 1.360441 (2.162407)\n",
            "==> Epoch 24/30\n",
            "Batch 100/3125\t Loss 2.394467 (2.158990)\n",
            "Batch 200/3125\t Loss 1.928568 (2.162832)\n",
            "Batch 300/3125\t Loss 3.841728 (2.170139)\n",
            "Batch 400/3125\t Loss 2.251584 (2.151126)\n",
            "Batch 500/3125\t Loss 1.837593 (2.163547)\n",
            "Batch 600/3125\t Loss 2.753286 (2.159304)\n",
            "Batch 700/3125\t Loss 2.337588 (2.164122)\n",
            "Batch 800/3125\t Loss 1.612969 (2.171915)\n",
            "Batch 900/3125\t Loss 2.449359 (2.172746)\n",
            "Batch 1000/3125\t Loss 0.976330 (2.168591)\n",
            "Batch 1100/3125\t Loss 2.512754 (2.162324)\n",
            "Batch 1200/3125\t Loss 2.651869 (2.162284)\n",
            "Batch 1300/3125\t Loss 2.187819 (2.166285)\n",
            "Batch 1400/3125\t Loss 1.012208 (2.168950)\n",
            "Batch 1500/3125\t Loss 3.817252 (2.168902)\n",
            "Batch 1600/3125\t Loss 2.811619 (2.167401)\n",
            "Batch 1700/3125\t Loss 2.327409 (2.173131)\n",
            "Batch 1800/3125\t Loss 2.963959 (2.171725)\n",
            "Batch 1900/3125\t Loss 2.234161 (2.175702)\n",
            "Batch 2000/3125\t Loss 2.361396 (2.173579)\n",
            "Batch 2100/3125\t Loss 1.771639 (2.173138)\n",
            "Batch 2200/3125\t Loss 0.752923 (2.174250)\n",
            "Batch 2300/3125\t Loss 2.030445 (2.174289)\n",
            "Batch 2400/3125\t Loss 1.738475 (2.168297)\n",
            "Batch 2500/3125\t Loss 1.647872 (2.169996)\n",
            "Batch 2600/3125\t Loss 1.340445 (2.165942)\n",
            "Batch 2700/3125\t Loss 2.475468 (2.166203)\n",
            "Batch 2800/3125\t Loss 1.424833 (2.166616)\n",
            "Batch 2900/3125\t Loss 2.716401 (2.169918)\n",
            "Batch 3000/3125\t Loss 1.866771 (2.170805)\n",
            "Batch 3100/3125\t Loss 1.620150 (2.170297)\n",
            "==> Epoch 25/30\n",
            "Batch 100/3125\t Loss 2.212027 (2.224770)\n",
            "Batch 200/3125\t Loss 1.998732 (2.191876)\n",
            "Batch 300/3125\t Loss 2.059107 (2.232008)\n",
            "Batch 400/3125\t Loss 2.833163 (2.215454)\n",
            "Batch 500/3125\t Loss 2.134380 (2.188083)\n",
            "Batch 600/3125\t Loss 2.115420 (2.195274)\n",
            "Batch 700/3125\t Loss 3.439616 (2.188969)\n",
            "Batch 800/3125\t Loss 2.116085 (2.188216)\n",
            "Batch 900/3125\t Loss 1.165826 (2.188004)\n",
            "Batch 1000/3125\t Loss 1.297639 (2.184429)\n",
            "Batch 1100/3125\t Loss 3.510514 (2.176040)\n",
            "Batch 1200/3125\t Loss 1.346056 (2.173117)\n",
            "Batch 1300/3125\t Loss 2.570312 (2.177257)\n",
            "Batch 1400/3125\t Loss 3.370428 (2.178894)\n",
            "Batch 1500/3125\t Loss 2.686106 (2.175694)\n",
            "Batch 1600/3125\t Loss 2.017184 (2.177357)\n",
            "Batch 1700/3125\t Loss 1.706197 (2.171383)\n",
            "Batch 1800/3125\t Loss 1.905236 (2.175208)\n",
            "Batch 1900/3125\t Loss 3.620072 (2.173575)\n",
            "Batch 2000/3125\t Loss 2.953128 (2.174907)\n",
            "Batch 2100/3125\t Loss 3.059232 (2.175658)\n",
            "Batch 2200/3125\t Loss 0.873507 (2.172487)\n",
            "Batch 2300/3125\t Loss 2.693980 (2.177083)\n",
            "Batch 2400/3125\t Loss 3.265156 (2.173939)\n",
            "Batch 2500/3125\t Loss 1.292361 (2.175302)\n",
            "Batch 2600/3125\t Loss 2.119534 (2.176501)\n",
            "Batch 2700/3125\t Loss 2.088090 (2.179491)\n",
            "Batch 2800/3125\t Loss 2.530852 (2.181689)\n",
            "Batch 2900/3125\t Loss 1.957482 (2.177023)\n",
            "Batch 3000/3125\t Loss 2.324598 (2.178427)\n",
            "Batch 3100/3125\t Loss 1.944444 (2.180528)\n",
            "==> Test\n",
            "       TNR    AUROC  DTACC  AUIN   AUOUT \n",
            "Bas    17.700 72.753 66.615 74.624 69.559\n",
            "Acc: 67.63000\n",
            "==> Epoch 26/30\n",
            "Batch 100/3125\t Loss 2.127320 (2.170753)\n",
            "Batch 200/3125\t Loss 2.197526 (2.161896)\n",
            "Batch 300/3125\t Loss 1.998372 (2.180441)\n",
            "Batch 400/3125\t Loss 1.526206 (2.201333)\n",
            "Batch 500/3125\t Loss 2.273575 (2.203341)\n",
            "Batch 600/3125\t Loss 1.153894 (2.201003)\n",
            "Batch 700/3125\t Loss 1.816625 (2.215830)\n",
            "Batch 800/3125\t Loss 2.012477 (2.199576)\n",
            "Batch 900/3125\t Loss 1.494696 (2.198217)\n",
            "Batch 1000/3125\t Loss 1.640917 (2.187341)\n",
            "Batch 1100/3125\t Loss 2.508443 (2.187826)\n",
            "Batch 1200/3125\t Loss 1.957916 (2.187747)\n",
            "Batch 1300/3125\t Loss 1.700863 (2.182190)\n",
            "Batch 1400/3125\t Loss 1.079985 (2.176951)\n",
            "Batch 1500/3125\t Loss 1.490083 (2.177268)\n",
            "Batch 1600/3125\t Loss 1.681439 (2.172531)\n",
            "Batch 1700/3125\t Loss 2.518379 (2.182050)\n",
            "Batch 1800/3125\t Loss 2.423026 (2.186363)\n",
            "Batch 1900/3125\t Loss 2.066456 (2.187429)\n",
            "Batch 2000/3125\t Loss 1.323876 (2.186996)\n",
            "Batch 2100/3125\t Loss 2.862357 (2.186769)\n",
            "Batch 2200/3125\t Loss 1.891918 (2.184178)\n",
            "Batch 2300/3125\t Loss 1.351884 (2.188304)\n",
            "Batch 2400/3125\t Loss 2.203060 (2.189450)\n",
            "Batch 2500/3125\t Loss 2.388293 (2.189787)\n",
            "Batch 2600/3125\t Loss 1.702380 (2.187277)\n",
            "Batch 2700/3125\t Loss 1.960742 (2.188393)\n",
            "Batch 2800/3125\t Loss 1.372262 (2.184865)\n",
            "Batch 2900/3125\t Loss 2.557310 (2.183982)\n",
            "Batch 3000/3125\t Loss 1.295549 (2.182115)\n",
            "Batch 3100/3125\t Loss 2.297619 (2.178009)\n",
            "==> Epoch 27/30\n",
            "Batch 100/3125\t Loss 1.907581 (2.061202)\n",
            "Batch 200/3125\t Loss 1.937353 (2.104204)\n",
            "Batch 300/3125\t Loss 1.195700 (2.087954)\n",
            "Batch 400/3125\t Loss 2.166695 (2.114683)\n",
            "Batch 500/3125\t Loss 0.604352 (2.102766)\n",
            "Batch 600/3125\t Loss 2.323335 (2.135026)\n",
            "Batch 700/3125\t Loss 2.429228 (2.134067)\n",
            "Batch 800/3125\t Loss 2.800223 (2.134201)\n",
            "Batch 900/3125\t Loss 1.925660 (2.144084)\n",
            "Batch 1000/3125\t Loss 1.872304 (2.161422)\n",
            "Batch 1100/3125\t Loss 2.401061 (2.166326)\n",
            "Batch 1200/3125\t Loss 3.023752 (2.165758)\n",
            "Batch 1300/3125\t Loss 3.323139 (2.171016)\n",
            "Batch 1400/3125\t Loss 1.124443 (2.166371)\n",
            "Batch 1500/3125\t Loss 2.454373 (2.169920)\n",
            "Batch 1600/3125\t Loss 0.720661 (2.164413)\n",
            "Batch 1700/3125\t Loss 2.001376 (2.176199)\n",
            "Batch 1800/3125\t Loss 3.395518 (2.177126)\n",
            "Batch 1900/3125\t Loss 2.770693 (2.179253)\n",
            "Batch 2000/3125\t Loss 2.284332 (2.184971)\n",
            "Batch 2100/3125\t Loss 3.042322 (2.187163)\n",
            "Batch 2200/3125\t Loss 2.886260 (2.190361)\n",
            "Batch 2300/3125\t Loss 2.278371 (2.187573)\n",
            "Batch 2400/3125\t Loss 2.099685 (2.182940)\n",
            "Batch 2500/3125\t Loss 3.689631 (2.182179)\n",
            "Batch 2600/3125\t Loss 2.355220 (2.181818)\n",
            "Batch 2700/3125\t Loss 2.250609 (2.181759)\n",
            "Batch 2800/3125\t Loss 2.260634 (2.183749)\n",
            "Batch 2900/3125\t Loss 2.839924 (2.186374)\n",
            "Batch 3000/3125\t Loss 2.729453 (2.183871)\n",
            "Batch 3100/3125\t Loss 1.807640 (2.187921)\n",
            "==> Epoch 28/30\n",
            "Batch 100/3125\t Loss 1.761320 (2.248551)\n",
            "Batch 200/3125\t Loss 3.503516 (2.137056)\n",
            "Batch 300/3125\t Loss 0.855412 (2.152493)\n",
            "Batch 400/3125\t Loss 2.584196 (2.165806)\n",
            "Batch 500/3125\t Loss 1.972160 (2.189153)\n",
            "Batch 600/3125\t Loss 1.803094 (2.182112)\n",
            "Batch 700/3125\t Loss 2.182854 (2.189092)\n",
            "Batch 800/3125\t Loss 2.522470 (2.189473)\n",
            "Batch 900/3125\t Loss 2.608023 (2.179826)\n",
            "Batch 1000/3125\t Loss 2.291693 (2.177937)\n",
            "Batch 1100/3125\t Loss 2.100013 (2.176081)\n",
            "Batch 1200/3125\t Loss 1.542243 (2.179801)\n",
            "Batch 1300/3125\t Loss 3.309281 (2.186157)\n",
            "Batch 1400/3125\t Loss 1.845501 (2.175855)\n",
            "Batch 1500/3125\t Loss 2.204693 (2.172550)\n",
            "Batch 1600/3125\t Loss 1.770990 (2.169342)\n",
            "Batch 1700/3125\t Loss 2.692888 (2.174237)\n",
            "Batch 1800/3125\t Loss 2.029713 (2.172439)\n",
            "Batch 1900/3125\t Loss 2.246091 (2.172021)\n",
            "Batch 2000/3125\t Loss 2.062086 (2.171842)\n",
            "Batch 2100/3125\t Loss 1.462955 (2.174285)\n",
            "Batch 2200/3125\t Loss 2.988045 (2.171695)\n",
            "Batch 2300/3125\t Loss 3.337291 (2.174566)\n",
            "Batch 2400/3125\t Loss 2.523239 (2.177372)\n",
            "Batch 2500/3125\t Loss 3.042538 (2.179404)\n",
            "Batch 2600/3125\t Loss 2.058426 (2.181518)\n",
            "Batch 2700/3125\t Loss 1.809161 (2.180494)\n",
            "Batch 2800/3125\t Loss 3.320635 (2.181499)\n",
            "Batch 2900/3125\t Loss 1.195009 (2.180373)\n",
            "Batch 3000/3125\t Loss 2.054742 (2.180602)\n",
            "Batch 3100/3125\t Loss 2.046148 (2.180319)\n",
            "==> Epoch 29/30\n",
            "Batch 100/3125\t Loss 2.397796 (2.175422)\n",
            "Batch 200/3125\t Loss 3.032574 (2.196119)\n",
            "Batch 300/3125\t Loss 2.001762 (2.203526)\n",
            "Batch 400/3125\t Loss 1.810369 (2.175371)\n",
            "Batch 500/3125\t Loss 2.651938 (2.178330)\n",
            "Batch 600/3125\t Loss 1.651729 (2.175469)\n",
            "Batch 700/3125\t Loss 1.027354 (2.183586)\n",
            "Batch 800/3125\t Loss 1.740189 (2.168195)\n",
            "Batch 900/3125\t Loss 2.929791 (2.175660)\n",
            "Batch 1000/3125\t Loss 2.439662 (2.187994)\n",
            "Batch 1100/3125\t Loss 2.463450 (2.181583)\n",
            "Batch 1200/3125\t Loss 2.129783 (2.181465)\n",
            "Batch 1300/3125\t Loss 1.993060 (2.188101)\n",
            "Batch 1400/3125\t Loss 3.443759 (2.186852)\n",
            "Batch 1500/3125\t Loss 1.580856 (2.186942)\n",
            "Batch 1600/3125\t Loss 2.537081 (2.183845)\n",
            "Batch 1700/3125\t Loss 1.062865 (2.178735)\n",
            "Batch 1800/3125\t Loss 1.717335 (2.178784)\n",
            "Batch 1900/3125\t Loss 2.022529 (2.178995)\n",
            "Batch 2000/3125\t Loss 2.344918 (2.179756)\n",
            "Batch 2100/3125\t Loss 1.611686 (2.176585)\n",
            "Batch 2200/3125\t Loss 1.049762 (2.174048)\n",
            "Batch 2300/3125\t Loss 1.548749 (2.172743)\n",
            "Batch 2400/3125\t Loss 1.574635 (2.176085)\n",
            "Batch 2500/3125\t Loss 1.666075 (2.174253)\n",
            "Batch 2600/3125\t Loss 1.318420 (2.174822)\n",
            "Batch 2700/3125\t Loss 1.490244 (2.175408)\n",
            "Batch 2800/3125\t Loss 3.755411 (2.175974)\n",
            "Batch 2900/3125\t Loss 1.673970 (2.175115)\n",
            "Batch 3000/3125\t Loss 1.746945 (2.174271)\n",
            "Batch 3100/3125\t Loss 2.715705 (2.174482)\n",
            "==> Epoch 30/30\n",
            "Batch 100/3125\t Loss 2.265012 (2.156481)\n",
            "Batch 200/3125\t Loss 1.967224 (2.180069)\n",
            "Batch 300/3125\t Loss 3.438711 (2.153033)\n",
            "Batch 400/3125\t Loss 2.520828 (2.147762)\n",
            "Batch 500/3125\t Loss 1.983433 (2.141289)\n",
            "Batch 600/3125\t Loss 1.511079 (2.150276)\n",
            "Batch 700/3125\t Loss 1.992288 (2.152601)\n",
            "Batch 800/3125\t Loss 2.948814 (2.150817)\n",
            "Batch 900/3125\t Loss 2.621644 (2.161941)\n",
            "Batch 1000/3125\t Loss 2.150521 (2.164341)\n",
            "Batch 1100/3125\t Loss 1.993831 (2.162548)\n",
            "Batch 1200/3125\t Loss 2.602620 (2.158250)\n",
            "Batch 1300/3125\t Loss 2.383878 (2.166741)\n",
            "Batch 1400/3125\t Loss 2.023003 (2.159287)\n",
            "Batch 1500/3125\t Loss 2.624055 (2.162078)\n",
            "Batch 1600/3125\t Loss 2.200436 (2.166416)\n",
            "Batch 1700/3125\t Loss 2.071734 (2.162946)\n",
            "Batch 1800/3125\t Loss 3.462545 (2.165735)\n",
            "Batch 1900/3125\t Loss 2.659863 (2.164608)\n",
            "Batch 2000/3125\t Loss 2.245565 (2.167409)\n",
            "Batch 2100/3125\t Loss 2.699142 (2.164589)\n",
            "Batch 2200/3125\t Loss 2.211888 (2.169893)\n",
            "Batch 2300/3125\t Loss 1.278849 (2.175063)\n",
            "Batch 2400/3125\t Loss 2.832862 (2.175065)\n",
            "Batch 2500/3125\t Loss 2.369803 (2.172853)\n",
            "Batch 2600/3125\t Loss 2.092709 (2.172996)\n",
            "Batch 2700/3125\t Loss 2.183184 (2.170994)\n",
            "Batch 2800/3125\t Loss 1.975428 (2.172551)\n",
            "Batch 2900/3125\t Loss 1.895848 (2.176559)\n",
            "Batch 3000/3125\t Loss 2.492870 (2.176027)\n",
            "Batch 3100/3125\t Loss 1.526268 (2.173843)\n",
            "==> Test\n",
            "       TNR    AUROC  DTACC  AUIN   AUOUT \n",
            "Bas    13.400 70.849 65.750 73.630 66.571\n",
            "Acc: 61.37000\n",
            "Finished. Total elapsed time (h:m:s): 1:08:39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if options['eval']:\n",
        "        net, criterion = load_networks(net, options['outf'], file_name, criterion=criterion)\n",
        "        outloaders = Data.out_loaders\n",
        "        results = test(net, criterion, testloader, outloader, epoch=0, **options)\n",
        "        acc = results['ACC']\n",
        "        res = dict()\n",
        "        res['ACC'] = dict()\n",
        "        acc_res = []\n",
        "        for key in Data.out_keys:\n",
        "            results = test_robustness(net, criterion, outloaders[key], epoch=0, label=key, **options)\n",
        "            print('{} (%): {:.3f}\\t'.format(key, results['ACC']))\n",
        "            res['ACC'][key] = results['ACC']\n",
        "            acc_res.append(results['ACC'])\n",
        "        print('Mean ACC:', np.mean(acc_res))\n",
        "        print('Mean Error:', 100-np.mean(acc_res))"
      ],
      "metadata": {
        "id": "mqmZ2gN1vKcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad5cfa57-07e1-432a-9215-5a36d9259193"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       TNR    AUROC  DTACC  AUIN   AUOUT \n",
            "Bas    13.400 70.849 65.750 73.630 66.571\n",
            "Acc: 61.37000\n",
            "gaussian_noise (%): 35.832\t\n",
            "shot_noise (%): 40.040\t\n",
            "impulse_noise (%): 30.806\t\n",
            "defocus_blur (%): 54.316\t\n",
            "glass_blur (%): 24.294\t\n",
            "motion_blur (%): 44.128\t\n",
            "zoom_blur (%): 50.416\t\n",
            "snow (%): 44.484\t\n",
            "frost (%): 39.116\t\n",
            "fog (%): 51.996\t\n",
            "brightness (%): 55.910\t\n",
            "contrast (%): 41.886\t\n",
            "elastic_transform (%): 51.132\t\n",
            "pixelate (%): 47.178\t\n",
            "jpeg_compression (%): 49.916\t\n",
            "Mean ACC: 44.09666666666666\n",
            "Mean Error: 55.90333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "options['aug'] ='aprs'\n",
        "\n",
        "print(\"Creating model: {}\".format(options['model']))\n",
        "if 'wide_resnet' in options['model']:\n",
        "        print('wide_resnet') \n",
        "        net = WideResNet(40, num_classes, 2, 0.0)\n",
        "elif 'allconv' in options['model']:\n",
        "        print('allconv')\n",
        "        net = AllConvNet(num_classes)\n",
        "elif 'densenet' in options['model']:\n",
        "        print('densenet')\n",
        "        net = densenet(num_classes=num_classes)\n",
        "elif 'resnext' in options['model']:\n",
        "        print('resnext29')\n",
        "        net = resnext29(num_classes)\n",
        "else:\n",
        "        print('resnet18')\n",
        "        net = ResNet18(num_classes=num_classes)\n",
        "\n",
        "torch.manual_seed(options['seed'])\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = options['gpu']\n",
        "use_gpu = torch.cuda.is_available()\n",
        "if options['use_cpu']: use_gpu = False\n",
        "\n",
        "options.update({'use_gpu': use_gpu})\n",
        "options['use_gpu']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "if use_gpu:\n",
        "        net = nn.DataParallel(net, device_ids=[i for i in range(len(options['gpu'].split(',')))]).cuda()\n",
        "        criterion = criterion.cuda()\n",
        "file_name = '{}_{}_{}'.format(options['model'], options['dataset'], options['aug'])\n",
        "params_list = [{'params': net.parameters()},\n",
        "                {'params': criterion.parameters()}]\n",
        "optimizer = torch.optim.SGD(params_list, lr=options['lr'], momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
        "scheduler = lr_scheduler.MultiStepLR(optimizer, gamma=0.2, milestones=[60, 120, 160, 190])\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "best_acc = 0.0"
      ],
      "metadata": {
        "id": "tBUBN1zO08tZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e825bfa4-6111-4ceb-f21e-631c4482680f"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model: resnet18\n",
            "resnet18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(options['max_epoch']):\n",
        "        print(\"==> Epoch {}/{}\".format(epoch+1, options['max_epoch']))\n",
        "\n",
        "        train(net, criterion, optimizer, trainloader, epoch=epoch, **options)\n",
        "\n",
        "        if options['eval_freq'] > 0 and (epoch+1) % options['eval_freq'] == 0 or (epoch+1) == options['max_epoch'] or epoch > 160:\n",
        "            print(\"==> Test\")\n",
        "            results = test(net, criterion, testloader, outloader, epoch=epoch, **options)\n",
        "\n",
        "            if best_acc < results['ACC']:\n",
        "                best_acc = results['ACC']\n",
        "                print(\"Best Acc (%): {:.3f}\\t\".format(best_acc))\n",
        "            \n",
        "            save_networks(net, options['outf'], file_name, criterion=criterion)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "elapsed = round(time.time() - start_time)\n",
        "elapsed = str(datetime.timedelta(seconds=elapsed))\n",
        "print(\"Finished. Total elapsed time (h:m:s): {}\".format(elapsed))"
      ],
      "metadata": {
        "id": "jmz0PTNOWowV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df12873c-75fb-4fc9-e44e-a01b989ae366"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Epoch 1/30\n",
            "Batch 100/3125\t Loss 4.645138 (6.602592)\n",
            "Batch 200/3125\t Loss 4.720240 (5.610760)\n",
            "Batch 300/3125\t Loss 4.599995 (5.284326)\n",
            "Batch 400/3125\t Loss 4.366092 (5.091783)\n",
            "Batch 500/3125\t Loss 4.562318 (4.963527)\n",
            "Batch 600/3125\t Loss 3.920765 (4.873240)\n",
            "Batch 700/3125\t Loss 4.157507 (4.798044)\n",
            "Batch 800/3125\t Loss 4.457466 (4.736681)\n",
            "Batch 900/3125\t Loss 4.088552 (4.682462)\n",
            "Batch 1000/3125\t Loss 4.267346 (4.636492)\n",
            "Batch 1100/3125\t Loss 3.805567 (4.598159)\n",
            "Batch 1200/3125\t Loss 3.609895 (4.566699)\n",
            "Batch 1300/3125\t Loss 3.870838 (4.540388)\n",
            "Batch 1400/3125\t Loss 4.331417 (4.516303)\n",
            "Batch 1500/3125\t Loss 3.544564 (4.490362)\n",
            "Batch 1600/3125\t Loss 4.245543 (4.460930)\n",
            "Batch 1700/3125\t Loss 4.132457 (4.438366)\n",
            "Batch 1800/3125\t Loss 3.484869 (4.415374)\n",
            "Batch 1900/3125\t Loss 3.759391 (4.393358)\n",
            "Batch 2000/3125\t Loss 4.575977 (4.375034)\n",
            "Batch 2100/3125\t Loss 4.326635 (4.356388)\n",
            "Batch 2200/3125\t Loss 3.336173 (4.337988)\n",
            "Batch 2300/3125\t Loss 3.371729 (4.320944)\n",
            "Batch 2400/3125\t Loss 3.808022 (4.303392)\n",
            "Batch 2500/3125\t Loss 3.433015 (4.289072)\n",
            "Batch 2600/3125\t Loss 3.996141 (4.272543)\n",
            "Batch 2700/3125\t Loss 3.578305 (4.260510)\n",
            "Batch 2800/3125\t Loss 3.643664 (4.241620)\n",
            "Batch 2900/3125\t Loss 3.785009 (4.228604)\n",
            "Batch 3000/3125\t Loss 4.040744 (4.212267)\n",
            "Batch 3100/3125\t Loss 3.943716 (4.195857)\n",
            "==> Epoch 2/30\n",
            "Batch 100/3125\t Loss 3.661348 (3.721974)\n",
            "Batch 200/3125\t Loss 3.821374 (3.741941)\n",
            "Batch 300/3125\t Loss 3.523063 (3.770951)\n",
            "Batch 400/3125\t Loss 4.160656 (3.760587)\n",
            "Batch 500/3125\t Loss 3.739206 (3.761340)\n",
            "Batch 600/3125\t Loss 3.832705 (3.748046)\n",
            "Batch 700/3125\t Loss 3.916095 (3.737145)\n",
            "Batch 800/3125\t Loss 3.746966 (3.720245)\n",
            "Batch 900/3125\t Loss 2.754128 (3.704281)\n",
            "Batch 1000/3125\t Loss 3.740821 (3.694213)\n",
            "Batch 1100/3125\t Loss 3.237013 (3.681938)\n",
            "Batch 1200/3125\t Loss 3.343762 (3.670184)\n",
            "Batch 1300/3125\t Loss 4.065110 (3.655387)\n",
            "Batch 1400/3125\t Loss 3.736544 (3.646940)\n",
            "Batch 1500/3125\t Loss 3.768044 (3.629895)\n",
            "Batch 1600/3125\t Loss 2.706228 (3.618504)\n",
            "Batch 1700/3125\t Loss 2.902042 (3.607177)\n",
            "Batch 1800/3125\t Loss 4.234020 (3.597041)\n",
            "Batch 1900/3125\t Loss 3.435518 (3.585122)\n",
            "Batch 2000/3125\t Loss 3.299031 (3.570217)\n",
            "Batch 2100/3125\t Loss 3.926606 (3.553770)\n",
            "Batch 2200/3125\t Loss 2.694628 (3.541666)\n",
            "Batch 2300/3125\t Loss 3.077045 (3.526247)\n",
            "Batch 2400/3125\t Loss 2.799528 (3.512489)\n",
            "Batch 2500/3125\t Loss 3.080050 (3.498973)\n",
            "Batch 2600/3125\t Loss 3.743320 (3.488524)\n",
            "Batch 2700/3125\t Loss 3.674318 (3.474217)\n",
            "Batch 2800/3125\t Loss 3.487933 (3.460066)\n",
            "Batch 2900/3125\t Loss 2.336290 (3.450851)\n",
            "Batch 3000/3125\t Loss 2.634141 (3.439980)\n",
            "Batch 3100/3125\t Loss 3.080420 (3.430218)\n",
            "==> Epoch 3/30\n",
            "Batch 100/3125\t Loss 3.473869 (3.051622)\n",
            "Batch 200/3125\t Loss 3.428596 (3.050499)\n",
            "Batch 300/3125\t Loss 2.367126 (3.034206)\n",
            "Batch 400/3125\t Loss 3.557420 (3.004315)\n",
            "Batch 500/3125\t Loss 2.956173 (2.998437)\n",
            "Batch 600/3125\t Loss 2.835770 (2.981366)\n",
            "Batch 700/3125\t Loss 3.732247 (2.975805)\n",
            "Batch 800/3125\t Loss 3.728759 (2.966278)\n",
            "Batch 900/3125\t Loss 3.750594 (2.951836)\n",
            "Batch 1000/3125\t Loss 2.542242 (2.940824)\n",
            "Batch 1100/3125\t Loss 3.403270 (2.930626)\n",
            "Batch 1200/3125\t Loss 2.047129 (2.921445)\n",
            "Batch 1300/3125\t Loss 1.705823 (2.917197)\n",
            "Batch 1400/3125\t Loss 2.762116 (2.911666)\n",
            "Batch 1500/3125\t Loss 2.474507 (2.905155)\n",
            "Batch 1600/3125\t Loss 3.636245 (2.902037)\n",
            "Batch 1700/3125\t Loss 1.483146 (2.895728)\n",
            "Batch 1800/3125\t Loss 2.176960 (2.887946)\n",
            "Batch 1900/3125\t Loss 2.960162 (2.881523)\n",
            "Batch 2000/3125\t Loss 3.623127 (2.872121)\n",
            "Batch 2100/3125\t Loss 2.301193 (2.863158)\n",
            "Batch 2200/3125\t Loss 4.143651 (2.859615)\n",
            "Batch 2300/3125\t Loss 2.543038 (2.856722)\n",
            "Batch 2400/3125\t Loss 3.896386 (2.848347)\n",
            "Batch 2500/3125\t Loss 2.800199 (2.839148)\n",
            "Batch 2600/3125\t Loss 2.917027 (2.832995)\n",
            "Batch 2700/3125\t Loss 2.917206 (2.828830)\n",
            "Batch 2800/3125\t Loss 2.860074 (2.823733)\n",
            "Batch 2900/3125\t Loss 2.855481 (2.816738)\n",
            "Batch 3000/3125\t Loss 4.044341 (2.809211)\n",
            "Batch 3100/3125\t Loss 3.592022 (2.802498)\n",
            "==> Epoch 4/30\n",
            "Batch 100/3125\t Loss 2.331069 (2.590163)\n",
            "Batch 200/3125\t Loss 3.488430 (2.579554)\n",
            "Batch 300/3125\t Loss 2.329312 (2.587184)\n",
            "Batch 400/3125\t Loss 2.746069 (2.583945)\n",
            "Batch 500/3125\t Loss 2.068326 (2.567432)\n",
            "Batch 600/3125\t Loss 1.691324 (2.578557)\n",
            "Batch 700/3125\t Loss 2.425902 (2.579669)\n",
            "Batch 800/3125\t Loss 2.715852 (2.580697)\n",
            "Batch 900/3125\t Loss 2.682627 (2.578965)\n",
            "Batch 1000/3125\t Loss 3.658372 (2.586745)\n",
            "Batch 1100/3125\t Loss 3.352095 (2.582547)\n",
            "Batch 1200/3125\t Loss 2.669476 (2.579915)\n",
            "Batch 1300/3125\t Loss 3.517814 (2.576357)\n",
            "Batch 1400/3125\t Loss 3.715173 (2.573903)\n",
            "Batch 1500/3125\t Loss 2.037963 (2.569398)\n",
            "Batch 1600/3125\t Loss 2.866380 (2.572126)\n",
            "Batch 1700/3125\t Loss 2.484978 (2.570459)\n",
            "Batch 1800/3125\t Loss 1.782070 (2.569120)\n",
            "Batch 1900/3125\t Loss 3.148046 (2.565673)\n",
            "Batch 2000/3125\t Loss 2.167547 (2.561404)\n",
            "Batch 2100/3125\t Loss 1.435878 (2.558131)\n",
            "Batch 2200/3125\t Loss 2.834514 (2.557713)\n",
            "Batch 2300/3125\t Loss 1.277138 (2.559296)\n",
            "Batch 2400/3125\t Loss 3.456082 (2.562554)\n",
            "Batch 2500/3125\t Loss 2.266878 (2.556670)\n",
            "Batch 2600/3125\t Loss 2.015393 (2.552731)\n",
            "Batch 2700/3125\t Loss 2.512379 (2.556709)\n",
            "Batch 2800/3125\t Loss 3.074413 (2.554194)\n",
            "Batch 2900/3125\t Loss 2.799682 (2.553834)\n",
            "Batch 3000/3125\t Loss 1.650728 (2.551891)\n",
            "Batch 3100/3125\t Loss 2.776731 (2.549722)\n",
            "==> Epoch 5/30\n",
            "Batch 100/3125\t Loss 2.781171 (2.409614)\n",
            "Batch 200/3125\t Loss 1.326932 (2.385105)\n",
            "Batch 300/3125\t Loss 2.424380 (2.440733)\n",
            "Batch 400/3125\t Loss 2.501108 (2.457380)\n",
            "Batch 500/3125\t Loss 2.127007 (2.468478)\n",
            "Batch 600/3125\t Loss 1.867150 (2.478108)\n",
            "Batch 700/3125\t Loss 1.995233 (2.470922)\n",
            "Batch 800/3125\t Loss 2.149442 (2.473249)\n",
            "Batch 900/3125\t Loss 2.496315 (2.474310)\n",
            "Batch 1000/3125\t Loss 2.488427 (2.474973)\n",
            "Batch 1100/3125\t Loss 1.996627 (2.474178)\n",
            "Batch 1200/3125\t Loss 3.369550 (2.469753)\n",
            "Batch 1300/3125\t Loss 2.230591 (2.467653)\n",
            "Batch 1400/3125\t Loss 1.978752 (2.468553)\n",
            "Batch 1500/3125\t Loss 2.677892 (2.466808)\n",
            "Batch 1600/3125\t Loss 1.318063 (2.466951)\n",
            "Batch 1700/3125\t Loss 2.560144 (2.462484)\n",
            "Batch 1800/3125\t Loss 3.387583 (2.461937)\n",
            "Batch 1900/3125\t Loss 2.718171 (2.459952)\n",
            "Batch 2000/3125\t Loss 2.236370 (2.460482)\n",
            "Batch 2100/3125\t Loss 2.911139 (2.456341)\n",
            "Batch 2200/3125\t Loss 2.587620 (2.454490)\n",
            "Batch 2300/3125\t Loss 3.741531 (2.453280)\n",
            "Batch 2400/3125\t Loss 2.769359 (2.450829)\n",
            "Batch 2500/3125\t Loss 2.158447 (2.452314)\n",
            "Batch 2600/3125\t Loss 2.702203 (2.449997)\n",
            "Batch 2700/3125\t Loss 2.702299 (2.452590)\n",
            "Batch 2800/3125\t Loss 3.215055 (2.453549)\n",
            "Batch 2900/3125\t Loss 3.067513 (2.451716)\n",
            "Batch 3000/3125\t Loss 2.182907 (2.452796)\n",
            "Batch 3100/3125\t Loss 1.794275 (2.450825)\n",
            "==> Test\n",
            "       TNR    AUROC  DTACC  AUIN   AUOUT \n",
            "Bas    15.480 72.822 67.365 74.908 68.712\n",
            "Acc: 69.15000\n",
            "Best Acc (%): 69.150\t\n",
            "==> Epoch 6/30\n",
            "Batch 100/3125\t Loss 2.141877 (2.459526)\n",
            "Batch 200/3125\t Loss 1.739954 (2.387569)\n",
            "Batch 300/3125\t Loss 2.509542 (2.398033)\n",
            "Batch 400/3125\t Loss 1.695915 (2.376846)\n",
            "Batch 500/3125\t Loss 2.124576 (2.380911)\n",
            "Batch 600/3125\t Loss 2.429744 (2.385074)\n",
            "Batch 700/3125\t Loss 1.899889 (2.373273)\n",
            "Batch 800/3125\t Loss 1.977695 (2.372875)\n",
            "Batch 900/3125\t Loss 1.818086 (2.370256)\n",
            "Batch 1000/3125\t Loss 3.049170 (2.372289)\n",
            "Batch 1100/3125\t Loss 2.013478 (2.381831)\n",
            "Batch 1200/3125\t Loss 2.517542 (2.388692)\n",
            "Batch 1300/3125\t Loss 1.623267 (2.381841)\n",
            "Batch 1400/3125\t Loss 1.757822 (2.389610)\n",
            "Batch 1500/3125\t Loss 2.563387 (2.387120)\n",
            "Batch 1600/3125\t Loss 2.828626 (2.387332)\n",
            "Batch 1700/3125\t Loss 2.088519 (2.385805)\n",
            "Batch 1800/3125\t Loss 3.336193 (2.391605)\n",
            "Batch 1900/3125\t Loss 2.732436 (2.394094)\n",
            "Batch 2000/3125\t Loss 2.638627 (2.393065)\n",
            "Batch 2100/3125\t Loss 1.144657 (2.392420)\n",
            "Batch 2200/3125\t Loss 1.516981 (2.392635)\n",
            "Batch 2300/3125\t Loss 2.967227 (2.389026)\n",
            "Batch 2400/3125\t Loss 1.028506 (2.388937)\n",
            "Batch 2500/3125\t Loss 2.426136 (2.386281)\n",
            "Batch 2600/3125\t Loss 0.996904 (2.384299)\n",
            "Batch 2700/3125\t Loss 2.274375 (2.384797)\n",
            "Batch 2800/3125\t Loss 1.857339 (2.384581)\n",
            "Batch 2900/3125\t Loss 2.281505 (2.381613)\n",
            "Batch 3000/3125\t Loss 1.965154 (2.382007)\n",
            "Batch 3100/3125\t Loss 3.131105 (2.383748)\n",
            "==> Epoch 7/30\n",
            "Batch 100/3125\t Loss 1.982651 (2.250086)\n",
            "Batch 200/3125\t Loss 2.240310 (2.349383)\n",
            "Batch 300/3125\t Loss 2.527872 (2.352046)\n",
            "Batch 400/3125\t Loss 1.945076 (2.308204)\n",
            "Batch 500/3125\t Loss 1.492055 (2.320833)\n",
            "Batch 600/3125\t Loss 2.178050 (2.318255)\n",
            "Batch 700/3125\t Loss 2.346015 (2.319929)\n",
            "Batch 800/3125\t Loss 1.839313 (2.321531)\n",
            "Batch 900/3125\t Loss 3.189102 (2.323100)\n",
            "Batch 1000/3125\t Loss 1.800927 (2.319222)\n",
            "Batch 1100/3125\t Loss 3.500466 (2.324982)\n",
            "Batch 1200/3125\t Loss 2.306775 (2.322087)\n",
            "Batch 1300/3125\t Loss 1.086400 (2.327131)\n",
            "Batch 1400/3125\t Loss 2.937308 (2.325813)\n",
            "Batch 1500/3125\t Loss 2.200736 (2.332964)\n",
            "Batch 1600/3125\t Loss 2.482090 (2.335418)\n",
            "Batch 1700/3125\t Loss 1.456640 (2.334401)\n",
            "Batch 1800/3125\t Loss 2.084888 (2.336263)\n",
            "Batch 1900/3125\t Loss 2.212637 (2.332428)\n",
            "Batch 2000/3125\t Loss 2.469388 (2.335245)\n",
            "Batch 2100/3125\t Loss 2.247107 (2.336972)\n",
            "Batch 2200/3125\t Loss 1.983958 (2.334991)\n",
            "Batch 2300/3125\t Loss 2.823162 (2.339542)\n",
            "Batch 2400/3125\t Loss 1.908083 (2.339420)\n",
            "Batch 2500/3125\t Loss 2.667069 (2.335901)\n",
            "Batch 2600/3125\t Loss 2.428437 (2.336665)\n",
            "Batch 2700/3125\t Loss 1.988316 (2.332921)\n",
            "Batch 2800/3125\t Loss 2.288658 (2.333547)\n",
            "Batch 2900/3125\t Loss 1.752896 (2.328473)\n",
            "Batch 3000/3125\t Loss 2.419740 (2.331269)\n",
            "Batch 3100/3125\t Loss 1.805898 (2.330012)\n",
            "==> Epoch 8/30\n",
            "Batch 100/3125\t Loss 2.147200 (2.220587)\n",
            "Batch 200/3125\t Loss 1.998556 (2.287967)\n",
            "Batch 300/3125\t Loss 2.189363 (2.291590)\n",
            "Batch 400/3125\t Loss 3.012328 (2.309688)\n",
            "Batch 500/3125\t Loss 3.368984 (2.315828)\n",
            "Batch 600/3125\t Loss 1.443465 (2.297659)\n",
            "Batch 700/3125\t Loss 2.696331 (2.303798)\n",
            "Batch 800/3125\t Loss 1.944831 (2.307319)\n",
            "Batch 900/3125\t Loss 2.182225 (2.310702)\n",
            "Batch 1000/3125\t Loss 2.210017 (2.306659)\n",
            "Batch 1100/3125\t Loss 1.869120 (2.306094)\n",
            "Batch 1200/3125\t Loss 1.955603 (2.304460)\n",
            "Batch 1300/3125\t Loss 1.841674 (2.304420)\n",
            "Batch 1400/3125\t Loss 2.671246 (2.300576)\n",
            "Batch 1500/3125\t Loss 2.465731 (2.299840)\n",
            "Batch 1600/3125\t Loss 1.809823 (2.301066)\n",
            "Batch 1700/3125\t Loss 2.088395 (2.303941)\n",
            "Batch 1800/3125\t Loss 3.524080 (2.303439)\n",
            "Batch 1900/3125\t Loss 1.341426 (2.304364)\n",
            "Batch 2000/3125\t Loss 3.136648 (2.302311)\n",
            "Batch 2100/3125\t Loss 2.326931 (2.304239)\n",
            "Batch 2200/3125\t Loss 3.360394 (2.304525)\n",
            "Batch 2300/3125\t Loss 2.941783 (2.304159)\n",
            "Batch 2400/3125\t Loss 2.802382 (2.300296)\n",
            "Batch 2500/3125\t Loss 2.932260 (2.299833)\n",
            "Batch 2600/3125\t Loss 2.487677 (2.301875)\n",
            "Batch 2700/3125\t Loss 2.220108 (2.299153)\n",
            "Batch 2800/3125\t Loss 3.055383 (2.296787)\n",
            "Batch 2900/3125\t Loss 1.705819 (2.298675)\n",
            "Batch 3000/3125\t Loss 1.056249 (2.297559)\n",
            "Batch 3100/3125\t Loss 3.101243 (2.298325)\n",
            "==> Epoch 9/30\n",
            "Batch 100/3125\t Loss 2.310479 (2.215932)\n",
            "Batch 200/3125\t Loss 2.327040 (2.285565)\n",
            "Batch 300/3125\t Loss 1.761860 (2.321347)\n",
            "Batch 400/3125\t Loss 2.256828 (2.294323)\n",
            "Batch 500/3125\t Loss 1.330309 (2.291583)\n",
            "Batch 600/3125\t Loss 2.639274 (2.299264)\n",
            "Batch 700/3125\t Loss 2.349726 (2.297508)\n",
            "Batch 800/3125\t Loss 2.700776 (2.292001)\n",
            "Batch 900/3125\t Loss 2.198267 (2.297313)\n",
            "Batch 1000/3125\t Loss 3.158774 (2.303350)\n",
            "Batch 1100/3125\t Loss 2.107734 (2.299625)\n",
            "Batch 1200/3125\t Loss 2.382879 (2.301167)\n",
            "Batch 1300/3125\t Loss 2.915399 (2.303039)\n",
            "Batch 1400/3125\t Loss 2.174660 (2.297584)\n",
            "Batch 1500/3125\t Loss 2.835986 (2.292392)\n",
            "Batch 1600/3125\t Loss 2.885466 (2.295435)\n",
            "Batch 1700/3125\t Loss 1.824104 (2.292412)\n",
            "Batch 1800/3125\t Loss 3.801650 (2.296858)\n",
            "Batch 1900/3125\t Loss 1.533630 (2.294238)\n",
            "Batch 2000/3125\t Loss 2.027726 (2.293409)\n",
            "Batch 2100/3125\t Loss 2.393430 (2.296415)\n",
            "Batch 2200/3125\t Loss 1.482382 (2.298226)\n",
            "Batch 2300/3125\t Loss 2.229082 (2.300735)\n",
            "Batch 2400/3125\t Loss 2.926370 (2.300036)\n",
            "Batch 2500/3125\t Loss 2.453908 (2.295802)\n",
            "Batch 2600/3125\t Loss 1.864780 (2.292191)\n",
            "Batch 2700/3125\t Loss 2.614720 (2.289132)\n",
            "Batch 2800/3125\t Loss 2.260828 (2.286995)\n",
            "Batch 2900/3125\t Loss 2.525192 (2.289189)\n",
            "Batch 3000/3125\t Loss 2.107822 (2.290627)\n",
            "Batch 3100/3125\t Loss 1.462359 (2.291024)\n",
            "==> Epoch 10/30\n",
            "Batch 100/3125\t Loss 2.028765 (2.202175)\n",
            "Batch 200/3125\t Loss 3.027275 (2.237621)\n",
            "Batch 300/3125\t Loss 2.257913 (2.267189)\n",
            "Batch 400/3125\t Loss 2.139805 (2.245942)\n",
            "Batch 500/3125\t Loss 2.593208 (2.261227)\n",
            "Batch 600/3125\t Loss 1.569900 (2.270975)\n",
            "Batch 700/3125\t Loss 2.960142 (2.269495)\n",
            "Batch 800/3125\t Loss 3.537332 (2.280512)\n",
            "Batch 900/3125\t Loss 2.644713 (2.272957)\n",
            "Batch 1000/3125\t Loss 1.379543 (2.269661)\n",
            "Batch 1100/3125\t Loss 3.188320 (2.274292)\n",
            "Batch 1200/3125\t Loss 2.245924 (2.266280)\n",
            "Batch 1300/3125\t Loss 2.155539 (2.274348)\n",
            "Batch 1400/3125\t Loss 2.831553 (2.277560)\n",
            "Batch 1500/3125\t Loss 3.657302 (2.280123)\n",
            "Batch 1600/3125\t Loss 1.675268 (2.277826)\n",
            "Batch 1700/3125\t Loss 2.084510 (2.278102)\n",
            "Batch 1800/3125\t Loss 1.938383 (2.277706)\n",
            "Batch 1900/3125\t Loss 2.395731 (2.276773)\n",
            "Batch 2000/3125\t Loss 1.973704 (2.275322)\n",
            "Batch 2100/3125\t Loss 2.303999 (2.276414)\n",
            "Batch 2200/3125\t Loss 1.853892 (2.274281)\n",
            "Batch 2300/3125\t Loss 2.248391 (2.276382)\n",
            "Batch 2400/3125\t Loss 2.578755 (2.277681)\n",
            "Batch 2500/3125\t Loss 1.526501 (2.277598)\n",
            "Batch 2600/3125\t Loss 2.073257 (2.276548)\n",
            "Batch 2700/3125\t Loss 1.177989 (2.275138)\n",
            "Batch 2800/3125\t Loss 1.710240 (2.277712)\n",
            "Batch 2900/3125\t Loss 1.924289 (2.275241)\n",
            "Batch 3000/3125\t Loss 1.992998 (2.275972)\n",
            "Batch 3100/3125\t Loss 2.107893 (2.278057)\n",
            "==> Test\n",
            "       TNR    AUROC  DTACC  AUIN   AUOUT \n",
            "Bas    14.870 72.362 66.815 74.634 68.422\n",
            "Acc: 63.90000\n",
            "==> Epoch 11/30\n",
            "Batch 100/3125\t Loss 2.659841 (2.306429)\n",
            "Batch 200/3125\t Loss 1.528004 (2.262642)\n",
            "Batch 300/3125\t Loss 2.264200 (2.273507)\n",
            "Batch 400/3125\t Loss 2.605820 (2.307389)\n",
            "Batch 500/3125\t Loss 2.888949 (2.293283)\n",
            "Batch 600/3125\t Loss 1.820430 (2.282949)\n",
            "Batch 700/3125\t Loss 1.773719 (2.277672)\n",
            "Batch 800/3125\t Loss 0.770470 (2.277807)\n",
            "Batch 900/3125\t Loss 2.963091 (2.275353)\n",
            "Batch 1000/3125\t Loss 2.906940 (2.270029)\n",
            "Batch 1100/3125\t Loss 1.600394 (2.270744)\n",
            "Batch 1200/3125\t Loss 1.674709 (2.271698)\n",
            "Batch 1300/3125\t Loss 1.783330 (2.266453)\n",
            "Batch 1400/3125\t Loss 2.344031 (2.262036)\n",
            "Batch 1500/3125\t Loss 2.007117 (2.266668)\n",
            "Batch 1600/3125\t Loss 2.192582 (2.259425)\n",
            "Batch 1700/3125\t Loss 1.862436 (2.262115)\n",
            "Batch 1800/3125\t Loss 3.865936 (2.262484)\n",
            "Batch 1900/3125\t Loss 2.636464 (2.257340)\n",
            "Batch 2000/3125\t Loss 2.069877 (2.255450)\n",
            "Batch 2100/3125\t Loss 2.653598 (2.259796)\n",
            "Batch 2200/3125\t Loss 1.807204 (2.258669)\n",
            "Batch 2300/3125\t Loss 1.398506 (2.257932)\n",
            "Batch 2400/3125\t Loss 2.480634 (2.256267)\n",
            "Batch 2500/3125\t Loss 2.610955 (2.254830)\n",
            "Batch 2600/3125\t Loss 2.534989 (2.257152)\n",
            "Batch 2700/3125\t Loss 1.564939 (2.257343)\n",
            "Batch 2800/3125\t Loss 2.097381 (2.256502)\n",
            "Batch 2900/3125\t Loss 1.995852 (2.256669)\n",
            "Batch 3000/3125\t Loss 2.126897 (2.256074)\n",
            "Batch 3100/3125\t Loss 0.750859 (2.256745)\n",
            "==> Epoch 12/30\n",
            "Batch 100/3125\t Loss 2.401634 (2.226181)\n",
            "Batch 200/3125\t Loss 1.687702 (2.227970)\n",
            "Batch 300/3125\t Loss 1.532405 (2.214012)\n",
            "Batch 400/3125\t Loss 2.815732 (2.235784)\n",
            "Batch 500/3125\t Loss 2.741656 (2.235044)\n",
            "Batch 600/3125\t Loss 4.037484 (2.242831)\n",
            "Batch 700/3125\t Loss 2.146197 (2.231158)\n",
            "Batch 800/3125\t Loss 2.282177 (2.243512)\n",
            "Batch 900/3125\t Loss 2.656161 (2.243387)\n",
            "Batch 1000/3125\t Loss 1.854210 (2.243070)\n",
            "Batch 1100/3125\t Loss 2.190336 (2.244722)\n",
            "Batch 1200/3125\t Loss 1.521476 (2.241220)\n",
            "Batch 1300/3125\t Loss 1.880972 (2.242957)\n",
            "Batch 1400/3125\t Loss 2.147224 (2.241265)\n",
            "Batch 1500/3125\t Loss 1.771478 (2.243864)\n",
            "Batch 1600/3125\t Loss 3.363801 (2.246388)\n",
            "Batch 1700/3125\t Loss 2.556859 (2.246712)\n",
            "Batch 1800/3125\t Loss 1.608436 (2.244644)\n",
            "Batch 1900/3125\t Loss 2.529344 (2.243550)\n",
            "Batch 2000/3125\t Loss 2.214376 (2.241873)\n",
            "Batch 2100/3125\t Loss 3.086627 (2.241882)\n",
            "Batch 2200/3125\t Loss 1.805793 (2.240001)\n",
            "Batch 2300/3125\t Loss 0.666504 (2.240039)\n",
            "Batch 2400/3125\t Loss 2.499950 (2.240427)\n",
            "Batch 2500/3125\t Loss 2.072641 (2.239189)\n",
            "Batch 2600/3125\t Loss 2.600809 (2.239982)\n",
            "Batch 2700/3125\t Loss 2.552957 (2.236838)\n",
            "Batch 2800/3125\t Loss 1.820227 (2.241003)\n",
            "Batch 2900/3125\t Loss 1.863194 (2.242640)\n",
            "Batch 3000/3125\t Loss 2.381062 (2.243524)\n",
            "Batch 3100/3125\t Loss 1.320382 (2.242858)\n",
            "==> Epoch 13/30\n",
            "Batch 100/3125\t Loss 1.798794 (2.186767)\n",
            "Batch 200/3125\t Loss 1.699063 (2.176846)\n",
            "Batch 300/3125\t Loss 3.057858 (2.192547)\n",
            "Batch 400/3125\t Loss 2.971946 (2.215932)\n",
            "Batch 500/3125\t Loss 2.996631 (2.208029)\n",
            "Batch 600/3125\t Loss 2.553650 (2.219225)\n",
            "Batch 700/3125\t Loss 2.950652 (2.223024)\n",
            "Batch 800/3125\t Loss 4.374426 (2.225608)\n",
            "Batch 900/3125\t Loss 2.168138 (2.208634)\n",
            "Batch 1000/3125\t Loss 2.191231 (2.207816)\n",
            "Batch 1100/3125\t Loss 2.064468 (2.210389)\n",
            "Batch 1200/3125\t Loss 1.487861 (2.209800)\n",
            "Batch 1300/3125\t Loss 3.001997 (2.215311)\n",
            "Batch 1400/3125\t Loss 2.188327 (2.215654)\n",
            "Batch 1500/3125\t Loss 3.042979 (2.220238)\n",
            "Batch 1600/3125\t Loss 3.243527 (2.221787)\n",
            "Batch 1700/3125\t Loss 2.564643 (2.225206)\n",
            "Batch 1800/3125\t Loss 1.892417 (2.224337)\n",
            "Batch 1900/3125\t Loss 2.164202 (2.224410)\n",
            "Batch 2000/3125\t Loss 2.230871 (2.225791)\n",
            "Batch 2100/3125\t Loss 3.002792 (2.224275)\n",
            "Batch 2200/3125\t Loss 1.996571 (2.223978)\n",
            "Batch 2300/3125\t Loss 1.547482 (2.221637)\n",
            "Batch 2400/3125\t Loss 3.281671 (2.223880)\n",
            "Batch 2500/3125\t Loss 3.666256 (2.226773)\n",
            "Batch 2600/3125\t Loss 2.134619 (2.226780)\n",
            "Batch 2700/3125\t Loss 1.961684 (2.226072)\n",
            "Batch 2800/3125\t Loss 3.436291 (2.225821)\n",
            "Batch 2900/3125\t Loss 1.440821 (2.225633)\n",
            "Batch 3000/3125\t Loss 1.698931 (2.224771)\n",
            "Batch 3100/3125\t Loss 1.630160 (2.227851)\n",
            "==> Epoch 14/30\n",
            "Batch 100/3125\t Loss 2.307261 (2.131582)\n",
            "Batch 200/3125\t Loss 2.542426 (2.204601)\n",
            "Batch 300/3125\t Loss 1.104993 (2.190930)\n",
            "Batch 400/3125\t Loss 3.200012 (2.201537)\n",
            "Batch 500/3125\t Loss 1.656697 (2.193111)\n",
            "Batch 600/3125\t Loss 2.994846 (2.211412)\n",
            "Batch 700/3125\t Loss 1.760266 (2.196124)\n",
            "Batch 800/3125\t Loss 2.500684 (2.197007)\n",
            "Batch 900/3125\t Loss 2.613418 (2.193015)\n",
            "Batch 1000/3125\t Loss 2.487974 (2.205995)\n",
            "Batch 1100/3125\t Loss 2.419811 (2.209576)\n",
            "Batch 1200/3125\t Loss 3.613533 (2.219941)\n",
            "Batch 1300/3125\t Loss 1.942566 (2.216941)\n",
            "Batch 1400/3125\t Loss 2.664441 (2.217361)\n",
            "Batch 1500/3125\t Loss 2.500400 (2.212671)\n",
            "Batch 1600/3125\t Loss 2.566092 (2.211668)\n",
            "Batch 1700/3125\t Loss 1.983072 (2.210436)\n",
            "Batch 1800/3125\t Loss 1.488125 (2.207157)\n",
            "Batch 1900/3125\t Loss 2.432283 (2.211921)\n",
            "Batch 2000/3125\t Loss 3.506867 (2.213180)\n",
            "Batch 2100/3125\t Loss 2.195682 (2.212404)\n",
            "Batch 2200/3125\t Loss 1.815668 (2.208214)\n",
            "Batch 2300/3125\t Loss 1.615708 (2.212888)\n",
            "Batch 2400/3125\t Loss 2.134524 (2.214351)\n",
            "Batch 2500/3125\t Loss 2.589880 (2.214620)\n",
            "Batch 2600/3125\t Loss 1.767413 (2.213737)\n",
            "Batch 2700/3125\t Loss 1.483197 (2.213116)\n",
            "Batch 2800/3125\t Loss 3.253190 (2.209972)\n",
            "Batch 2900/3125\t Loss 2.339264 (2.209958)\n",
            "Batch 3000/3125\t Loss 2.054205 (2.211859)\n",
            "Batch 3100/3125\t Loss 2.717897 (2.212530)\n",
            "==> Epoch 15/30\n",
            "Batch 100/3125\t Loss 1.580494 (2.164290)\n",
            "Batch 200/3125\t Loss 2.279999 (2.151967)\n",
            "Batch 300/3125\t Loss 3.084070 (2.188458)\n",
            "Batch 400/3125\t Loss 2.382878 (2.152667)\n",
            "Batch 500/3125\t Loss 2.384363 (2.176052)\n",
            "Batch 600/3125\t Loss 1.964021 (2.174208)\n",
            "Batch 700/3125\t Loss 2.010385 (2.186057)\n",
            "Batch 800/3125\t Loss 2.239920 (2.185416)\n",
            "Batch 900/3125\t Loss 2.037679 (2.198035)\n",
            "Batch 1000/3125\t Loss 1.573769 (2.198531)\n",
            "Batch 1100/3125\t Loss 2.405500 (2.192414)\n",
            "Batch 1200/3125\t Loss 1.979796 (2.188835)\n",
            "Batch 1300/3125\t Loss 1.233050 (2.191504)\n",
            "Batch 1400/3125\t Loss 1.974660 (2.197105)\n",
            "Batch 1500/3125\t Loss 2.576569 (2.203518)\n",
            "Batch 1600/3125\t Loss 2.248729 (2.203560)\n",
            "Batch 1700/3125\t Loss 1.725384 (2.201107)\n",
            "Batch 1800/3125\t Loss 2.375530 (2.206498)\n",
            "Batch 1900/3125\t Loss 3.091216 (2.209454)\n",
            "Batch 2000/3125\t Loss 2.135900 (2.209999)\n",
            "Batch 2100/3125\t Loss 3.246772 (2.215018)\n",
            "Batch 2200/3125\t Loss 3.005354 (2.215110)\n",
            "Batch 2300/3125\t Loss 1.684330 (2.217770)\n",
            "Batch 2400/3125\t Loss 1.316504 (2.219535)\n",
            "Batch 2500/3125\t Loss 1.907454 (2.219103)\n",
            "Batch 2600/3125\t Loss 1.099739 (2.218478)\n",
            "Batch 2700/3125\t Loss 1.102522 (2.220099)\n",
            "Batch 2800/3125\t Loss 2.371960 (2.217108)\n",
            "Batch 2900/3125\t Loss 2.344168 (2.218195)\n",
            "Batch 3000/3125\t Loss 2.178083 (2.217011)\n",
            "Batch 3100/3125\t Loss 2.864441 (2.218814)\n",
            "==> Test\n",
            "       TNR    AUROC  DTACC  AUIN   AUOUT \n",
            "Bas    10.810 62.983 58.780 64.073 60.993\n",
            "Acc: 61.69000\n",
            "==> Epoch 16/30\n",
            "Batch 100/3125\t Loss 1.451102 (2.071782)\n",
            "Batch 200/3125\t Loss 1.972589 (2.155991)\n",
            "Batch 300/3125\t Loss 1.795262 (2.170427)\n",
            "Batch 400/3125\t Loss 3.058239 (2.183588)\n",
            "Batch 500/3125\t Loss 2.992795 (2.183082)\n",
            "Batch 600/3125\t Loss 2.633228 (2.193724)\n",
            "Batch 700/3125\t Loss 2.501427 (2.202301)\n",
            "Batch 800/3125\t Loss 2.294359 (2.208548)\n",
            "Batch 900/3125\t Loss 1.421262 (2.199256)\n",
            "Batch 1000/3125\t Loss 3.393541 (2.195472)\n",
            "Batch 1100/3125\t Loss 1.698247 (2.200706)\n",
            "Batch 1200/3125\t Loss 2.120471 (2.195879)\n",
            "Batch 1300/3125\t Loss 2.798556 (2.196943)\n",
            "Batch 1400/3125\t Loss 1.942751 (2.195624)\n",
            "Batch 1500/3125\t Loss 2.154253 (2.197249)\n",
            "Batch 1600/3125\t Loss 2.165193 (2.197037)\n",
            "Batch 1700/3125\t Loss 1.288955 (2.192716)\n",
            "Batch 1800/3125\t Loss 3.021819 (2.189754)\n",
            "Batch 1900/3125\t Loss 2.421527 (2.190642)\n",
            "Batch 2000/3125\t Loss 1.390926 (2.191055)\n",
            "Batch 2100/3125\t Loss 1.935114 (2.193338)\n",
            "Batch 2200/3125\t Loss 2.533644 (2.189266)\n",
            "Batch 2300/3125\t Loss 2.373313 (2.189235)\n",
            "Batch 2400/3125\t Loss 1.914189 (2.187433)\n",
            "Batch 2500/3125\t Loss 2.682881 (2.186631)\n",
            "Batch 2600/3125\t Loss 1.975586 (2.186418)\n",
            "Batch 2700/3125\t Loss 1.740715 (2.186310)\n",
            "Batch 2800/3125\t Loss 1.855242 (2.189611)\n",
            "Batch 2900/3125\t Loss 2.987494 (2.191660)\n",
            "Batch 3000/3125\t Loss 1.541077 (2.190538)\n",
            "Batch 3100/3125\t Loss 1.898457 (2.193143)\n",
            "==> Epoch 17/30\n",
            "Batch 100/3125\t Loss 3.352624 (2.177042)\n",
            "Batch 200/3125\t Loss 2.369920 (2.260186)\n",
            "Batch 300/3125\t Loss 2.667298 (2.259918)\n",
            "Batch 400/3125\t Loss 1.873662 (2.257203)\n",
            "Batch 500/3125\t Loss 0.808667 (2.242733)\n",
            "Batch 600/3125\t Loss 1.962815 (2.238299)\n",
            "Batch 700/3125\t Loss 1.466609 (2.223290)\n",
            "Batch 800/3125\t Loss 2.602246 (2.212553)\n",
            "Batch 900/3125\t Loss 2.407693 (2.211948)\n",
            "Batch 1000/3125\t Loss 2.776897 (2.217213)\n",
            "Batch 1100/3125\t Loss 1.868125 (2.210212)\n",
            "Batch 1200/3125\t Loss 2.004349 (2.216195)\n",
            "Batch 1300/3125\t Loss 1.877226 (2.217448)\n",
            "Batch 1400/3125\t Loss 1.513401 (2.214324)\n",
            "Batch 1500/3125\t Loss 3.013578 (2.209592)\n",
            "Batch 1600/3125\t Loss 2.178257 (2.208630)\n",
            "Batch 1700/3125\t Loss 2.063949 (2.209889)\n",
            "Batch 1800/3125\t Loss 2.531096 (2.212210)\n",
            "Batch 1900/3125\t Loss 2.600139 (2.210447)\n",
            "Batch 2000/3125\t Loss 1.790565 (2.209678)\n",
            "Batch 2100/3125\t Loss 3.134591 (2.211569)\n",
            "Batch 2200/3125\t Loss 1.203693 (2.203523)\n",
            "Batch 2300/3125\t Loss 2.778476 (2.201039)\n",
            "Batch 2400/3125\t Loss 2.064544 (2.201592)\n",
            "Batch 2500/3125\t Loss 2.334840 (2.198200)\n",
            "Batch 2600/3125\t Loss 0.953455 (2.195021)\n",
            "Batch 2700/3125\t Loss 1.937943 (2.196156)\n",
            "Batch 2800/3125\t Loss 2.369025 (2.199982)\n",
            "Batch 2900/3125\t Loss 1.774374 (2.199648)\n",
            "Batch 3000/3125\t Loss 2.444872 (2.200268)\n",
            "Batch 3100/3125\t Loss 2.675552 (2.200267)\n",
            "==> Epoch 18/30\n",
            "Batch 100/3125\t Loss 2.618527 (2.147147)\n",
            "Batch 200/3125\t Loss 2.670857 (2.190536)\n",
            "Batch 300/3125\t Loss 1.982524 (2.194849)\n",
            "Batch 400/3125\t Loss 1.542086 (2.176020)\n",
            "Batch 500/3125\t Loss 1.062365 (2.176881)\n",
            "Batch 600/3125\t Loss 2.266912 (2.172400)\n",
            "Batch 700/3125\t Loss 1.930429 (2.184125)\n",
            "Batch 800/3125\t Loss 2.906093 (2.190887)\n",
            "Batch 900/3125\t Loss 2.315588 (2.201972)\n",
            "Batch 1000/3125\t Loss 1.146067 (2.195448)\n",
            "Batch 1100/3125\t Loss 2.656530 (2.201376)\n",
            "Batch 1200/3125\t Loss 2.236979 (2.198921)\n",
            "Batch 1300/3125\t Loss 1.842174 (2.198440)\n",
            "Batch 1400/3125\t Loss 1.525722 (2.195629)\n",
            "Batch 1500/3125\t Loss 1.939786 (2.190151)\n",
            "Batch 1600/3125\t Loss 2.197756 (2.191351)\n",
            "Batch 1700/3125\t Loss 1.752280 (2.191470)\n",
            "Batch 1800/3125\t Loss 0.806215 (2.188743)\n",
            "Batch 1900/3125\t Loss 2.295009 (2.186344)\n",
            "Batch 2000/3125\t Loss 2.116617 (2.192604)\n",
            "Batch 2100/3125\t Loss 2.530195 (2.194752)\n",
            "Batch 2200/3125\t Loss 2.644614 (2.196537)\n",
            "Batch 2300/3125\t Loss 1.228373 (2.190536)\n",
            "Batch 2400/3125\t Loss 2.714852 (2.192725)\n",
            "Batch 2500/3125\t Loss 2.399480 (2.194722)\n",
            "Batch 2600/3125\t Loss 3.926157 (2.194880)\n",
            "Batch 2700/3125\t Loss 1.483419 (2.191035)\n",
            "Batch 2800/3125\t Loss 1.812527 (2.188194)\n",
            "Batch 2900/3125\t Loss 2.138661 (2.187767)\n",
            "Batch 3000/3125\t Loss 2.114791 (2.188486)\n",
            "Batch 3100/3125\t Loss 0.856793 (2.189546)\n",
            "==> Epoch 19/30\n",
            "Batch 100/3125\t Loss 2.484660 (2.136549)\n",
            "Batch 200/3125\t Loss 1.090151 (2.168375)\n",
            "Batch 300/3125\t Loss 3.149797 (2.147433)\n",
            "Batch 400/3125\t Loss 2.548737 (2.165852)\n",
            "Batch 500/3125\t Loss 1.866129 (2.174416)\n",
            "Batch 600/3125\t Loss 2.017401 (2.172046)\n",
            "Batch 700/3125\t Loss 1.128257 (2.169096)\n",
            "Batch 800/3125\t Loss 1.383434 (2.168901)\n",
            "Batch 900/3125\t Loss 2.714855 (2.170445)\n",
            "Batch 1000/3125\t Loss 2.581908 (2.171822)\n",
            "Batch 1100/3125\t Loss 2.461890 (2.177724)\n",
            "Batch 1200/3125\t Loss 0.473245 (2.187716)\n",
            "Batch 1300/3125\t Loss 1.698794 (2.185769)\n",
            "Batch 1400/3125\t Loss 3.217828 (2.197295)\n",
            "Batch 1500/3125\t Loss 2.171847 (2.198446)\n",
            "Batch 1600/3125\t Loss 2.133430 (2.199086)\n",
            "Batch 1700/3125\t Loss 1.889804 (2.200612)\n",
            "Batch 1800/3125\t Loss 2.834342 (2.203833)\n",
            "Batch 1900/3125\t Loss 1.298960 (2.203394)\n",
            "Batch 2000/3125\t Loss 2.321107 (2.197040)\n",
            "Batch 2100/3125\t Loss 2.198479 (2.194366)\n",
            "Batch 2200/3125\t Loss 3.351647 (2.192481)\n",
            "Batch 2300/3125\t Loss 1.632471 (2.192320)\n",
            "Batch 2400/3125\t Loss 1.733869 (2.193219)\n",
            "Batch 2500/3125\t Loss 1.885062 (2.196955)\n",
            "Batch 2600/3125\t Loss 2.242997 (2.193666)\n",
            "Batch 2700/3125\t Loss 1.486523 (2.190026)\n",
            "Batch 2800/3125\t Loss 1.699421 (2.186932)\n",
            "Batch 2900/3125\t Loss 2.548955 (2.191622)\n",
            "Batch 3000/3125\t Loss 2.074635 (2.190178)\n",
            "Batch 3100/3125\t Loss 1.412022 (2.189142)\n",
            "==> Epoch 20/30\n",
            "Batch 100/3125\t Loss 2.024424 (2.190186)\n",
            "Batch 200/3125\t Loss 1.872983 (2.177086)\n",
            "Batch 300/3125\t Loss 1.258621 (2.174085)\n",
            "Batch 400/3125\t Loss 2.056309 (2.161448)\n",
            "Batch 500/3125\t Loss 2.623015 (2.167572)\n",
            "Batch 600/3125\t Loss 2.503626 (2.173920)\n",
            "Batch 700/3125\t Loss 3.398907 (2.178293)\n",
            "Batch 800/3125\t Loss 2.508024 (2.176952)\n",
            "Batch 900/3125\t Loss 1.511520 (2.161437)\n",
            "Batch 1000/3125\t Loss 2.478875 (2.160173)\n",
            "Batch 1100/3125\t Loss 1.572378 (2.157153)\n",
            "Batch 1200/3125\t Loss 1.339741 (2.158271)\n",
            "Batch 1300/3125\t Loss 2.186798 (2.169748)\n",
            "Batch 1400/3125\t Loss 2.230224 (2.172304)\n",
            "Batch 1500/3125\t Loss 1.965955 (2.177434)\n",
            "Batch 1600/3125\t Loss 1.878210 (2.176476)\n",
            "Batch 1700/3125\t Loss 2.187322 (2.177721)\n",
            "Batch 1800/3125\t Loss 2.296097 (2.182740)\n",
            "Batch 1900/3125\t Loss 3.100199 (2.184032)\n",
            "Batch 2000/3125\t Loss 2.228559 (2.179626)\n",
            "Batch 2100/3125\t Loss 2.099518 (2.181822)\n",
            "Batch 2200/3125\t Loss 2.217291 (2.181949)\n",
            "Batch 2300/3125\t Loss 2.941998 (2.180153)\n",
            "Batch 2400/3125\t Loss 1.913159 (2.178789)\n",
            "Batch 2500/3125\t Loss 2.234166 (2.180550)\n",
            "Batch 2600/3125\t Loss 2.165601 (2.183750)\n",
            "Batch 2700/3125\t Loss 2.651299 (2.187171)\n",
            "Batch 2800/3125\t Loss 1.973435 (2.185119)\n",
            "Batch 2900/3125\t Loss 1.769300 (2.186892)\n",
            "Batch 3000/3125\t Loss 1.149181 (2.187745)\n",
            "Batch 3100/3125\t Loss 2.497853 (2.187929)\n",
            "==> Test\n",
            "       TNR    AUROC  DTACC  AUIN   AUOUT \n",
            "Bas    18.070 74.408 68.255 76.205 70.874\n",
            "Acc: 71.31000\n",
            "Best Acc (%): 71.310\t\n",
            "==> Epoch 21/30\n",
            "Batch 100/3125\t Loss 3.332638 (2.174949)\n",
            "Batch 200/3125\t Loss 3.159270 (2.137585)\n",
            "Batch 300/3125\t Loss 1.895781 (2.138570)\n",
            "Batch 400/3125\t Loss 2.373652 (2.170332)\n",
            "Batch 500/3125\t Loss 2.116507 (2.177967)\n",
            "Batch 600/3125\t Loss 1.500024 (2.196860)\n",
            "Batch 700/3125\t Loss 2.904150 (2.202356)\n",
            "Batch 800/3125\t Loss 2.790987 (2.186114)\n",
            "Batch 900/3125\t Loss 2.098932 (2.184935)\n",
            "Batch 1000/3125\t Loss 3.147331 (2.186055)\n",
            "Batch 1100/3125\t Loss 2.192053 (2.193949)\n",
            "Batch 1200/3125\t Loss 1.900410 (2.196396)\n",
            "Batch 1300/3125\t Loss 2.528709 (2.201502)\n",
            "Batch 1400/3125\t Loss 1.689417 (2.199625)\n",
            "Batch 1500/3125\t Loss 1.646339 (2.203575)\n",
            "Batch 1600/3125\t Loss 1.053484 (2.207161)\n",
            "Batch 1700/3125\t Loss 2.221614 (2.211623)\n",
            "Batch 1800/3125\t Loss 2.775621 (2.206597)\n",
            "Batch 1900/3125\t Loss 1.536474 (2.203514)\n",
            "Batch 2000/3125\t Loss 1.882469 (2.201195)\n",
            "Batch 2100/3125\t Loss 2.430102 (2.196047)\n",
            "Batch 2200/3125\t Loss 3.252928 (2.198612)\n",
            "Batch 2300/3125\t Loss 2.203382 (2.196333)\n",
            "Batch 2400/3125\t Loss 2.961020 (2.195932)\n",
            "Batch 2500/3125\t Loss 4.484718 (2.194109)\n",
            "Batch 2600/3125\t Loss 1.275659 (2.194400)\n",
            "Batch 2700/3125\t Loss 2.941951 (2.195851)\n",
            "Batch 2800/3125\t Loss 2.289486 (2.196288)\n",
            "Batch 2900/3125\t Loss 2.212612 (2.193885)\n",
            "Batch 3000/3125\t Loss 2.856238 (2.195586)\n",
            "Batch 3100/3125\t Loss 1.254539 (2.196746)\n",
            "==> Epoch 22/30\n",
            "Batch 100/3125\t Loss 2.668067 (2.109887)\n",
            "Batch 200/3125\t Loss 2.553863 (2.196245)\n",
            "Batch 300/3125\t Loss 2.362350 (2.200339)\n",
            "Batch 400/3125\t Loss 2.085590 (2.181561)\n",
            "Batch 500/3125\t Loss 1.140681 (2.181055)\n",
            "Batch 600/3125\t Loss 3.510649 (2.176624)\n",
            "Batch 700/3125\t Loss 2.548012 (2.180512)\n",
            "Batch 800/3125\t Loss 2.066330 (2.190207)\n",
            "Batch 900/3125\t Loss 1.868507 (2.186512)\n",
            "Batch 1000/3125\t Loss 2.130750 (2.186173)\n",
            "Batch 1100/3125\t Loss 2.343866 (2.188919)\n",
            "Batch 1200/3125\t Loss 2.384001 (2.187955)\n",
            "Batch 1300/3125\t Loss 2.397716 (2.179671)\n",
            "Batch 1400/3125\t Loss 2.012374 (2.187063)\n",
            "Batch 1500/3125\t Loss 3.239658 (2.179124)\n",
            "Batch 1600/3125\t Loss 2.670117 (2.178472)\n",
            "Batch 1700/3125\t Loss 1.961826 (2.176367)\n",
            "Batch 1800/3125\t Loss 2.022807 (2.182258)\n",
            "Batch 1900/3125\t Loss 2.479954 (2.181801)\n",
            "Batch 2000/3125\t Loss 3.197187 (2.183116)\n",
            "Batch 2100/3125\t Loss 2.698288 (2.180027)\n",
            "Batch 2200/3125\t Loss 1.714868 (2.182257)\n",
            "Batch 2300/3125\t Loss 1.854680 (2.185511)\n",
            "Batch 2400/3125\t Loss 2.032722 (2.184643)\n",
            "Batch 2500/3125\t Loss 2.921752 (2.184941)\n",
            "Batch 2600/3125\t Loss 2.874356 (2.185461)\n",
            "Batch 2700/3125\t Loss 1.855345 (2.186257)\n",
            "Batch 2800/3125\t Loss 2.542863 (2.185737)\n",
            "Batch 2900/3125\t Loss 1.989024 (2.185229)\n",
            "Batch 3000/3125\t Loss 1.791740 (2.182791)\n",
            "Batch 3100/3125\t Loss 3.887739 (2.180847)\n",
            "==> Epoch 23/30\n",
            "Batch 100/3125\t Loss 3.300234 (2.155363)\n",
            "Batch 200/3125\t Loss 1.567154 (2.200930)\n",
            "Batch 300/3125\t Loss 1.760664 (2.209359)\n",
            "Batch 400/3125\t Loss 2.437231 (2.211751)\n",
            "Batch 500/3125\t Loss 1.478680 (2.211820)\n",
            "Batch 600/3125\t Loss 2.077120 (2.189639)\n",
            "Batch 700/3125\t Loss 1.943885 (2.192324)\n",
            "Batch 800/3125\t Loss 1.545450 (2.183064)\n",
            "Batch 900/3125\t Loss 2.667879 (2.177786)\n",
            "Batch 1000/3125\t Loss 2.978393 (2.177877)\n",
            "Batch 1100/3125\t Loss 2.096593 (2.171710)\n",
            "Batch 1200/3125\t Loss 2.116668 (2.173457)\n",
            "Batch 1300/3125\t Loss 1.816708 (2.171159)\n",
            "Batch 1400/3125\t Loss 1.772371 (2.178173)\n",
            "Batch 1500/3125\t Loss 3.328391 (2.175418)\n",
            "Batch 1600/3125\t Loss 3.026305 (2.184933)\n",
            "Batch 1700/3125\t Loss 2.027206 (2.189513)\n",
            "Batch 1800/3125\t Loss 1.743385 (2.196769)\n",
            "Batch 1900/3125\t Loss 1.023721 (2.198506)\n",
            "Batch 2000/3125\t Loss 0.995634 (2.193820)\n",
            "Batch 2100/3125\t Loss 1.945377 (2.193831)\n",
            "Batch 2200/3125\t Loss 2.379757 (2.193671)\n",
            "Batch 2300/3125\t Loss 2.502438 (2.190532)\n",
            "Batch 2400/3125\t Loss 1.373374 (2.188261)\n",
            "Batch 2500/3125\t Loss 2.076069 (2.181857)\n",
            "Batch 2600/3125\t Loss 2.389660 (2.180316)\n",
            "Batch 2700/3125\t Loss 2.138751 (2.179548)\n",
            "Batch 2800/3125\t Loss 1.644148 (2.177345)\n",
            "Batch 2900/3125\t Loss 1.849566 (2.178016)\n",
            "Batch 3000/3125\t Loss 3.344188 (2.176090)\n",
            "Batch 3100/3125\t Loss 2.427282 (2.174862)\n",
            "==> Epoch 24/30\n",
            "Batch 100/3125\t Loss 1.825811 (2.180854)\n",
            "Batch 200/3125\t Loss 2.660626 (2.163603)\n",
            "Batch 300/3125\t Loss 2.025686 (2.147576)\n",
            "Batch 400/3125\t Loss 2.258409 (2.133393)\n",
            "Batch 500/3125\t Loss 1.972463 (2.137039)\n",
            "Batch 600/3125\t Loss 3.408844 (2.150853)\n",
            "Batch 700/3125\t Loss 3.197293 (2.140518)\n",
            "Batch 800/3125\t Loss 1.647439 (2.151109)\n",
            "Batch 900/3125\t Loss 2.102415 (2.160256)\n",
            "Batch 1000/3125\t Loss 2.278933 (2.158658)\n",
            "Batch 1100/3125\t Loss 2.109137 (2.167781)\n",
            "Batch 1200/3125\t Loss 1.760539 (2.167132)\n",
            "Batch 1300/3125\t Loss 2.169892 (2.167270)\n",
            "Batch 1400/3125\t Loss 2.375850 (2.170521)\n",
            "Batch 1500/3125\t Loss 2.261284 (2.174283)\n",
            "Batch 1600/3125\t Loss 2.584540 (2.169932)\n",
            "Batch 1700/3125\t Loss 2.233184 (2.171947)\n",
            "Batch 1800/3125\t Loss 2.428206 (2.173757)\n",
            "Batch 1900/3125\t Loss 2.121133 (2.175732)\n",
            "Batch 2000/3125\t Loss 2.713871 (2.177075)\n",
            "Batch 2100/3125\t Loss 2.570842 (2.178923)\n",
            "Batch 2200/3125\t Loss 1.748122 (2.179184)\n",
            "Batch 2300/3125\t Loss 3.019802 (2.182479)\n",
            "Batch 2400/3125\t Loss 2.124441 (2.181532)\n",
            "Batch 2500/3125\t Loss 2.564204 (2.182919)\n",
            "Batch 2600/3125\t Loss 2.232453 (2.185387)\n",
            "Batch 2700/3125\t Loss 2.555682 (2.185453)\n",
            "Batch 2800/3125\t Loss 2.114503 (2.189319)\n",
            "Batch 2900/3125\t Loss 1.725388 (2.190709)\n",
            "Batch 3000/3125\t Loss 2.523174 (2.190495)\n",
            "Batch 3100/3125\t Loss 2.114966 (2.189843)\n",
            "==> Epoch 25/30\n",
            "Batch 100/3125\t Loss 1.237093 (2.171313)\n",
            "Batch 200/3125\t Loss 1.269154 (2.106746)\n",
            "Batch 300/3125\t Loss 1.589469 (2.137290)\n",
            "Batch 400/3125\t Loss 1.812512 (2.156450)\n",
            "Batch 500/3125\t Loss 2.205782 (2.142142)\n",
            "Batch 600/3125\t Loss 2.173895 (2.143959)\n",
            "Batch 700/3125\t Loss 2.305432 (2.149614)\n",
            "Batch 800/3125\t Loss 1.812620 (2.156544)\n",
            "Batch 900/3125\t Loss 0.926167 (2.165470)\n",
            "Batch 1000/3125\t Loss 1.494938 (2.173450)\n",
            "Batch 1100/3125\t Loss 2.112259 (2.172134)\n",
            "Batch 1200/3125\t Loss 2.009562 (2.178358)\n",
            "Batch 1300/3125\t Loss 1.837572 (2.174511)\n",
            "Batch 1400/3125\t Loss 2.432313 (2.176206)\n",
            "Batch 1500/3125\t Loss 2.319040 (2.178572)\n",
            "Batch 1600/3125\t Loss 1.800024 (2.172222)\n",
            "Batch 1700/3125\t Loss 2.128551 (2.174907)\n",
            "Batch 1800/3125\t Loss 1.770891 (2.172458)\n",
            "Batch 1900/3125\t Loss 2.084087 (2.169401)\n",
            "Batch 2000/3125\t Loss 1.546702 (2.170660)\n",
            "Batch 2100/3125\t Loss 1.719114 (2.166725)\n",
            "Batch 2200/3125\t Loss 1.631202 (2.166495)\n",
            "Batch 2300/3125\t Loss 2.023832 (2.164778)\n",
            "Batch 2400/3125\t Loss 1.595183 (2.164638)\n",
            "Batch 2500/3125\t Loss 4.107738 (2.167675)\n",
            "Batch 2600/3125\t Loss 2.312656 (2.169131)\n",
            "Batch 2700/3125\t Loss 2.500268 (2.169363)\n",
            "Batch 2800/3125\t Loss 2.957676 (2.168174)\n",
            "Batch 2900/3125\t Loss 2.613096 (2.170729)\n",
            "Batch 3000/3125\t Loss 1.973821 (2.172952)\n",
            "Batch 3100/3125\t Loss 2.819700 (2.175196)\n",
            "==> Test\n",
            "       TNR    AUROC  DTACC  AUIN   AUOUT \n",
            "Bas    16.950 74.486 68.460 76.470 70.515\n",
            "Acc: 69.12000\n",
            "==> Epoch 26/30\n",
            "Batch 100/3125\t Loss 1.724663 (2.184695)\n",
            "Batch 200/3125\t Loss 1.522887 (2.180463)\n",
            "Batch 300/3125\t Loss 2.606751 (2.173345)\n",
            "Batch 400/3125\t Loss 1.715011 (2.171337)\n",
            "Batch 500/3125\t Loss 1.018177 (2.171295)\n",
            "Batch 600/3125\t Loss 2.494050 (2.167002)\n",
            "Batch 700/3125\t Loss 2.900496 (2.179044)\n",
            "Batch 800/3125\t Loss 2.673489 (2.172497)\n",
            "Batch 900/3125\t Loss 1.940215 (2.169990)\n",
            "Batch 1000/3125\t Loss 1.663168 (2.161600)\n",
            "Batch 1100/3125\t Loss 1.805007 (2.162278)\n",
            "Batch 1200/3125\t Loss 2.819501 (2.164606)\n",
            "Batch 1300/3125\t Loss 2.561813 (2.159596)\n",
            "Batch 1400/3125\t Loss 2.290458 (2.160493)\n",
            "Batch 1500/3125\t Loss 1.383190 (2.162625)\n",
            "Batch 1600/3125\t Loss 1.381703 (2.162471)\n",
            "Batch 1700/3125\t Loss 2.145046 (2.166368)\n",
            "Batch 1800/3125\t Loss 2.485924 (2.166851)\n",
            "Batch 1900/3125\t Loss 2.991422 (2.164215)\n",
            "Batch 2000/3125\t Loss 1.554429 (2.164856)\n",
            "Batch 2100/3125\t Loss 1.708329 (2.166469)\n",
            "Batch 2200/3125\t Loss 4.442996 (2.170236)\n",
            "Batch 2300/3125\t Loss 2.482609 (2.167637)\n",
            "Batch 2400/3125\t Loss 2.833164 (2.167847)\n",
            "Batch 2500/3125\t Loss 1.564854 (2.165392)\n",
            "Batch 2600/3125\t Loss 2.274534 (2.169843)\n",
            "Batch 2700/3125\t Loss 2.901262 (2.171675)\n",
            "Batch 2800/3125\t Loss 2.891698 (2.171885)\n",
            "Batch 2900/3125\t Loss 1.636118 (2.170424)\n",
            "Batch 3000/3125\t Loss 1.085423 (2.172047)\n",
            "Batch 3100/3125\t Loss 1.513255 (2.173802)\n",
            "==> Epoch 27/30\n",
            "Batch 100/3125\t Loss 1.434265 (2.235693)\n",
            "Batch 200/3125\t Loss 2.224344 (2.216736)\n",
            "Batch 300/3125\t Loss 1.529124 (2.216439)\n",
            "Batch 400/3125\t Loss 2.649876 (2.212408)\n",
            "Batch 500/3125\t Loss 3.942402 (2.203595)\n",
            "Batch 600/3125\t Loss 1.376623 (2.184536)\n",
            "Batch 700/3125\t Loss 1.834409 (2.191667)\n",
            "Batch 800/3125\t Loss 2.317845 (2.191557)\n",
            "Batch 900/3125\t Loss 3.494470 (2.203446)\n",
            "Batch 1000/3125\t Loss 0.975722 (2.203555)\n",
            "Batch 1100/3125\t Loss 1.951031 (2.197383)\n",
            "Batch 1200/3125\t Loss 3.002120 (2.192128)\n",
            "Batch 1300/3125\t Loss 1.929529 (2.198609)\n",
            "Batch 1400/3125\t Loss 1.938696 (2.189978)\n",
            "Batch 1500/3125\t Loss 2.583942 (2.190462)\n",
            "Batch 1600/3125\t Loss 2.883385 (2.191537)\n",
            "Batch 1700/3125\t Loss 1.663405 (2.194649)\n",
            "Batch 1800/3125\t Loss 1.852916 (2.192405)\n",
            "Batch 1900/3125\t Loss 2.091750 (2.194821)\n",
            "Batch 2000/3125\t Loss 2.703991 (2.192875)\n",
            "Batch 2100/3125\t Loss 2.030468 (2.191105)\n",
            "Batch 2200/3125\t Loss 2.988771 (2.190807)\n",
            "Batch 2300/3125\t Loss 1.477852 (2.189427)\n",
            "Batch 2400/3125\t Loss 1.822047 (2.189751)\n",
            "Batch 2500/3125\t Loss 2.201783 (2.188421)\n",
            "Batch 2600/3125\t Loss 3.569599 (2.189166)\n",
            "Batch 2700/3125\t Loss 2.094131 (2.185015)\n",
            "Batch 2800/3125\t Loss 2.017842 (2.184961)\n",
            "Batch 2900/3125\t Loss 1.939322 (2.182283)\n",
            "Batch 3000/3125\t Loss 3.376823 (2.180717)\n",
            "Batch 3100/3125\t Loss 1.781563 (2.181904)\n",
            "==> Epoch 28/30\n",
            "Batch 100/3125\t Loss 2.169969 (2.170273)\n",
            "Batch 200/3125\t Loss 0.825441 (2.166042)\n",
            "Batch 300/3125\t Loss 1.484799 (2.152551)\n",
            "Batch 400/3125\t Loss 1.557079 (2.124841)\n",
            "Batch 500/3125\t Loss 2.597049 (2.131376)\n",
            "Batch 600/3125\t Loss 1.341955 (2.112327)\n",
            "Batch 700/3125\t Loss 1.893814 (2.110393)\n",
            "Batch 800/3125\t Loss 2.838846 (2.127033)\n",
            "Batch 900/3125\t Loss 1.815426 (2.130219)\n",
            "Batch 1000/3125\t Loss 2.599733 (2.137858)\n",
            "Batch 1100/3125\t Loss 1.866892 (2.131761)\n",
            "Batch 1200/3125\t Loss 1.271708 (2.127340)\n",
            "Batch 1300/3125\t Loss 2.064821 (2.133183)\n",
            "Batch 1400/3125\t Loss 1.315333 (2.140459)\n",
            "Batch 1500/3125\t Loss 1.655781 (2.147990)\n",
            "Batch 1600/3125\t Loss 2.157275 (2.152644)\n",
            "Batch 1700/3125\t Loss 1.919089 (2.151483)\n",
            "Batch 1800/3125\t Loss 2.315540 (2.153815)\n",
            "Batch 1900/3125\t Loss 2.444250 (2.153501)\n",
            "Batch 2000/3125\t Loss 1.300710 (2.153797)\n",
            "Batch 2100/3125\t Loss 2.567889 (2.156441)\n",
            "Batch 2200/3125\t Loss 1.406398 (2.159700)\n",
            "Batch 2300/3125\t Loss 1.464358 (2.161054)\n",
            "Batch 2400/3125\t Loss 1.649871 (2.158159)\n",
            "Batch 2500/3125\t Loss 1.737968 (2.158297)\n",
            "Batch 2600/3125\t Loss 1.986859 (2.160687)\n",
            "Batch 2700/3125\t Loss 2.077795 (2.162843)\n",
            "Batch 2800/3125\t Loss 1.543201 (2.165215)\n",
            "Batch 2900/3125\t Loss 2.890387 (2.168172)\n",
            "Batch 3000/3125\t Loss 2.495693 (2.166378)\n",
            "Batch 3100/3125\t Loss 1.886601 (2.166597)\n",
            "==> Epoch 29/30\n",
            "Batch 100/3125\t Loss 1.262232 (2.261468)\n",
            "Batch 200/3125\t Loss 1.356715 (2.203204)\n",
            "Batch 300/3125\t Loss 2.529749 (2.222131)\n",
            "Batch 400/3125\t Loss 3.025633 (2.238965)\n",
            "Batch 500/3125\t Loss 2.815336 (2.228048)\n",
            "Batch 600/3125\t Loss 3.169341 (2.211655)\n",
            "Batch 700/3125\t Loss 2.651791 (2.215744)\n",
            "Batch 800/3125\t Loss 2.082350 (2.208391)\n",
            "Batch 900/3125\t Loss 3.772501 (2.210516)\n",
            "Batch 1000/3125\t Loss 3.700676 (2.200690)\n",
            "Batch 1100/3125\t Loss 2.266223 (2.206518)\n",
            "Batch 1200/3125\t Loss 1.771375 (2.202970)\n",
            "Batch 1300/3125\t Loss 2.403095 (2.194338)\n",
            "Batch 1400/3125\t Loss 1.118126 (2.196214)\n",
            "Batch 1500/3125\t Loss 2.025695 (2.196556)\n",
            "Batch 1600/3125\t Loss 1.790231 (2.186167)\n",
            "Batch 1700/3125\t Loss 2.196048 (2.187158)\n",
            "Batch 1800/3125\t Loss 1.385579 (2.180426)\n",
            "Batch 1900/3125\t Loss 2.365852 (2.180040)\n",
            "Batch 2000/3125\t Loss 2.332381 (2.181239)\n",
            "Batch 2100/3125\t Loss 1.625522 (2.178686)\n",
            "Batch 2200/3125\t Loss 2.179863 (2.179088)\n",
            "Batch 2300/3125\t Loss 2.010510 (2.178198)\n",
            "Batch 2400/3125\t Loss 2.987103 (2.177697)\n",
            "Batch 2500/3125\t Loss 2.040820 (2.176335)\n",
            "Batch 2600/3125\t Loss 2.449109 (2.175007)\n",
            "Batch 2700/3125\t Loss 2.130070 (2.177490)\n",
            "Batch 2800/3125\t Loss 1.570297 (2.176064)\n",
            "Batch 2900/3125\t Loss 1.130503 (2.172242)\n",
            "Batch 3000/3125\t Loss 0.846851 (2.172009)\n",
            "Batch 3100/3125\t Loss 1.629992 (2.170282)\n",
            "==> Epoch 30/30\n",
            "Batch 100/3125\t Loss 1.623519 (2.109272)\n",
            "Batch 200/3125\t Loss 2.349148 (2.159889)\n",
            "Batch 300/3125\t Loss 2.530824 (2.155523)\n",
            "Batch 400/3125\t Loss 2.956870 (2.149766)\n",
            "Batch 500/3125\t Loss 2.006076 (2.166393)\n",
            "Batch 600/3125\t Loss 2.261662 (2.146118)\n",
            "Batch 700/3125\t Loss 2.297109 (2.156246)\n",
            "Batch 800/3125\t Loss 1.301995 (2.156850)\n",
            "Batch 900/3125\t Loss 2.492171 (2.160722)\n",
            "Batch 1000/3125\t Loss 2.838779 (2.165783)\n",
            "Batch 1100/3125\t Loss 2.770918 (2.170585)\n",
            "Batch 1200/3125\t Loss 2.195919 (2.172419)\n",
            "Batch 1300/3125\t Loss 1.337881 (2.172926)\n",
            "Batch 1400/3125\t Loss 1.840666 (2.174447)\n",
            "Batch 1500/3125\t Loss 1.742508 (2.175469)\n",
            "Batch 1600/3125\t Loss 2.795505 (2.173425)\n",
            "Batch 1700/3125\t Loss 1.869305 (2.170392)\n",
            "Batch 1800/3125\t Loss 1.809772 (2.166686)\n",
            "Batch 1900/3125\t Loss 1.852383 (2.172556)\n",
            "Batch 2000/3125\t Loss 2.404295 (2.169111)\n",
            "Batch 2100/3125\t Loss 1.101145 (2.172603)\n",
            "Batch 2200/3125\t Loss 1.544695 (2.173685)\n",
            "Batch 2300/3125\t Loss 2.251228 (2.172410)\n",
            "Batch 2400/3125\t Loss 1.545513 (2.167419)\n",
            "Batch 2500/3125\t Loss 1.074119 (2.165637)\n",
            "Batch 2600/3125\t Loss 2.758184 (2.162198)\n",
            "Batch 2700/3125\t Loss 2.285845 (2.162549)\n",
            "Batch 2800/3125\t Loss 1.721465 (2.161642)\n",
            "Batch 2900/3125\t Loss 1.895611 (2.162209)\n",
            "Batch 3000/3125\t Loss 2.816350 (2.164277)\n",
            "Batch 3100/3125\t Loss 2.069102 (2.163785)\n",
            "==> Test\n",
            "       TNR    AUROC  DTACC  AUIN   AUOUT \n",
            "Bas    16.990 66.374 61.830 64.789 66.111\n",
            "Acc: 61.86000\n",
            "Finished. Total elapsed time (h:m:s): 1:04:07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if options['eval']:\n",
        "        net, criterion = load_networks(net, options['outf'], file_name, criterion=criterion)\n",
        "        outloaders = Data.out_loaders\n",
        "        results = test(net, criterion, testloader, outloader, epoch=0, **options)\n",
        "        acc = results['ACC']\n",
        "        res = dict()\n",
        "        res['ACC'] = dict()\n",
        "        acc_res = []\n",
        "        for key in Data.out_keys:\n",
        "            results = test_robustness(net, criterion, outloaders[key], epoch=0, label=key, **options)\n",
        "            print('{} (%): {:.3f}\\t'.format(key, results['ACC']))\n",
        "            res['ACC'][key] = results['ACC']\n",
        "            acc_res.append(results['ACC'])\n",
        "        print('Mean ACC:', np.mean(acc_res))\n",
        "        print('Mean Error:', 100-np.mean(acc_res))"
      ],
      "metadata": {
        "id": "7tTiict2s8mr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df1c273e-199e-4e28-c47f-5eba8a399367"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       TNR    AUROC  DTACC  AUIN   AUOUT \n",
            "Bas    16.990 66.374 61.830 64.789 66.111\n",
            "Acc: 61.86000\n",
            "gaussian_noise (%): 42.266\t\n",
            "shot_noise (%): 48.196\t\n",
            "impulse_noise (%): 37.844\t\n",
            "defocus_blur (%): 51.190\t\n",
            "glass_blur (%): 39.514\t\n",
            "motion_blur (%): 49.154\t\n",
            "zoom_blur (%): 46.908\t\n",
            "snow (%): 51.426\t\n",
            "frost (%): 54.254\t\n",
            "fog (%): 57.340\t\n",
            "brightness (%): 59.646\t\n",
            "contrast (%): 47.958\t\n",
            "elastic_transform (%): 48.306\t\n",
            "pixelate (%): 57.940\t\n",
            "jpeg_compression (%): 49.144\t\n",
            "Mean ACC: 49.40573333333334\n",
            "Mean Error: 50.59426666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IRxS5jfTAQlV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}